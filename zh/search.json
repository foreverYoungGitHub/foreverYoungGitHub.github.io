[{"title":"pybind:为cpp/cuda代码提供python接口","date":"2020-09-20T05:00:00.000Z","url":"/zh/2020/09/20/zh/python_cpp_extension/","tags":[["python","/zh/tags/python/"],["cuda","/zh/tags/cuda/"],["cpp","/zh/tags/cpp/"]],"categories":[[" ",""]],"content":"python调用C++&#x2F;CUDA有不少的方法，如boost.python, cython, pybind11等。其中，pybind11是一个轻量级的仅标头的库。由于pybind11的易用性，pybind11被很多库用于于创建现有C++&#x2F;CUDA代码的Python绑定，比如pytorch，tvm等。此外，由于Python的缓冲区协议可以公开自定义数据类型的内部存储，python的矩阵类型（如numpy.ndarry,torch.Tensor）可以快速转化到C++中对应矩阵类型（如Eigen，cv::Mat，vector等），不须额外的复制操作。本篇文章将通过一个数列求和的例子来讲解如何使用pybind11来将C++&#x2F;CUDA代码进行Python绑定。 在通过C++和CUDA实现数列求和，在将其绑定为python函数，并在python中调用对应函数，验证结果。 基本环境 “hello, world!” for pybind11不同于pybind11官网推荐的通过CMake编译的编译方式，本文将采用pytorch中pybind11的编译方式，即在setup创建python包时编译项目中C++和CUDA代码。 代码：\"hello, world!\" 工程目录及编译： hello_world.cpp: setup_hello.py: 简单测试： 通过pybind11生成模块的相关文档： 数列求和"},{"title":"Numba: 简单装饰器加速python代码","date":"2020-07-04T05:00:00.000Z","url":"/zh/2020/07/04/zh/numba_python/","tags":[["python","/zh/tags/python/"],["numba","/zh/tags/numba/"]],"categories":[[" ",""]],"content":"简单易用与运行效率低是贴在python身上的两大标签。开发人员一方面对其简单的语法和丰富的库爱不释手，一方面又对其由于动态编译和解释执行带来的较低的运行效率和GIL带来的多线程难扩展的情况深恶痛绝。为了解决这些python中的固有问题，一些解释器如cython尝试对一些函数提前编译进而提高执行效率。但绝大多数的解释器或库函数的语法非常不pythonic，而且也不能够做到即插即用。本文简单介绍了如何通过numba库，为python函数装饰器的方式来对python函数进行加速。为读者提供一中，简单易用，灵活编译方式去解决python的固有问题，提高python代码的执行效率。 环境配置为了简化环境配置，本文中的编程环境将全部由conda配置，并在conda的虚拟环境中测试。所涉及的依赖库分别是numba和numpy。相关环境可以通过以下语令进行配置。 基本概念编译型语言（Compiled language），解释型语言（Interpreted language）和即时编译（Just-In-Time）用高级语言编写的程序一般可通过由解释器（Interpreter）直接执行，或由编译器（Compiler）编译成机械码再执行。由解释器直接执行的高级语言称为解释型语言（Interpreted language），其执行流程一般为：（解释型语言-&gt;程序运行-&gt;字节码-&gt;机械码），常见的解释型语言有Perl，Python，MATLAB和Ruby。需要通过编译器进行提前编译（AoT: ahead of time）再执行的高级语言称为编译型语言（Compiled language），其执行流程一般为：（编译型语言-&gt;机械码-&gt;程序运行），常见的编译型语言有c和c++。由于解释型语言可直接运行，其代码的灵活性相对更高。但在一般情况下，解释型语言需要边编译边运行，所以其执行效率相对于编译型语言较低。尤其对于部分反复执行的代码，解释型语言常需要对其进行反复编译。即时编译（Just-In-Time）结合了编译器的速度和解释器的灵活性，并允许自适应优化。 python是一种典型的解释型语言。在执行时，解释器首先将程序的字节码存储到.pyc，在将字节码发送到python虚拟机上进一步的解释执行字节码。如果程序未发生变化，python的字节码并不需要反复生成，但在python虚拟机上的解释步骤是需要反复执行的。在python numba中，可将numba.jit的函数在第一次执行时生成的机械码，进而在该函数可以直接调用生成的机械码而省去反复解释的过程，进而达到与编译型语言相似的速度。 装饰器（Decorator）装饰器是一种常见的设计模式，其允许向一个现有的对象添加新的功能，同时又不改变其结构。在python中，装饰器可以通过函数或类的形式定义，并通过@decorator的方式调用，一些常见的装饰器结构定义和调用方式可见如下代码。 代码：装饰器 通过函数定义装饰器 输出: 通过类定义装饰器 装饰器的本质是为函数添加新功能但不更改函数结构，比如在本例中计算时间代价，或是将函数注册到模块中。因此，为了保证函数调用的一致性，装饰器返回的仍是函数对象。被返回的函数一般有两种类型，原始输入函数，或是调用原始输入函数的wrapper函数。需要注意的是，当返回函数为wrapper函数时，由于其是一个新的函数对象，函数相关的属性也就不同。如上例timef0所示，简单返回wrapper函数会导致装饰函数的名字变为新函数的名字，进而破坏了装饰函数在装饰前和装饰后的一致性。为了使得装饰前与装饰后函数属性一致，需要在使用时对wrapper函数加入@functools.wraps(func)装饰器。 在装饰器被调用时，其真正执行的是decorator(func)。如对add_val_f1添加@timef1装饰器时，其在声明时是等同于timef1(add_val_f1)。因此也就可以通过函数嵌套的形式，为装饰器传参。比如上例中的timef2，在声明时相当于timef2(num_runs=2)(add_val_f2)；由于timef2(num_runs=2)的返回值为decorator函数，所以该声明也就等价于decorator(add_val_f2)。在这种函数情况下，timef2只负责传参，而decorator函数才完成类似于timef1的功能。当然笔者更喜欢使用timef2所示的递归的方式，来进行更加灵活的定义和传参。 此外，装饰器是可以嵌套的，具体例子会在下面小节展示。 通过numba.jit加速python for循环上小节提到，在python的for中，即使是相同的代码也需要对字节码进行反复解释，这种执行编译的方式是低效的。在numba中，可以通过添加简单@numba.jit装饰器进行加速，比如可以通过如下代码对上述例子进行加速。 代码：numba.jit加速for循环 输出: 其中__pychache__中生成对应的缓存文件： 在上诉例子中，可以看到通过简单添加@numba.jit装饰器来对python代码进行加速。通过add_val_fn和add_val_fn_warmedup极大的速度差距可以看到，在@numba.jit中，对于第一次执行还是存在解释编译，因此执行效率相对缓慢。而在第一次解释编译之后，其执行速度几乎达到甚至超过numpy经过优化之后的速度。 在numba.jit中常用到的参数有三个，分别为：nopython，cache和parallel。 nopython参数用于控制numba的编译模式，numba有两种编译模式，分别为非python模式和对象模式。当nopython&#x3D;True时，numba编译模式为非python模式。这种编译模式会产生最高性能的代码，但由于其生成的代码无法访问python c api，对于原生python代码的兼容型一般。比如非python模式无法兼任原生的python类作为输入类型，但其可以通过numba.jitclass或是numpy的结构体来解决。 cache参数用于控制函数的缓存至磁盘文件，可以通过传递cache&#x3D;True避免每次调用Python程序时都要进行编译，进而提升第一次执行相对缓慢相对缓慢的情况。如在上例中，add_val_f4通过cache&#x3D;True缓存下编译过的文件，进而达到与add_val_f4_warmedup相似的速度。需要注意的是当函数缓存文件不存在时，第一次执行该函数将生成缓存文件，而本次执行的速度同样相对缓慢。 parallel参数用于控制函数的并行。当parallel&#x3D;True时，numba对函数内的操作进行并行优化。也可以通过numba.prange进行显示并行循环进而进一步提高执行效率。此外，numba通过numba.config.NUMBA_DEFAULT_NUM_THREADS来指定并行的线程数。 numba.jit足以应付绝大多数的循环，但当一个操作只想针对数组的某一维度进行操作，numba.jit就显得力不从心了。下一小节将讨论如何通过numba中的vectroize和guvectorize生成numpy的通用函数去解决多维数组的扩展问题。 通过numba中的vectroize和guvectorize创建高效的numpy通用函数在numpy中通用函数ufunc（universal function），是一种能对数组的每个元素进行操作的函数，一些基本操作如add，dot，max，any等在numpy中都通过c来实现进而达到较好的执行效率。 对于相对较复杂的操作，numpy允许自定义ufunc。但相较于通过c实现的ufunc，通过python自定义的ufunc与原始python函数速度差异不大。而且numpy自定义的ufunc只支持按位操作（elementwise operation），对于相对较复杂的操作如矩阵乘法这种广义通用函数，numpy的支持并不友好。而numba的vectroize和guvectorize在兼容numpy.ufunc的特性的同时，执行效率高并且支持广义通用函数。 本节示例展示了通过numba中的vectroize和guvectorize分别来加速numpy自定义的ufunc。 代码：numba.vectroize创建高效numpy通用函数 - 按位乘法 输出： 代码：numba.guvectroize创建高效numpy广义通用函数 - 矩阵乘法 输出： 通过上例可以看到，相较于numpy.vectorize所生成的ufunc，numba.vectorize执行效率更高，甚至超过numpy通过c实现的通用函数的执行速度。而对于numba.guvectorize虽然可以灵活自定义，并且远远超过原始python实现，但相较于numpy优化过的代码仍有差距。 类似于numba.jit，numba.vectorize和numba.guvectorize同样可以通过nopython和cache参数来设置编译模式和缓存文件。与之不同的是，vectorize和guvectorize通过target参数来控制并行计算。当target&#x3D;”cpu”时numba使用单线程，当target&#x3D;”parallel”时numba使用多线程并行。值得一提的是target还可以允许使用nvidia cuda作为numpy通用函数的计算单元，虽然笔者并不建议。如果读者希望通过cuda加速python代码，可以看笔者的另一篇博客Numba: 通过python快速学习cuda编程。 为什么要用ufunc而不是jit？jit更直观，更灵活，甚至更快。主要原因是因为numba生成的numpy.ufunc可以对某些轴进行广播（broadcast），在一些情况下可以会某一些轴进行缩减和累积。这样输入数据的维度就可以更自由。但除此以外，笔者更推荐numba.jit。 numba.pycc提前编译python函数虽然numba主要的编译方式都是jit，但numba也如cython一样提供提前编译（AoT）的编译方式。本小节对之前numba.jit的例子进行改写，达到了提前编译的效果。 代码：numba.pycc提前编译python函数 运行生成.so的二进制文件 输出: 通过上例可以看到，通过numba.pycc可以提前编译生成的python函数为机械码。被编译的模块在运行时没有编译开销，也不依赖于Numba库。numba.pycc还可以将编译步骤集成到setuptools等脚本中。虽然numba.pycc使用起来灵活方便，但也有其局限性。如摆脱对numba依赖的同时也无法使用numba的一下特性，比如并行计算。而且numba.pycc编译仅允许使用常规的numba.jit函数，不能使用ufuncs。 至此，笔者对于通过numba加速python代码的介绍就结束了。在笔者使用numba时，虽然有些许的局限性，但其灵活简单的语法，对numpy数组较为完备的支持，保持python原有函数结构上对其极大的提速令笔者非常满意。此外，本文对一些其他笔者不太常用的功能并没有提及，如numba中的cfunc，jit中的nogil等。而且numba在仍在高效的迭代开发，一些实验性的功能如jit_module等都非常有趣。希望读者通过本文对numba有初步的了解，进而运用到自己的项目中。如有需要，多多查看官方手册。祝好，共勉。 参考在撰写本文时，大量的参考了官方文档和其他博客，收益良多。观点表述如有雷同，可视为出自原作者。相关参考链接如下： numba document python高性能编程与实战案例代码总结 numba 的基本应用 Hello, JIT World: The Joy of Simple JITs "},{"title":"通过NPP加速TensorRT部署时图片数据预处理","date":"2020-06-17T05:00:00.000Z","url":"/zh/2020/06/17/zh/trt_preproc_npp/","tags":[["cuda","/zh/tags/cuda/"],["cpp","/zh/tags/cpp/"],["tensorrt","/zh/tags/tensorrt/"]],"categories":[[" ",""]],"content":"TensorRT(TRT)是NVIDIA推出的一个高性能的深度学习推理框架，可以让深度学习模型在NVIDIA GPU上实现低延迟，高吞吐量的部署。主流框架的模型可以通过转换为TensorRT在NVIDIA GPU进而达到极大地提速。然而，由于TensorRT并不支持常见的图片数据类型uint8，这使得往往需要在cpu上进行图片数据预处理，转换为其所支持的float并传入到gpu模型输入。当图片较大时，数据在cpu上的处理和传递时间较慢。本文将介绍如何通过cuda中的npp库来加速这一过程。 基本环境 模型是由ssds.pytorch训练的Yolov3目标检测器，已转换为TRT模型。其输入大小为1x3x736x1280，特征提取器为ResNet18，计算精度为int8。 需要注意，当将模型转换为TRT模型时，TRT会根据gpu框架和性能来来选择不同的核函数及其参数，进而最大程度优化推理速度。因此TRT在执行时，必须使用统一gpu框架所生成的TRT模型，否则将无法推断。并且即使是统一框架不同型号的gpu所生成的TRT模型，其推理速度也会有些许削弱。比如虽然2080ti和t4同属7.5计算框架，在T4上推断，由2080ti所生成的TRT模型要比由T4生成的模型推断速度慢3~10%。 CPU图片数据预处理在一些深度学习框架中，在其推断时可以指定推断时所接受的数据类型，并将数据预处理的步骤在计算图中定义。如在tensorflow将权重转化为推断模型(frozen graph)时，可以通过tf.placeholder(dtype=tf.uint8, shape=input_shape, name=&#39;image_tensor&#39;)来指定推理时模型接受的数据类型。这种改变在推理时模型的操作在TensorRT上似乎不那么行得通。虽然TensorRT支持多种数据类型，并且输入输出接口的数据类型可由转换的onnx或uff文件决定，但当输入输出的数据类型改为除float32以外的其他类型时，常常无法成功转换为TRT推理模型。 TensorRT中这种输入输出类型的限制，对于计算机视觉的模型显得更加不友好。计算机视觉常处理的图像或视频在计算机中常存储为0～255的uint8数据，这种类型本身就不被TensorRT所支持，须将图片先转为float数据再输入到TensorRT。而在一些任务中，其输入模型的图片或视频片段分辨率较大，如4k或8k，uint8和float从cpu内存传输到gpu显存的速度差别就比较大。这些原因导致了在计算机视觉模型部署时，图片数据预处理和传输有时候成为了模型部署的瓶颈。 大多数github上的TRT项目在进行推断中的图片数据预处理常采用TensorRT官方示例中给出了一种在CPU图片数据预处理。笔者个人喜欢用OpenCV自带函数进行操作，这两种图片数据预处理的示例代码如下。 代码：CPU图片数据预处理 TensorRT官方示例图片数据预处理代码 OpenCV图片数据预处理代码 OpenCV图片数据预处理运算速度(ms) GPU(Precision) Image2Float Copy2GPU Inference GPU2CPU t4(int8) 2.53026 0.935451 2.56143 0.0210528 如上例代码所示，在CPU图片数据预处理中，首先将数据转化为float类型并做归一化，然后将数据排列从NHWC转化为NCHW，最后将float型nchw图片数据传递给到TRT模型预留的gpu显存。可以看到，对于该模型图像预处理和传输的速度反而大于模型推断速度。这种cpu图片预处理方式无法高效使用gpu的性能，使得整个模型部署效率较低。 GPU-NPP图片数据预处理上面提到，cpu图片数据预处理效率较低由两方面原因导致：其一，cpu将图像从uint8转化为float32的效率较低；其二，相较于uint8，float32从cpu传输到gpu数据量大四倍，传输效率较慢。所以比较朴素的提速想法就是将uint8数据传输到gpu，并由gpu完成从uint8转化为float32的转化。NPP就可以方便快速的实现如上过程。 NPP是nvidia推出的用于gpu加速2D图像和信号处理的cuda库，其本身就内嵌于cuda库中。其分为多个部分，可以在gpu上高效的进行数据类型转换，颜色变化，几何变化等功能。本例中采用其中的NPPC，NPPIDEI和NPPIAL部分来进行图片数据预处理中的uint8到float32的数据类型转化，NHWC到NCHW的通道变化，以及归一化操作。其具体代码如下。 代码：GPU-NPP图片数据预处理 NPP图片数据预处理代码 NPP图片数据预处理代码（无通道变化和归一化） NPP图片数据预处理（无通道变化和归一化）运算速度(ms) GPU(Precision) Image2GPU2Float Inference GPU2CPU t4(int8) 0.532469 3.07869 0.0208867 如上例代码所示，在GPU图片数据预处理中，首先将uint8数据传输到gpu显存中，然后将数据排列从NHWC转化为NCHW，最后转化为float类型并做归一化，归一化后的数据直接存储在TRT模型预留的gpu显存中。由于按位运算(elementwise)和通道转换(channel permute)在TRT模型中执行效率较高，可将预处理中的归一化和通道转换移到模型内计算。可以看到，相较于cpu的图像预处理，gpu图像预处理的时间从3.5ms降到0.5ms，整个模型总运行时间从6ms降到3.5ms，每秒帧处理量（fps）从166帧提升到了285帧，整体达到了1.7倍的提速。 需要注意的是，由于TRT模型转化时间较长，本文示例只测试batch为1时的执行速度。如果在部署时遇到batch较大而导致gpu图片预处理速度较慢，由于cuda代码执行和传输特性，可考虑整批图像一起从cpu内存拷贝到gpu并进行uint8到float32的转化，进而提高大batch情况下的GPU图片数据预处理处理速度。 参考 tensorrt document gpu计算框架 ssds.pytorch 如果有TensorRT的项目，不妨来试试通过本文提到的GPU图片数据预处理的方法来提速模型推断流程的速度吧！"},{"title":"Numba: 通过python快速学习cuda编程","date":"2020-06-10T05:00:00.000Z","url":"/zh/2020/06/10/zh/numba_cuda/","tags":[["python","/zh/tags/python/"],["numba","/zh/tags/numba/"],["cuda","/zh/tags/cuda/"]],"categories":[[" ",""]],"content":"复杂的编译环境配置，指针操作和debug过程，CUDA编程对于初学者来说总是望而生畏。本文旨在通过使用python的numba库，快速学习和了解CUDA多线程高并发的编程思路。 环境配置为了简化环境配置，本文中的编程环境将全部由conda配置，并在conda的虚拟环境中测试。所涉及的依赖库分别是cudatoolkit, numba, numpy。相关环境可以通过以下语令进行配置。 当安装完成后，可以通过nvidia-smi去检查gpu状态，如下表所示。通过该表可以看到运行在每个gpu上的程序以及它们显存的使用占用，也可以通过memory-usage检查每个显卡总体显存占用。 并且也可以通过python -c &quot;from numba import cuda; print(cuda.gpus)&quot;检查是否可以成功在numba库中访问到gpu设备。当其返回为&lt;Managed Device 0&gt;...时则证明可在python环境下成功访问显卡。 当主机没有gpu设备时，依然可以通过numba提供的gpu模拟器去运行python的cuda代码，只需设置相关环境变量即可：export NUMBA_ENABLE_CUDASIM=1。需要注意的是该模拟器通过cpu进行模拟调试，物理上的计算单元个数远小于gpu个数。所以通过模拟器运行的程序运行速度略慢于等同cpu多线程。而且在模拟器能够运行的程序，不一定能够在gpu设备上运行。因此只能做学习使用。 当环境搭建成功后，接下来就开始CUDA编程之旅吧~ “Hello, World!”: 初窥GPU中的线程网格（Grid），线程块（Block）和线程（Thread）环境配置成功后，本小节将通过在gpu设备上执行打印”Hello, World!”程序，来初步了解gpu编程中的基本概念以及启动gpu函数的基本流程。具体程序如下。 代码：\"Hello, World!\" 输出: 在numba中，cuda相关的函数被封装在numba.cuda中，可通过from numba import cuda引入。如果需要将代码在gpu上执行，需要将代码封装到函数中，并为函数加装饰器@cuda.jit。添加@cuda.jit装饰器的函数常备称为核函数(kernal)，由cpu调用，但只在gpu设备上执行。 具体而言，核函数运行在gpu的最小计算单元(core)上，每次运行由单个线程（Thread）所调用，多个线程组成一个线程块（Block），多个线程块组成线程网格（Grid）。粗略的讲，通常执行单个核函数的所有线程在一个线程网络中。该线程网络控制着线程块的数量和生存周期，而每个线程块又控制着其所属的线程的个数和生存周期。运行线程的计算单元被称为流处理器（SP: Streaming Processor），多个线程组成的线程块运行在多流处理器上（SM: Streaming Multiprocessor），多个线程块组成的线程网格运行在一个gpu显卡上。 因此在核函数被cpu调用时，需要传入该核函数所在的线程网络中线程块的总体个数，以及每个线程块的线程个数。因此在执行numba.cuda中的核函数时需要定义线程块和线程的个数，如kernal[num_block_per_grid, num_thread_per_block]()。在本例中，gpu_print[2, 4]()表示使用2个线程块，每个线程块中使用4个线程去并行执行该核函数。因此共有2*4=8个线程并行执行该核函数，也就输出了8次&quot;hello world!&quot;。 核函数的启动方式是异步的：启动gpu核函数后，cpu不会等待gpu核函数执行完毕才执行下一行代码。必要时，需要调用cuda.synchronize()，告知cpu等待gpu执行完核函数后，再进行cpu端后续计算。这个过程被称为同步。如果不调用cuda.synchronize()函数，执行结果也将改变，cpu: &quot;hello world!&quot;将先被打印。虽然gpu核函数调用在前，但是程序并没有等待核函数执行完，而是继续执行后面的cpu_print函数。由于cpu调用gpu有一定的延迟，反而后面的cpu_print先被执行，因此cpu_print的结果先被打印了出来。 需要注意的是每个执行核函数的线程都知道其所在的线程块索引(block index)和线程索引(thread index)，以及线程块和线程的个数(grid dimension and block dimension)。由于索引值可以通过计算等价于循环中的索引值，因此可以较为简单的通过使用cuda并行计算来取代程序中的顺序循环。线程（块）索引和个数可以有最多三个维度，可分别通过x,y,z进行访问。比如，在本例中我们通过cuda.blockIdx.x及cuda.threadIdx.x来得到该线程的线程块索引和线程索引，并且将其打印。 下文会通过并行计算矩阵加法来初步了解cuda多线程并行计算。 矩阵加法： 小试并行计算上一小节通过”Hello, World!”程序，对gpu核函数的运行流程以及cuda编程中的线程网格（Grid），线程块（Block）和线程（Thread）有了一个基本了解。本节将通过二维矩阵加法来进一步阐述cuda编程中的线程设置以及数据拷贝。具体程序如下。 代码：矩阵加法 输出1: 输出2： 当计算独立时，并行的cuda核函数可以简单取代程序中的循环并实现巨大的提速。在本例中，纯python编程想要计算两矩阵相加，必须通过循环来遍历矩阵上的所有点来进行计算。正如上一小节所讲，在cuda核函数中的索引值等价于循环中的索引值，python程序中的通过顺序循环遍历矩阵就可以被cuda中的多线程核函数并行遍历矩阵所取代。在核函数中，线程可以通过计算索引来访问到对应的点的位置。比如，循环中的行和列的索引值ridx,cidx 在核函数可通过线程（块）索引和个数来计算得到ridx = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y，cidx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x。如程序所示，该核函数中只计算了矩阵中一个点计算加法。因此在执行该gpu函数时，每个线程只为矩阵中一个点计算加法，多个线程并行运行，进而实现了快速高并发的并行计算。可以看到，相较于纯python编程，cuda并行计算可提速3倍到18倍。但有一点值得奇怪，如果该函数在运行第二次时，其速度为0.8ms左右，也就是存在264～1654倍的提速。所以可能在numba中第一遍速度较慢是由于numba的动态编译所导致的，具体原因尚需细查。 上小节提到线程块运行在多流处理器上，线程运行在流处理器上。在物理显卡中，多流处理器所包含的流处理器的数量是固定的，对目前市场上的主流显卡，多流处理器一般包含1024或512个流处理器。因此，在设置线程个数时，必须小于等于每个多流处理器包含流处理器的个数，也就是要小于或等于1024或512。所以一般线程个数在程序中常作为常量存在，如在本例线程的个数被固定为(16, 16)，也就是256个线程。 并且，多流处理器在物理上通过线程束（warp）来分组调度流处理器，每个线程束调度的线程个数也是固定的，一般为32个。每个线程束中所有流处理器（32个）是一起工作的，执行相同的指令。如果没有这么多的流处理器需要工作，那么这个线程束中的一些流处理器是不工作的。为了使每个线程束中所有的线程都在工作，当进行并行计算时，线程数尽量为32的倍数。如在本例中，线程的个数是256（(16, 16)），也就是32的8倍。如果线程数设置为1的话，线程束会生成掩码，使得32个线程中只有一个线程在真正执行，其他31个线程会进入静默状态。这样计算资源就会浪费，在计算量较大时，执行效率就明显变低了。 由于线程的个数在程序中常为常量，为了能够取代所有循环，一般线程块的个数可以由总循环次数除以线程个数，并向上取整。如在本例中，为了能够遍历到矩阵中所有点，线程块个数被设置为blockspergrid_x = math.ceil(n_col / threadsperblock[0]), blockspergrid_y = math.ceil(n_row / threadsperblock[1])，在本例中也就是(120,68)，也就是8160个线程块。但向上取整存在一个问题，通过该设置所生成的线程个数大于本身的循环次数，即(8160*256 &gt; 1920*1080)，所以在核函数中一般需要进行判断来防止索引溢出。如本例中的if ridx &lt; n_rows and cidx &lt; n_cols:。上小节将到了线程（块）个数最多可定义为三个维度，但每个维度都有可能向上取整，维度越多造成资源浪费的可能性越大。这也就导致了虽然多维线程（块）个数在编程中更容易理解，但为了减少资源浪费的可能，在cuda编程中更建议用单个维度（一维）来定义线程（块）个数。 上小节说，核函数被cpu调用，被gpu执行。再延伸一点，核函数被gpu执行所调用的数据需存储在gpu显存中而非cpu内存中。所以在gpu编程中，常常需要先将输入数据从cpu内存拷贝到gpu显存中，也需要将gpu输出结果从gpu显存拷贝回cpu内存中。在numba.cuda中，如果发现传入数据在cpu内存上时，在执行核函数前，会自动将所有输入数据从cpu内存拷贝到gpu显存中，并在核函数结束后，将所有输入数据从gpu显存同步回cpu内存。但这样就导致了不必要的拷贝和额外的时间开销，如在本例中matrix_A和matrix_B的是不需要拷贝回cpu内存的。所以在cuda编程中，建议通过cuda.to_device()将输入数据从cpu内存拷贝到gpu显存中，结果并将结果通过arr.copy_to_host()从gpu显存拷贝回到cpu内存中。如果想要避免结果数组从cpu内存拷贝到gpu显存造成的额外开销，也可以通过cuda.device_array()在gpu显存中一个空向量来实现。在本例中，在执行核函数时，手动拷贝数据的执行效率要比自动拷贝数据效率要高的多。 此外，因为gpu的传输带宽较大，一般多次传输小数据的速率要低于单次传输同等数据的速率。所以在cpu与gpu相互拷贝数据时，尽量采用单次传输。但当数据较大时，gpu无法同时计算所有的数据，这时分批拷贝并计算就可以进一步提速cuda核函数。 在下一小节，将通过分批处理来进一步优化矩阵加法，并阐述了gpu编程中流(stream)的概念。 分批处理矩阵：流(stream)上节讲到，当数据计算量较大时，gpu无法同时计算所有的数据，这时候gpu可以将计算任务分批放在一个队列中，排队顺序执行。这种按照队列顺序流水线处理的操作叫做流(stream)。这一小节依然进行矩阵加法，但将计算量是上一小节的100倍，并采用了流进行分批处理。具体代码如下。 代码：矩阵加法-多流 输出1: 输出2： 由于gpu的硬件特性，cuda中的很多操作是相互独立的，比如核函数的计算，cpu内存与gpu显存间的相互拷贝等。针对这种互相独立的硬件架构，cuda使用多流(multistream)作为一种高并发的方案：把一个大任务拆分开放到多个流中，每次只对一部分数据进行拷贝、计算和回写，并把这个流程做成流水线。这样数据拷贝和核函数计算重叠的时间是重叠的，进而获得性能的提升。如在本例中，通过cuda流处理比简单的队列处理要略快一些。 既然要形成多流的队列，那么队列中的每一步操作都需要知道自己在流。因此在cuda编程中，需要先创建每个流的对象，再把流对象赋值给每一步操作。在numba中，流对象可通过cuda.stream()来创建；执行核函数时，需要与线程块和线程个数一起定义，如kernal[num_block_per_grid, num_thread_per_block, stream]()；而在拷贝数据时，也需要将其加入拷贝函数，如cuda.to_device(stream=stream)和arr.copy_to_host(stream=stream)。这样，每个流水线可以知道自己所需要执行的步骤了。当不指定具体流对象时，这些操作会在默认的流上执行。 每个流水线是顺序执行的，但非默认的流水线是异步操作的，也就是说先创建的流水线可能后完成。默认流有阻塞的作用。如果调用默认流，那么默认流会等非默认流都执行完才能执行；同样，默认流执行完，才能再次执行其他非默认流。另外，流不宜分配过多，当流过多时，有可能在每个流水线上的核函数计算时间代价会小于单次数据拷贝的时间（如内存到显存或显存到内存）。由于每种操作自身时不独立的，所以其必须等待其他的流完成数据拷贝操作才可以开始自己的数据拷贝操作。这样反而会导致流水线处理的执行效率变低。在实践中，可能需要硬件设备的执行效率和核函数的计算量来微调流的个数，进而达到相对好的表现。 关于矩阵加法的例子先到这里，下面的小节将通过矩阵求和，来阐述并行归约（Reduction）算法及其优化过程中的线程束分化和内存访问。 矩阵求和：并行归约（Reduction）在之前的矩阵加法中，每个点的计算是相互独立的，各个线程不需要考虑其他线程的计算结果，所以其循环替代和核函数计算相对简单直观。但在很多任务中，循环需要依赖上一步的执行结果，替换此类核函数相对复杂了。本小节将引入矩阵求和，来简单介绍通过并行归约算法解决二元操作问题。其代码如下： 代码：矩阵求和 输出1: 输出2: 并行归约（Reduction）是一种基础的并行算法。该算法常处理如下问题：假设有N个输入数据，使用一个符合结合律的二元操作符作用其上，最终生成1个结果。这个二元操作符可以是求和、取最大、取最小、平方、逻辑与或等等。本例以加法为例。这种问题的最基本的解决思路就是串行遍历，如示例代码中的matrix_sum_cpu函数。当将问题转化为并行计算时，由于加法的交换律和结合律，矩阵可以以任意顺序求和，其解决思路可以为：首先把输入数组划分为更小的数据块，之后用一个线程计算一个数据块的部分和，最后把所有部分和再求和得出最终结果。这种思路就被称为并行归约。相较于串行计算，并行归约的时间复杂度由O(N)变为O(logN)。由于并行归约非常经典，又在cuda编程中常常使用，numba.cuda本身就通过cuda.reduce实现了为1维数组的并行归约算法，其相关调用可参考本例中的matrix_sum_reduce_numba_cuda_reduce函数。 在本例中，并行归约在numba.cuda上的具体实现可参考matrix_sum_reduce_numba_cuda函数。具体而言，核函数循环stride，stride的上限为线程块中的线程个数，每次循环stride*2。当线程所索引到的数据对步长取余为0时，将线程所索引到的数据和与该数据距离stride的数据进行求和。最后返回该线程块的求和结果，并在cpu为所有线程块的结果求和。之所以只求得线程块的和，是因为核函数只能在每一个线程块中实现同步线程进度。具体而言，在并行归约中，循环中的每一步都需要上一步全部完成才能得到正确结果。因此如果想实现全局求和，需要同步每一个线程块的每一个线程来确认都已完成该步骤。而基于上面小节得知，不同的线程块的开始和结束是不一致的，所以在cuda编程中核函数无法同步不同线程块，甚至需要分批计算。假如某核函数需要10个线程块，但gpu只能并行计算5个线程块，这10个线程块将分成两批进行计算，也就无法为这10个线程块的所有线程达到同一时间的同步。因此核函数只能在每一个线程块中实现同步，无法跨线程块同步线程进度。在numba.cuda中，线程块内线程进度的同步通过cuda.syncthreads()来完成。 在本例的matrix_sum_reduce_numba_cuda函数中，每次循环只有线程索引为是stride的倍数的线程在进行执行，其他线程则保持静默。这样就导致在每一个线程束中只有稀疏的线程在执行，这种情况被称为线程束分化。但由于硬件设计，调度会以一整个线程束为单位进行，所以影响了程序的效率。线程束分化可以通过重新组织线程索引来解决。如本例matrix_sum_reduce_numba_cuda_v2函数所示，通过重新组织线程索引，可以保证在每一个block中一部分线程束的所有线程都在活跃，而另一部分线程束的所有线程都在静默。如在第一轮迭代，前16个线程束执行计算，后16个线程束什么都不做。通过简单整理线程索引，可以提高程序效率，如本例v2比v1执行速度提升了1.5倍。 此外，对全局内存的访问尽量进行合并访问与存储，能够达到尽量最大的带宽，也能够提升程序效率。比如在本例matrix_sum_reduce_numba_cuda_v2函数，每一循环所访问的数据索引不仅由线程索引决定，也由循环的跨度决定，即idx = 2 * stride * cuda.threadIdx.x。这就导致每次循环依据索引所访问到的数据并不连续，其跨度为stride。为了缓解这种现象，在并行归约中可以重新组织配对方法，进而让对内存的访问更加集中。如本例中matrix_sum_reduce_numba_cuda_v3所示，通过交错配对的方法，使得结果每次循环中只访问和存储到统一地址，进而提升了访问效率。如本例v3比v2执行速度提升了1.14倍。 （未完待续） 参考在撰写本文时，大量的参考了官方文档和其他博客，收益良多。观点表述如有雷同，可视为出自原作者。相关参考链接如下： numba document Python GPU快速教程 CUDA编程入门 眼过千遍，不如手过一遍。希望在阅读本文之后能够自己复现出来上述cuda代码。共勉。"},{"date":"2022-02-27T01:01:38.657Z","url":"/zh/tags/index.html","categories":[[" ",""]]},{"date":"2022-02-27T01:02:31.027Z","url":"/zh/search/index.html","categories":[[" ",""]]},{"date":"2022-02-27T01:01:34.883Z","url":"/zh/categories/index.html","categories":[[" ",""]]}]
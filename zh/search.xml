<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>深度学习模型大小与模型推理速度的探讨</title>
    <url>/zh/2022/03/04/zh/model_speed_vs_size/</url>
    <content><![CDATA[<p>在衡量深度学习的推理速度时，人们经常用深度模型的参数大小（#Param）和计算量（FLOPS）作为参考。然而简单的通过将Conv换成DepthWise
Conv或是其他结构去减少模型大小和计算量往往发现并不能有效的提升模型部署时速度。</p>
<p>本文将对衡量深度学习模型大小的一些常用指标，如计算量、参数量、访存量、内存占用等进行探讨，分析这些指标对模型部署推理的影响，尤其是计算量与访存量对模型推理速度的影响，并给出在不同硬件架构下设计网络结构的一些建议。减少网络设计与部署之间的
gap，更高效的完成网络设计与部署工作。</p>
<p>本文转载自<a
href="https://zhuanlan.zhihu.com/p/411522457">田子宸的知乎专栏</a>。</p>
<span id="more"></span>
<h2 id="常用的模型大小评估指标">常用的模型大小评估指标</h2>
<p>目前常用于评价模型大小的指标有：计算量、参数量、访存量、内存占用等，这些指标从不同维度评价了模型的大小。</p>
<h3 id="计算量">计算量</h3>
<p>计算量可以说是评价模型大小最常用的指标了，很多论文在跟 baseline
进行比较时，都会把计算量作为重要的比较依据。</p>
<p>计算量是模型所需的计算次数，反映了模型对硬件计算单元的需求。计算量一般用
OPs(Operations)，即计算次数来表示。由于最常用的数据格式为float32，因此也常常被写作FLOPs(Floating
Point
Operations)，即浮点计算次数。（这里为了跟传统习惯保持一致，下文就统一采用FLOPs啦）</p>
<p>模型的整体计算量等于模型中每个算子的计算量之和。而每个算子的计算量计算方法各不一致。例如对于
Eltwise Sum 来讲，两个大小均为 (N, C, H, W) 的 Tensor 相加，计算量就是
<span class="math inline">\(N \times C \times H \times
W\)</span>；而对于卷积来说，计算量公式为（乘加各算一次）：</p>
<p><span class="math display">\[FLOPs_{Conv2D} = N \times OC \times OH
\times OW \times KH \times KW \times IC \times 2\]</span></p>
<p>PyTorch有不少工具可以模型计算量，但需要注意的是这些工具有可能会遗漏一些算子的计算量，将其计算量算成0，从而导致统计的计算量跟实际计算量有轻微的偏差，不过大多数情况下这些偏差影响不大。</p>
<h3 id="参数量">参数量</h3>
<p>早期的论文也很喜欢用参数量来评价模型大小。</p>
<p>参数量是模型中的参数的总和，跟模型在磁盘中所需的空间大小直接相关。对于CNN来说参数主要由Conv/FC层的Weight构成，当然其他的一些算子也有参数，不过一般忽略不计了。</p>
<p>参数量往往是被算作访存量的一部分，因此参数量不直接影响模型推理性能。但是参数量一方面会影响内存占用，另一方面也会影响程序初始化的时间。</p>
<p>参数量会直接影响软件包的大小。当软件包大小是很重要的指标时，参数量至关重要，例如手机
APP 场景，往往对 APK
包的大小有比较严格的限制；此外有些嵌入式设备的Flash空间很小，如果模型磁盘所需空间很大的话，可能会放不下，因此也会对参数量有所要求。</p>
<p>除了在设计模型时减少参数量外，还可以通过压缩模型的方式降低软件包大小。例如
Caffe 和 ONNX 采用的 Protobuf
就会对模型进行高效的编码压缩。不过压缩模型会带来解压缩开销，会一定程度增加程序初始化的时间。</p>
<h3 id="访存量macs">访存量（MACs）</h3>
<p>访存量往往是最容易忽视的评价指标，但其实是现在的计算架构中对性能影响极大的指标。</p>
<p>访存量是指模型计算时所需访问存储单元的字节大小，反映了模型对存储单元带宽的需求。访存量一般用Bytes（或者
KB/MB/GB）来表示，即模型计算到底需要存/取多少Bytes的数据。</p>
<p>和计算量一样，模型整体访存量等于模型各个算子的访存量之和。对于
Eltwise Sum 来讲，两个大小均为 (N, C, H, W) 的 Tensor 相加，访存量是
<span class="math inline">\((2 + 1) \times N \times C \times H \times W
\times
sizeof(datatype)\)</span>，其中2代表读两个Tensor，1代表写一个Tensor；而对于卷积来说，访存量公式为：</p>
<p><span class="math display">\[
\begin{equation}
\begin{split}
MACs_{Conv} &amp; = MACs_{Input} + MACs_{Weights} + MACs_{Output} \\
&amp; = (N \times IC \times IH \times IW + OC \times IC \times KH \times
KW + N \times OC \times OH \times OW)\times sizeof(datatype)
\end{split}
\end{equation}
\]</span></p>
<p>访存量对模型的推理速度至关重要，设计模型时需要予以关注。</p>
<h3 id="内存占用">内存占用</h3>
<p>内存占用是指模型运行时，所占用的内存/显存大小。一般有工程意义的是最大内存占用，当然有的场景下会使用平均内存占用。这里要注意的是，内存占用
≠ 访存量。</p>
<p>内存占用在论文里不常用，主要原因是其大小除了受模型本身影响外，还受软件实现的影响。例如有的框架为了保证推理速度，会将模型中每一个
Tensor 所需的内存都提前分配好，因此内存占用为网络所有 Tensor
大小的总和；但更多的框架会提供 lite 内存模式，即动态为 Tensor
分配内存，以最大程度节省内存占用（当然可能会牺牲一部分性能）。</p>
<p>和参数量一样，内存占用不会直接影响推理速度，往往算作访存量的一部分。但在同一平台上有多个任务并发的环境下，如推理服务器、车载平台、手机
APP，往往要求内存占用可控。可控一方面是指内存/显存占用量，如果占用太多，其他任务就无法在平台上运行；另一方面是指内存/显存的占用量不会大幅波动，影响其他任务的可用性。</p>
<h3 id="小结">小结</h3>
<p>计算量、参数量、访存量、内存占用从不同维度定义了模型的大小，应根据不同的场合选用合适的指标进行评价。</p>
<p>模型推理速度不单单受模型计算量的影响，也与访存量和一些其他因素息息相关。下文将详细讨论影响模型推理速度的因素。</p>
<h2 id="计算量越小模型推理就越快吗">计算量越小，模型推理就越快吗</h2>
<p>答案是否定的。</p>
<p>实际上计算量和实际的推理速度之间没有直接的因果关系。计算量仅能作为模型推理速度的一个参考依据。</p>
<p>模型在特定硬件上的推理速度，除了受计算量影响外，还会受访存量、硬件特性、软件实现、系统环境等诸多因素影响，呈现出复杂的特性。因此，在手头有硬件且测试方便的情况下，实测是最准确的性能评估方式。</p>
<p>在设计网络结构时，如果有实测的条件，建议在模型迭代早期对性能也进行测试。一些
NAS
的方法也会对搜索出来的网络结构进行测速，或者干脆对硬件速度进行了建模，也作为初期搜索的重要参数。这种方法设计出来的网络在后期部署时，会极大减少因性能问题迭代优化的时间和人力开销。</p>
<p>这里我将讨论影响模型在硬件上推理速度的一些因素，一方面希望可以帮助手动/自动设计网络结构的同学更快的设计更高效的网络结构，另一方面希望当模型部署时性能出现问题时能够为大家提供分析原因的思路。</p>
<p>这一问题我将从如下 3 个点进行讨论：</p>
<ul>
<li>计算密度与RoofLine模型</li>
<li>计算密集型算子与访存密集型算子</li>
<li>推理时间</li>
</ul>
<h3
id="计算密度computational-intensity与roofline模型">计算密度（Computational
Intensity）与RoofLine模型</h3>
<p>计算密度I（Computational
Intensity）是指一个程序在单位访存量下所需的计算量，单位是FLOPs/Byte。其计算公式很简单，很多教材、资料里也称之为计算访存比，用于反映一个程序相对于访存来说计算的密集程度：</p>
<p><span class="math display">\[Intensity = \frac{FLOPs}{MACs}
\]</span></p>
<p>RoofLine
模型是一个用于评估程序在硬件上能达到的性能上界的模型，可用下图表示：</p>
<img data-src="/zh/2022/03/04/zh/model_speed_vs_size/Roofline-model.png" class="" title="RoofLine Model">
<p>用公式描述：</p>
<p><span class="math display">\[Performance (FLOPs/s) = \min(Intensity
\times Bandwidth, P_{peak}) \]</span></p>
<p>当程序的计算密度I较小时，程序访存多而计算少，性能受内存带宽限制，称为访存密集型程序，即图中橙色区域。在此区域的程序性能上界=计算密度×内存带宽，表现为图中的斜线，其中斜率为内存带宽的大小。计算密度越大，程序所能达到的速度上界越高，但使用的内存带宽始终为最大值。</p>
<p>反之如果计算密度I较大，程序性能受硬件最大计算峰值（下文简称为算力）限制，称为计算密集型程序，即图中蓝色区域。此时性能上界=硬件算力，表现为图中的横线。此时计算速度不受计算密度影响，但计算密度越大，所需内存带宽就越少。</p>
<p>在两条线的交点处，计算速度和内存带宽同时到达最大值。</p>
<img data-src="/zh/2022/03/04/zh/model_speed_vs_size/Roofline-gpp-knl.png" class="" title="Roofline on Intel KNL">
<img data-src="/zh/2022/03/04/zh/model_speed_vs_size/Roofline-gpp-volta.png" class="" title="Roofline on NVIDIA V100">
<p>在不同设备上，同一个程序的性质可能发生变化。例如上图中的程序2(nw=2)，在的设备Intel
KNL上属于访存密集型程序，而在设备NVIDIA
V100上就属于计算密集型程序了。如果想要充分发挥设备Intel
KNL的性能，应当适当加大程序的计算密度（比如到程序3(nw=3)的位置）。</p>
<h3
id="计算密集型算子与访存密集型算子">计算密集型算子与访存密集型算子</h3>
<p>网络中的算子可以根据计算密度进行分类。一般来讲，Conv、FC、Deconv
算子属于计算密集型算子；ReLU、EltWise Add、Concat
等属于访存密集型算子。</p>
<p>同一个算子也会因参数的不同而导致计算密度变化，甚至改变性质，比如在其他参数不变的前提下，增大
Conv 的 group，或者减小 Conv 的 input channel 都会减小计算密度。</p>
<p>举个栗子，对于不同参数的卷积，计算密度如下：</p>
<img data-src="/zh/2022/03/04/zh/model_speed_vs_size/conv_computational_intensity.png" class="" title="Conv Computational Intensity">
<p>可以看到，不同参数下卷积算子的计算密度有很大的差异。第 4 个算子
Depthwise Conv 计算密度仅有
2.346，在当下的很多设备上都属于访存密集型算子。</p>
<p>算子的计算密度越大，约有可能提升硬件的计算效率，充分发挥硬件性能。我们以一个
Intel X86 服务器平台为例（10980 XE）。该平台 CPU 频率为 4.5 GHz，我们以
16 核为例，其理论 FP32 算力为 4.608 TFLOPs/s，内存带宽理论值为 96
GB/s。在此平台上的 RoofLine 模型为：</p>
<figure>
<img
src="https://pic1.zhimg.com/80/v2-eb73ee01bad120168f52f2cc84cc638c_1440w.jpg"
alt="Intel 10980 XE 16 核 RoofLine 模型，以及各个算子的计算密度与性能" />
<figcaption aria-hidden="true">Intel 10980 XE 16 核 RoofLine
模型，以及各个算子的计算密度与性能</figcaption>
</figure>
<p>该平台“拐点”的计算密度为 48，计算较为密集的 OP1 和 OP2
处在计算密集区，能够达到平台的算力峰值；而 OP3 和 OP4
处在访存密集区，受内存带宽限制不能到达算力峰值，尤其是
OP4，由于计算访存比过低，计算效率仅有可怜的 4.9%，计算效率并不高。</p>
<h3 id="推理时间">推理时间</h3>
<p>这里涉及到一个
gap，很多部署的同学们更喜欢谈“计算效率”，而实际上算法同学真正关心的点是“推理时间”，导致两者在对接的时候经常会出现一些misleading。因此我这里单独开一节来探讨一下“推理时间”的评估方法。</p>
<p>其实也很简单，按照RoofLine模型，我们很容易就能得到算子实际的执行时间：</p>
<p><span class="math display">\[Time = \frac{FLOPs}{Performance} =
\frac{FLOPs}{\min(Intensity \times Bandwidth, P_{peak})} \]</span></p>
<p>这是一个分段函数，拆开来可得：</p>
<p><span class="math display">\[
\begin{equation}
Time = \left\{ \begin{split}
&amp;\frac{MACs}{Bandwidth} \\
&amp;\frac{FLOPs}{Performance}
\end{split}
\right.
\end{equation}
\]</span></p>
<p>一句话总结：对于访存密集型算子，推理时间跟访存量呈线性关系，而对于计算密集型算子，推理时间跟计算量呈线性关系。</p>
<p>讲到这里，我们就能初步回答本章一开始的问题了：按照 RoofLine
模型，在计算密集区，计算量越小，确实推理时间越小。但是在访存密集区，计算量与推理时间没关系，真正起作用的是访存量，访存量越小，推理的时间才越快。在全局上，计算量和推理时间并非具有线性关系。</p>
<p>上一节中，OP4
虽然计算效率很低，但由于访存量也很低，因此其实推理速度还是快于其他几个
OP 的。但是我们可以观察到，其计算量虽然只有 OP1 的
1/130，但是推理时间仅降低到了
1/6，两者并非是线性关系（也是当年我把模型减到 1/10
计算量，但其实没快多少的原因）。</p>
<p>再举两个例子强化一下，首先看这两个卷积，他们的计算量差不多，但是因为都在访存密集区，OP3
的访存量远低于 OP5，其推理也更快：</p>

<p>下面这个栗子更明显，OP5 和 OP6 的区别仅仅是一个是 DepthWise
Conv，一个是普通 Conv，其他参数没有变化。按照我们之前的直观感受，Conv
换成 DepthWise Conv 应该会更快，但实际上两者的推理时间是差不多的：</p>

<h3 id="小结-1">小结</h3>
<p>从上面的讨论中我们可以看出：计算量并不能单独用来评估模型的推理时间，还必须结合硬件特性（算力&amp;带宽），以及访存量来进行综合评估。并非是计算量越低模型推理越快。在评价模型大小时，也建议加上访存量作为重要的评价指标。</p>
<p>需要强调的一点是，不同的硬件平台峰值算力和内存带宽不同，导致同一个模型在平台
1 上可能是计算密集的，在平台 2 上可能就变成了访存密集的。例如上文提到的
Intel X86 平台，“拐点”值为 48，而 NVIDIA V100“拐点”值为
173.6，上文举的例子在 V100 平台上仅有 OP2
落在了计算密集区，剩下的全部是访存密集的。因此，同样的模型在不同平台上性质可能会发生改变，需要具体情况具体分析。</p>
<p>我们很难给出一个通用性的结论，究其原因是RoofLine模型本身是一个非线性模型。这里必须要强调一点的是，除了峰值算力和内存带宽之外，还有硬件限制、系统环境、软件实现等诸多因素会影响程序的实际性能，使得其非线性特性更加严重。因此
RoofLine
模型仅仅只能提供一个性能上界的评估方式，并不代表能够达到的实际性能。实际性能最准确的测量方式只有真机实测。</p>
<p>RoofLine
模型更重要的是提供了一种分析性能的思想，即计算密集型程序更多的受限于硬件算力，而访存密集型程序更多的受限于硬件内存带宽。在理解这一点的基础上设计网络结构，并分析网络的性能，将更有理论参考。不会再对”计算量减半，为啥推理时间没变“这种问题抱有疑问了。</p>
<p>下文将对 RoofLine
模型的一些限制进行讨论，分析哪些因素将以何种方式影响程序，使得其到达不了
RoofLine 模型估计的性能上界。</p>
<h2 id="影响模型推理性能的其他因素">影响模型推理性能的其他因素</h2>
<p>RoofLine
模型可以用来评估程序的性能上界，但是实际能达到的性能还会受到硬件限制、系统环境、软件实现等诸多因素的影响，距离性能上界有一定距离。本章将对这些影响因素进行分析。</p>
<h3 id="硬件限制对性能上界的影响">硬件限制对性能上界的影响</h3>
<p>前面 RoofLine
模型使用的峰值算力及内存带宽，是根据纸面数据计算得到的，是理论上的最大值。但在实际情况下，硬件会因为种种原因，无法达到这个理论值。因此建议大家对硬件进行micro-benchmark，以获取硬件的真实性能上限。</p>
<p>以上文的 Intel X86 CPU 为例，我们之前计算的avx512理论算力为 4.608
TFLOPs/s，但这个数值的前提是频率能维持在
4.5GHz。然而实际上在使用16核跑avx512指令时，CPU频率会下降到约
2.9GHz，此时理论算力仅剩下 2.96 TFLOPs/s，而实测值仅有 2.86
TFLOPs/s。</p>
<p>除了频率之外，有些芯片可能会因为一些设计上或实现上的原因，导致在实际使用时达不到理论峰值。比如一些低端芯片不支持多发射、不支持乱序执行、采用了阻塞式
Cache
等等，一些芯片甚至会有一些性能bug，导致在实际使用时几乎到达不了理论峰值（这里我个人倾向于把这些原因归结为硬件限制带来的损失）。</p>
<p>内存同理，该平台理论带宽为 96GB/s，但实测下来最高读带宽仅有 74
GB/s，仅能到达理论带宽的 77%。</p>
<p>我们可以得到修正后的 RoofLine
模型，图中蓝色填充部分反映了因实际算力和内存带宽达到不了理论值而造成的损失：</p>
<figure>
<img
src="https://pic1.zhimg.com/80/v2-54230ca872d8d1d4ab08f0c7fc17d9b0_1440w.jpg"
alt="修正了实测峰值算力和内存带宽后的 RoofLine 模型，蓝色填充部分为硬件限制带来的损失" />
<figcaption aria-hidden="true">修正了实测峰值算力和内存带宽后的 RoofLine
模型，蓝色填充部分为硬件限制带来的损失</figcaption>
</figure>
<p>修正后的模型“拐点”发生了变化，因此算子的性质也会发生变化。建议拿到硬件后对硬件进行
micro-benchmark，这里推荐两个测试工具：</p>
<ul>
<li><a
href="https://github.com/pigirons/cpufp">cpufp浮点峰值测试</a></li>
<li><a
href="https://www.cs.virginia.edu/stream/">stream测试工具/测试内存带宽</a></li>
</ul>
<h3 id="系统环境对性能的影响">系统环境对性能的影响</h3>
<p>除非程序运行在裸机中，否则操作系统一定会对性能上界产生一定影响，比如操作系统在多核间的调度损失、操作系统的内存管理带来的损失、操作系统本身占用的运算资源等等。</p>
<p>对于一般的深度学习推理任务而言，现代操作系统对性能的影响并不是特别明显。但是在一些特殊情况下，也会带来严重的性能损失。我这里将会举两个例子：</p>
<p>一个是 Android 系统在大小核上的调度，一旦程序在 CPU
上的占用率不足（比如是周期工作的任务），则有可能被 Android
调度到小核上，带来性能损失。</p>
<p>另一个例子是内存缺页。在 Linux
系统上，当向系统申请内存页后，系统只是返回了虚拟页，等到程序实际使用虚拟页时，才会通过触发缺页异常的方式，进入操作系统内核分配物理页，这一过程会严重降低性能。</p>
<p>好在这些问题可以通过软件进行一部分弥补，例如调度问题可以使用绑核来解决，缺页问题可以通过绑定物理页（需要内核态）或内存池来解决。因此操作系统带来的影响是可控的。</p>
<p>除了操作系统带来的影响，系统中运行的其他进程也会对当前进程造成影响。比如一个系统中运行了多个深度学习实例，或者系统后台一些
APP
自启动了等等。这些进程都会占用核心算力和内存带宽，造成当前进程性能损失。</p>
<p>这往往会导致在工程测试环境下性能达标的模型，在实际部署时性能下降。因此，必须关注工程测试环境和实际部署系统环境的差异。如有条件，最好在实际部署环境下进行测试。</p>
<h3 id="软件实现对性能的影响">软件实现对性能的影响</h3>
<p>除了硬件限制和系统环境外，一个任务的软件实现好坏对性能有着重大的影响。</p>
<p>例如对于同样的矩阵操作任务，使用 python 写的多重 for 循环，和用 numpy
高度优化过的矩阵操作函数，性能可以差出 1~2 个数量级。</p>
<p>对于深度学习模型推理而言，推理框架对模型性能的影响主要体现在：是否充分利用了硬件的流水线资源、是否高效利用了硬件中的缓存、是否采用了时间复杂度更低的算法、是否解决了操作系统带来的性能损失（如上文的调度问题和内存缺页问题）、是否进行了正确高效的图优化等等。</p>
<p>由于影响因素很多，因此软件对性能的影响往往呈现出很强的非线性，导致在评估性能时很难给出一些普适性的结论，很多时候只能具体情况具体分析。</p>
<p>例如同样计算量的向量四则运算和超越函数，后者往往会慢于前者的原因是很多硬件不支持超越函数的
SIMD 指令；再比如空洞卷积（dilated
Conv）性能会弱于普通卷积的原因是前者对访存的利用不如后者高效等等。</p>
<p>在软件实现的影响下，RoofLine
模型的上界再次下降，达到图中的红线（真实的非线性可能会比我随手画的要复杂的多）：</p>
<figure>
<img
src="https://pic3.zhimg.com/80/v2-5bb258f610e0ba693be9ddd2a27512b6_1440w.jpg"
alt="RoofLine 模型各种性能损失示意图，图中曲线不代表真实比例" />
<figcaption aria-hidden="true">RoofLine
模型各种性能损失示意图，图中曲线不代表真实比例</figcaption>
</figure>
<p>因此，在评估或分析深度学习推理性能时，简单的计算量/访存量指标是完全不够的，只能做个性能上界参考。实际能达到的性能其实还要关注很多很多因素，例如算子的访存模式、数据排布、是否能够进行图融合、是否有精度可接受的低时间复杂度算法、算法并行度是否充足、各种运算的比例等等因素。</p>
<p>这些因素对于算法同学而言可能过于复杂，并不需要掌握。但如果所在的公司/部门有交流的机会的话，可以跟部署/优化的同学针对模型结构和算子进行探讨，以获取性能优化的建议。</p>
<p>这里可以一些一般性的结论，仅供参考：</p>
<ul>
<li>对于一些访存非常密集且访存 pattern 连续的算子，如 Concat、Eltwise
Sum、ReLU、LeakyReLU、ReflectionPad 等，在 Tensor
数据量很大的情况下，软件实现的损失会非常小，正常情况下基本都能达到内存带宽实测上限；如果框架采用了融合策略的话，基本可以达到
0 开销。</li>
<li>对于 Conv/FC/Deconv
等算子，在计算密度很高的情况下，大多数框架是能够很接近算力峰值的。但对于计算密度不是特别高的
case，不同框架的表现不一，需要实测才能确定。不过从大趋势而言，都是计算密度越高，硬件的利用率越高的。</li>
<li>尽量使用常用的算子参数，例如 Conv 尽量使用 3x3_s1/s2，1x1_s1/s2
等，这些常用参数往往会被特殊优化，性能更好。</li>
</ul>
<h3 id="小结-2">小结</h3>
<p>RoofLine
模型仅能用于估计模型所能达到的性能上界，而实际部署时，还会受硬件限制、系统环境、软件实现等因素的影响，导致无法达到
RoofLine 模型所定义的性能上界。</p>
<p>此外，由于这些因素往往会导致性能曲线有较强的非线性，理论分析和实测会有一定差距，有时这些因素会严重影响性能曲线，甚至会导致算子的性质发生变化。因此本节讨论的内容只是提供一些分析的思路与技巧，实测始终是最准确的性能评估方式。</p>
<h2 id="面向推理速度的模型设计建议">面向推理速度的模型设计建议</h2>
<p>前面讨论了一大堆，其实最实用的还是“怎么设计模型能够达到更快的推理速度”。</p>
<p>在给出我的个人建议之前，首先要先声明的是：由于不同硬件、不同环境、不同框架的差异会很大，这些建议可能并不是在所有条件下都适用。在设计算法或性能测试遇到疑问时，建议咨询部署/优化的同学。</p>
<p>方法论建议：</p>
<ul>
<li>了解目标硬件的峰值算力和内存带宽，最好是实测值，用于指导网络设计和算子参数选择。</li>
<li>明确测试环境和实际部署环境的差异，最好能够在实际部署环境下测试性能，或者在测试环境下模拟实际部署环境。</li>
<li>针对不同的硬件平台，可以设计不同计算密度的网络，以在各个平台上充分发挥硬件计算能力。</li>
<li>除了使用计算量来表示/对比模型大小外，建议引入访存量、特定平台执行时间，来综合反映模型大小。</li>
<li>实测是最准确的性能评估方式，如果有条件快速实测的话，建议以实测与理论分析相结合的方式设计并迭代网络。</li>
<li>遇到性能问题时，可以逐层
profiling，并与部署/优化同学保持紧密沟通，具体问题具体分析（适当了解一下计算相关理论的话，可以更高效的沟通）。</li>
</ul>
<p>网络设计建议：</p>
<ul>
<li>对于低算力平台（CPU、低端 GPU
等），模型很容易受限于硬件计算能力，因此可以采用计算量低的网络来降低推理时间。</li>
<li>对于高算力平台（GPU、DSP
等），一味降低计算量来降低推理时间就并不可取了，往往更需要关注访存量。单纯降低计算量，很容易导致网络落到硬件的访存密集区，导致推理时间与计算量不成线性关系，反而跟访存量呈强相关（而这类硬件往往内存弱于计算）。相对于低计算密度网络而言，高计算密度网络有可能因为硬件效率更高，耗时不变乃至于更短。</li>
<li>面向推理性能设计网络结构时，尽量采用经典结构，大部分框架会对这类结构进行图优化，能够有效减少计算量与访存量。例如
Conv-&gt;BN-&gt;ReLU 就会融合成一个算子，但 Conv-&gt;ReLU-&gt;BN
就无法直接融合 BN 层</li>
<li>算子的参数尽量使用常用配置，如 Conv 尽量使用 3x3_s1/s2、1x1_s1/s2
等，软件会对这些特殊参数做特殊优化。</li>
<li>CNN 网络 channel 数尽量选择 4/8/16/32
的幂次，很多框架的很多算子实现在这样的 channel
数下效果更好（具体用多少不同平台不同框架不太一样）。</li>
<li>框架除了计算耗时外，也处理网络拓扑、内存池、线程池等开销，这些开销跟网络层数成正比。因此相比于“大而浅”的网络，“小而深”的网络这部分开销更大。一般情况下这部分开销占比不大。但在网络算子非常碎、层数非常多的时候，这部分开销有可能会影响多线程的扩展性，乃至于成为不可忽视的耗时因素。</li>
</ul>
<p>一些其他建议：</p>
<ul>
<li>除了优化网络结构、推理框架性能外，还可以考虑通过一些其他工程技巧来提升系统整体的性能。例如：对推理服务流水化，并行数据读取与计算的过程，掩盖
IO 延时。</li>
</ul>
<p>本文介绍了评估模型大小的四个常用指标——计算量、参数量、访存量、内存占用，从
RoofLine
模型入手详细讨论了影响模型推理速度的影响因素，并给出了面向推理速度的模型设计方法论与建议。</p>
<p>撰写本文的目的，不仅仅是给算法同学提供有效的网络设计建议，更多的还是希望能够传达性能优化的基础知识与分析思路，减少算法设计到部署之间的
gap，更快速高效的设计推理友好的网络模型。希望能对大家的工作有所帮助。</p>
<h2 id="有没有访存量小的模型结构">有没有访存量小的模型结构</h2>
<p>一些研究工作，例如 ShuffleNetV2，
已经在设计网络的时候兼顾访存量了。但据我所知目前还没有像 DepthWise Conv
一样经典的节省计算量的模型结构。</p>
<p>关于这个问题，我个人是这么看的：</p>
<ul>
<li>访存量可以减小，但网络精度很难保证不变，因此需要一系列的研究来探索</li>
<li>一些白给访存量的技巧可以用上，一些白白浪费访存量的操作不要搞</li>
<li>低精度/量化有的时候节省访存量的意义远大于节省计算量</li>
</ul>
<p>回顾 Xception/ MobileNet 的研究就可以看出，DWConv 3X3 + Conv 1X1
的结构之所以成为经典结构，一方面是计算量确实减少了，另一方面也是其精度确实没有太大的损失。计算量可以在设计完网络时就可以算出，但网络精度只有在网络训练完之后才能评估，需要花费大量的时间与精力反复探索才能找到这一结构。</p>
<p>一些研究确实开始关注访存量对推理速度的影响，例如 ShuffleNetV2 在选定
group 的时候就是以访存量为依据的，但并不是整体的 block
都是围绕降低访存量来设计的。由于本人很久没有关注算法的研究进展了，据我所知目前是没有专注于减少放存量的模型结构及研究工作的（如果有的话欢迎在评论区留言）。</p>
<p>我个人认为这可以成为一个很好的研究主题，可以为模型部署带来很大的帮助。一种方法是可以通过手工设计网络结构，另一种方法是可以将访存量作为
NAS
的一个参数进行搜索。前者可解释性更强一些，后者可能研究起来更容易。但是有一点请务必注意：降低访存量的最终目的一定是为了减少模型的推理时间。如果模型处在目标设备的计算密集区，降低访存量的意义有限。</p>
<p>关于实际工程部署，有一些技巧/注意的点可以保证不浪费访存量：</p>
<ul>
<li>channel 数尽量保持在 4/8/16/32 的倍数，不要设计 channel = 23
这种结构。目前大部分推理框架为了加速计算，都会用特殊的数据排布，channel
会向上 pad。比如框架会把 channel pad 到 4 的倍数，那么 channel = 23 和
24 在访存量上其实是一致的。</li>
<li>一些非常细碎乃至毫无意义的后处理算子，例如
Gather、Squeeze、Unsqueeze 等，最好给融合掉。这种现象往往见于 PyTorch
导出 onnx 的时候，可以尝试使用 onnxsim
等工具来进行融合，或者手动添加大算子。</li>
<li>尝试一些部署无感的技巧，例如蒸馏、RepVGG等。</li>
</ul>
<p>最后想聊一下低精度/量化。对于设备算力很强但模型很小的情况，低精度/量化我个人认为其降低访存量的作用要远大于节省计算量，可以有效加快模型推理速度。但是要注意两点：一个是框架如果不支持
requant，而是每次计算前都量化一次，计算完之后再反量化，那么使用低精度/量化反而会增加访存量，可能造成推理性能的下降；另一个是对于支持混合精度推理的框架，要注意不同精度转换时是否会有额外的性能开销。如果有的话，要尽量减少精度的转换。</p>
]]></content>
      <categories>
        <category>model inference</category>
      </categories>
  </entry>
  <entry>
    <title>Numba: 通过python快速学习cuda编程</title>
    <url>/zh/2020/06/10/zh/numba_cuda/</url>
    <content><![CDATA[<p>复杂的编译环境配置，指针操作和debug过程，CUDA编程对于初学者来说总是望而生畏。本文旨在通过使用python的numba库，快速学习和了解CUDA多线程高并发的编程思路。</p>
<span id="more"></span>
<h2 id="环境配置">环境配置</h2>
<p>为了简化环境配置，本文中的编程环境将全部由conda配置，并在conda的虚拟环境中测试。所涉及的依赖库分别是<code>cudatoolkit</code>,
<code>numba</code>,
<code>numpy</code>。相关环境可以通过以下语令进行配置。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda create -n numba-cuda python=3.7</span><br><span class="line">conda activate numba-cuda</span><br><span class="line">conda install cudatoolkit numba numpy</span><br></pre></td></tr></table></figure>
<p>当安装完成后，可以通过<code>nvidia-smi</code>去检查gpu状态，如下表所示。通过该表可以看到运行在每个gpu上的程序以及它们显存的使用占用，也可以通过memory-usage检查每个显卡总体显存占用。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |</span><br><span class="line">| 27%   36C    P8   112W / 250W |   6338MiB / 11175MiB |     38%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                               </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                       GPU Memory |</span><br><span class="line">|  GPU       PID   Type   Process name                             Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">|    0     32597      G   /home/yang/anaconda3/envs/ssds/bin/python   6338MiB |</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>
<p>并且也可以通过<code>python -c "from numba import cuda; print(cuda.gpus)"</code>检查是否可以成功在numba库中访问到gpu设备。当其返回为<code>&lt;Managed Device 0&gt;...</code>时则证明可在python环境下成功访问显卡。</p>
<p>当主机没有gpu设备时，依然可以通过numba提供的gpu模拟器去运行python的cuda代码，只需设置相关环境变量即可：<code>export NUMBA_ENABLE_CUDASIM=1</code>。需要注意的是该模拟器通过cpu进行模拟调试，物理上的计算单元个数远小于gpu个数。所以通过模拟器运行的程序运行速度略慢于等同cpu多线程。而且在模拟器能够运行的程序，不一定能够在gpu设备上运行。因此只能做学习使用。</p>
<p>当环境搭建成功后，接下来就开始CUDA编程之旅吧~</p>
<h2
id="hello-world-初窥gpu中的线程网格grid线程块block和线程thread">"Hello,
World!":
初窥GPU中的线程网格（Grid），线程块（Block）和线程（Thread）</h2>
<p>环境配置成功后，本小节将通过在gpu设备上执行打印"Hello,
World!"程序，来初步了解gpu编程中的基本概念以及启动gpu函数的基本流程。具体程序如下。</p>
<details>
<summary>
代码："Hello, World!"
</summary>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numba <span class="keyword">import</span> cuda</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cpu_print</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;cpu: &quot;hello world!&quot;&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@cuda.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gpu_print</span>():</span><br><span class="line">    <span class="built_in">print</span>(cuda.blockIdx.x, cuda.threadIdx.x, <span class="string">&#x27;gpu: &quot;hello world!&quot;&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    gpu_print[<span class="number">2</span>, <span class="number">4</span>]()</span><br><span class="line">    cuda.synchronize()</span><br><span class="line">    cpu_print()</span><br></pre></td></tr></table></figure>
输出: <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">0 0 gpu: <span class="string">&quot;hello world!&quot;</span></span><br><span class="line">0 1 gpu: <span class="string">&quot;hello world!&quot;</span></span><br><span class="line">0 2 gpu: <span class="string">&quot;hello world!&quot;</span></span><br><span class="line">0 3 gpu: <span class="string">&quot;hello world!&quot;</span></span><br><span class="line">1 0 gpu: <span class="string">&quot;hello world!&quot;</span></span><br><span class="line">1 1 gpu: <span class="string">&quot;hello world!&quot;</span></span><br><span class="line">1 2 gpu: <span class="string">&quot;hello world!&quot;</span></span><br><span class="line">1 3 gpu: <span class="string">&quot;hello world!&quot;</span></span><br><span class="line">cpu: <span class="string">&quot;hello world!&quot;</span></span><br></pre></td></tr></table></figure>
</details>
<p>在numba中，cuda相关的函数被封装在<code>numba.cuda</code>中，可通过<code>from numba import cuda</code>引入。如果需要将代码在gpu上执行，需要将代码封装到函数中，并为函数加装饰器<code>@cuda.jit</code>。添加@cuda.jit装饰器的函数常备称为核函数(kernal)，由cpu调用，但只在gpu设备上执行。</p>
<p>具体而言，核函数运行在gpu的最小计算单元(core)上，每次运行由单个线程（Thread）所调用，多个线程组成一个线程块（Block），多个线程块组成线程网格（Grid）。粗略的讲，通常执行单个核函数的所有线程在一个线程网络中。该线程网络控制着线程块的数量和生存周期，而每个线程块又控制着其所属的线程的个数和生存周期。运行线程的计算单元被称为流处理器（SP:
Streaming Processor），多个线程组成的线程块运行在多流处理器上（SM:
Streaming
Multiprocessor），多个线程块组成的线程网格运行在一个gpu显卡上。</p>
<p>因此在核函数被cpu调用时，需要传入该核函数所在的线程网络中线程块的总体个数，以及每个线程块的线程个数。因此在执行<code>numba.cuda</code>中的核函数时需要定义线程块和线程的个数，如<code>kernal[num_block_per_grid, num_thread_per_block]()</code>。在本例中，<code>gpu_print[2, 4]()</code>表示使用2个线程块，每个线程块中使用4个线程去并行执行该核函数。因此共有<code>2*4=8</code>个线程并行执行该核函数，也就输出了8次<code>"hello world!"</code>。</p>
<p>核函数的启动方式是异步的：启动gpu核函数后，cpu不会等待gpu核函数执行完毕才执行下一行代码。必要时，需要调用<code>cuda.synchronize()</code>，告知cpu等待gpu执行完核函数后，再进行cpu端后续计算。这个过程被称为同步。如果不调用<code>cuda.synchronize()</code>函数，执行结果也将改变，<code>cpu: "hello world!"</code>将先被打印。虽然gpu核函数调用在前，但是程序并没有等待核函数执行完，而是继续执行后面的cpu_print函数。由于cpu调用gpu有一定的延迟，反而后面的<code>cpu_print</code>先被执行，因此<code>cpu_print</code>的结果先被打印了出来。</p>
<p>需要注意的是每个执行核函数的线程都知道其所在的线程块索引(block
index)和线程索引(thread index)，以及线程块和线程的个数(grid dimension
and block
dimension)。由于索引值可以通过计算等价于循环中的索引值，因此可以较为简单的通过使用cuda并行计算来取代程序中的顺序循环。线程（块）索引和个数可以有最多三个维度，可分别通过<code>x,y,z</code>进行访问。比如，在本例中我们通过<code>cuda.blockIdx.x</code>及<code>cuda.threadIdx.x</code>来得到该线程的线程块索引和线程索引，并且将其打印。</p>
<p>下文会通过并行计算矩阵加法来初步了解cuda多线程并行计算。</p>
<h2 id="矩阵加法-小试并行计算">矩阵加法： 小试并行计算</h2>
<p>上一小节通过"Hello,
World!"程序，对gpu核函数的运行流程以及cuda编程中的线程网格（Grid），线程块（Block）和线程（Thread）有了一个基本了解。本节将通过二维矩阵加法来进一步阐述cuda编程中的线程设置以及数据拷贝。具体程序如下。</p>
<details>
<summary>
代码：矩阵加法
</summary>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numba <span class="keyword">import</span> cuda, jit</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_add_cpu</span>(<span class="params">matrix_A, matrix_B, n_rows, n_cols</span>):</span><br><span class="line">    <span class="keyword">return</span> [[matrix_A[ridx][cidx] + matrix_B[ridx][cidx] <span class="keyword">for</span> cidx <span class="keyword">in</span> <span class="built_in">range</span>(n_cols)] <span class="keyword">for</span> ridx <span class="keyword">in</span> <span class="built_in">range</span>(n_rows)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_add_numpy</span>(<span class="params">matrix_A, matrix_B, n_rows, n_cols</span>):</span><br><span class="line">    <span class="keyword">return</span> matrix_A + matrix_B</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_add_cpu_v2</span>(<span class="params">matrix_A, matrix_B, matrix_C, n_rows, n_cols</span>):</span><br><span class="line">    <span class="keyword">for</span> ridx <span class="keyword">in</span> <span class="built_in">range</span>(n_rows):</span><br><span class="line">        <span class="keyword">for</span> cidx <span class="keyword">in</span> <span class="built_in">range</span>(n_cols):</span><br><span class="line">            matrix_C[ridx][cidx] = matrix_A[ridx][cidx] + matrix_B[ridx][cidx]</span><br><span class="line">    <span class="keyword">return</span> matrix_C</span><br><span class="line"></span><br><span class="line"><span class="meta">@jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_add_numba_jit</span>(<span class="params">matrix_A, matrix_B, matrix_C, n_rows, n_cols</span>):</span><br><span class="line">    <span class="keyword">for</span> ridx <span class="keyword">in</span> <span class="built_in">range</span>(n_rows):</span><br><span class="line">        <span class="keyword">for</span> cidx <span class="keyword">in</span> <span class="built_in">range</span>(n_cols):</span><br><span class="line">            matrix_C[ridx][cidx] = matrix_A[ridx][cidx] + matrix_B[ridx][cidx]</span><br><span class="line">    <span class="keyword">return</span> matrix_C</span><br><span class="line"></span><br><span class="line"><span class="meta">@cuda.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_add_numba_cuda</span>(<span class="params">matrix_A, matrix_B, matrix_C, n_rows, n_cols</span>):</span><br><span class="line">    ridx = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y</span><br><span class="line">    cidx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ridx &lt; n_rows <span class="keyword">and</span> cidx &lt; n_cols:</span><br><span class="line">        matrix_C[ridx][cidx] = matrix_A[ridx][cidx] + matrix_B[ridx][cidx]</span><br><span class="line"></span><br><span class="line"><span class="meta">@cuda.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_add_numba_cuda_copy</span>(<span class="params">matrix_A, matrix_B, matrix_C, n_rows, n_cols</span>):</span><br><span class="line">    ridx = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y</span><br><span class="line">    cidx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ridx &lt; n_rows <span class="keyword">and</span> cidx &lt; n_cols:</span><br><span class="line">        matrix_C[ridx][cidx] = matrix_A[ridx][cidx] + matrix_B[ridx][cidx]</span><br><span class="line"></span><br><span class="line"><span class="meta">@cuda.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_add_numba_cuda_v2</span>(<span class="params">matrix_A, matrix_B, matrix_C, n_rows, n_cols</span>):</span><br><span class="line">    idx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x</span><br><span class="line">    ridx = idx // n_cols</span><br><span class="line">    cidx = idx %  n_cols</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ridx &lt; n_rows <span class="keyword">and</span> cidx &lt; n_cols:</span><br><span class="line">        matrix_C[ridx][cidx] = matrix_A[ridx][cidx] + matrix_B[ridx][cidx]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    n_row, n_col = <span class="number">1080</span>, <span class="number">1920</span></span><br><span class="line"></span><br><span class="line">    arr_A, arr_B = [[[random.random() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_col)] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_row)] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)]</span><br><span class="line">    elapsed_time = time.time()</span><br><span class="line">    arr_cpu = matrix_add_cpu(arr_A, arr_B, n_row, n_col)</span><br><span class="line">    elapsed_time = time.time() - elapsed_time</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process time on cpu : &#123;:.03f&#125;ms&quot;</span>.<span class="built_in">format</span>(elapsed_time * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    arr_A, arr_B = np.array(arr_A), np.array(arr_B)</span><br><span class="line">    elapsed_time = time.time()</span><br><span class="line">    arr_numpy = matrix_add_numpy(arr_A, arr_B, n_row, n_col)</span><br><span class="line">    elapsed_time = time.time() - elapsed_time</span><br><span class="line">    <span class="keyword">if</span> (arr_numpy - np.array(arr_cpu)).<span class="built_in">max</span>() &lt; <span class="number">1e-8</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process time on numpy : &#123;:.03f&#125;ms&quot;</span>.<span class="built_in">format</span>(elapsed_time * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    arr_C = arr_B.copy()</span><br><span class="line">    elapsed_time = time.time()</span><br><span class="line">    arr_cpu_v2 = matrix_add_cpu_v2(arr_A, arr_B, arr_C, n_row, n_col)</span><br><span class="line">    elapsed_time = time.time() - elapsed_time</span><br><span class="line">    <span class="keyword">if</span> (arr_numpy - arr_cpu_v2).<span class="built_in">max</span>() &lt; <span class="number">1e-8</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process time on cpu v2 : &#123;:.03f&#125;ms&quot;</span>.<span class="built_in">format</span>(elapsed_time * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    elapsed_time = time.time()</span><br><span class="line">    arr_n_jit = matrix_add_numba_jit(arr_A, arr_B, arr_C, n_row, n_col)</span><br><span class="line">    elapsed_time = time.time() - elapsed_time</span><br><span class="line">    <span class="keyword">if</span> (arr_numpy - arr_n_jit).<span class="built_in">max</span>() &lt; <span class="number">1e-8</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process time on numba jit : &#123;:.03f&#125;ms&quot;</span>.<span class="built_in">format</span>(elapsed_time * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    threadsperblock = (<span class="number">16</span>, <span class="number">16</span>)</span><br><span class="line">    blockspergrid_x = math.ceil(n_col / threadsperblock[<span class="number">0</span>])</span><br><span class="line">    blockspergrid_y = math.ceil(n_row / threadsperblock[<span class="number">1</span>])</span><br><span class="line">    blockspergrid = (blockspergrid_x, blockspergrid_y)</span><br><span class="line">    elapsed_time = time.time()</span><br><span class="line">    matrix_add_numba_cuda[blockspergrid, threadsperblock](arr_A, arr_B, arr_C, n_row, n_col)</span><br><span class="line">    cuda.synchronize()</span><br><span class="line">    elapsed_time = time.time() - elapsed_time</span><br><span class="line">    <span class="keyword">if</span> (arr_numpy - arr_C).<span class="built_in">max</span>() &lt; <span class="number">1e-8</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process time on numba cuda : &#123;:.03f&#125;ms&quot;</span>.<span class="built_in">format</span>(elapsed_time * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    d_arr_A, d_arr_B, d_arr_C = cuda.to_device(arr_A), cuda.to_device(arr_B), cuda.to_device(arr_C)</span><br><span class="line">    cuda.synchronize()</span><br><span class="line">    elapsed_time = time.time()</span><br><span class="line">    matrix_add_numba_cuda_copy[blockspergrid, threadsperblock](d_arr_A, d_arr_B, d_arr_C, n_row, n_col)</span><br><span class="line">    cuda.synchronize()</span><br><span class="line">    elapsed_time = time.time() - elapsed_time</span><br><span class="line">    arr_C = d_arr_C.copy_to_host()</span><br><span class="line">    cuda.synchronize()</span><br><span class="line">    <span class="keyword">if</span> (arr_numpy - arr_C).<span class="built_in">max</span>() &lt; <span class="number">1e-8</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process time on numba cuda with copied data : &#123;:.03f&#125;ms&quot;</span>.<span class="built_in">format</span>(elapsed_time * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    threadsperblock = <span class="number">256</span></span><br><span class="line">    blockspergrid   = math.ceil((n_col*n_row)/threadsperblock)</span><br><span class="line">    elapsed_time = time.time()</span><br><span class="line">    matrix_add_numba_cuda_v2[blockspergrid, threadsperblock](d_arr_A, d_arr_B, d_arr_C, n_row, n_col)</span><br><span class="line">    cuda.synchronize()</span><br><span class="line">    elapsed_time = time.time() - elapsed_time</span><br><span class="line">    arr_C = d_arr_C.copy_to_host()</span><br><span class="line">    cuda.synchronize()</span><br><span class="line">    <span class="keyword">if</span> (arr_numpy - arr_C).<span class="built_in">max</span>() &lt; <span class="number">1e-8</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process time on numba cuda with copied data : &#123;:.03f&#125;ms&quot;</span>.<span class="built_in">format</span>(elapsed_time * <span class="number">1000</span>))</span><br></pre></td></tr></table></figure>
<p>输出1: <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">process time on cpu : 203.750ms</span><br><span class="line">process time on numpy : 2.630ms</span><br><span class="line">process time on cpu v2 : 1334.258ms</span><br><span class="line">process time on numba jit : 235.476ms</span><br><span class="line">process time on numba cuda : 217.982ms</span><br><span class="line">process time on numba cuda with copied data : 76.042ms</span><br><span class="line">process time on numba cuda v2 with copied data : 75.180ms</span><br></pre></td></tr></table></figure></p>
输出2： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">process time on cpu : 245.238ms</span><br><span class="line">process time on numpy : 2.766ms</span><br><span class="line">process time on cpu v2 : 1410.334ms</span><br><span class="line">process time on numba jit (1st): 234.586ms</span><br><span class="line">process time on numba jit (2nd): 3.086ms</span><br><span class="line">process time on numba cuda (1st): 234.172ms</span><br><span class="line">process time on numba cuda (2nd): 15.431ms</span><br><span class="line">process time on numba cuda with copied data (1st): 80.449ms</span><br><span class="line">process time on numba cuda with copied data (2nd): 0.642ms</span><br><span class="line">process time on numba cuda v2 with copied data (1st): 78.265ms</span><br><span class="line">process time on numba cuda v2 with copied data (2nd): 0.538ms</span><br></pre></td></tr></table></figure>
</details>
<p>当计算独立时，并行的cuda核函数可以简单取代程序中的循环并实现巨大的提速。在本例中，纯python编程想要计算两矩阵相加，必须通过循环来遍历矩阵上的所有点来进行计算。正如上一小节所讲，在cuda核函数中的索引值等价于循环中的索引值，python程序中的通过顺序循环遍历矩阵就可以被cuda中的多线程核函数并行遍历矩阵所取代。在核函数中，线程可以通过计算索引来访问到对应的点的位置。比如，循环中的行和列的索引值<code>ridx</code>,<code>cidx</code>
在核函数可通过线程（块）索引和个数来计算得到<code>ridx = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y</code>，<code>cidx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x</code>。如程序所示，该核函数中只计算了矩阵中一个点计算加法。因此在执行该gpu函数时，每个线程只为矩阵中一个点计算加法，多个线程并行运行，进而实现了快速高并发的并行计算。可以看到，相较于纯python编程，cuda并行计算可提速3倍到18倍。但有一点值得奇怪，如果该函数在运行第二次时，其速度为0.8ms左右，也就是存在264～1654倍的提速。所以可能在numba中第一遍速度较慢是由于numba的动态编译所导致的，具体原因尚需细查。</p>
<p>上小节提到线程块运行在多流处理器上，线程运行在流处理器上。在物理显卡中，多流处理器所包含的流处理器的数量是固定的，对目前市场上的主流显卡，多流处理器一般包含1024或512个流处理器。因此，在设置线程个数时，必须小于等于每个多流处理器包含流处理器的个数，也就是要小于或等于1024或512。所以一般线程个数在程序中常作为常量存在，如在本例线程的个数被固定为<code>(16, 16)</code>，也就是256个线程。
并且，多流处理器在物理上通过线程束（warp）来分组调度流处理器，每个线程束调度的线程个数也是固定的，一般为32个。每个线程束中所有流处理器（32个）是一起工作的，执行相同的指令。如果没有这么多的流处理器需要工作，那么这个线程束中的一些流处理器是不工作的。为了使每个线程束中所有的线程都在工作，当进行并行计算时，线程数尽量为32的倍数。如在本例中，线程的个数是256（<code>(16, 16)</code>），也就是32的8倍。如果线程数设置为1的话，线程束会生成掩码，使得32个线程中只有一个线程在真正执行，其他31个线程会进入静默状态。这样计算资源就会浪费，在计算量较大时，执行效率就明显变低了。</p>
<p>由于线程的个数在程序中常为常量，为了能够取代所有循环，一般线程块的个数可以由总循环次数除以线程个数，并向上取整。如在本例中，为了能够遍历到矩阵中所有点，线程块个数被设置为<code>blockspergrid_x = math.ceil(n_col / threadsperblock[0])</code>,
<code>blockspergrid_y = math.ceil(n_row / threadsperblock[1])</code>，在本例中也就是<code>(120,68)</code>，也就是8160个线程块。但向上取整存在一个问题，通过该设置所生成的线程个数大于本身的循环次数，即<code>(8160*256 &gt; 1920*1080)</code>，所以在核函数中一般需要进行判断来防止索引溢出。如本例中的<code>if ridx &lt; n_rows and cidx &lt; n_cols:</code>。上小节将到了线程（块）个数最多可定义为三个维度，但每个维度都有可能向上取整，维度越多造成资源浪费的可能性越大。这也就导致了虽然多维线程（块）个数在编程中更容易理解，但为了减少资源浪费的可能，在cuda编程中更建议用单个维度（一维）来定义线程（块）个数。</p>
<p>上小节说，核函数被cpu调用，被gpu执行。再延伸一点，核函数被gpu执行所调用的数据需存储在gpu显存中而非cpu内存中。所以在gpu编程中，常常需要先将输入数据从cpu内存拷贝到gpu显存中，也需要将gpu输出结果从gpu显存拷贝回cpu内存中。在<code>numba.cuda</code>中，如果发现传入数据在cpu内存上时，在执行核函数前，会自动将所有输入数据从cpu内存拷贝到gpu显存中，并在核函数结束后，将所有输入数据从gpu显存同步回cpu内存。但这样就导致了不必要的拷贝和额外的时间开销，如在本例中<code>matrix_A</code>和<code>matrix_B</code>的是不需要拷贝回cpu内存的。所以在cuda编程中，建议通过<code>cuda.to_device()</code>将输入数据从cpu内存拷贝到gpu显存中，结果并将结果通过<code>arr.copy_to_host()</code>从gpu显存拷贝回到cpu内存中。如果想要避免结果数组从cpu内存拷贝到gpu显存造成的额外开销，也可以通过<code>cuda.device_array()</code>在gpu显存中一个空向量来实现。在本例中，在执行核函数时，手动拷贝数据的执行效率要比自动拷贝数据效率要高的多。</p>
<p>此外，因为gpu的传输带宽较大，一般多次传输小数据的速率要低于单次传输同等数据的速率。所以在cpu与gpu相互拷贝数据时，尽量采用单次传输。但当数据较大时，gpu无法同时计算所有的数据，这时分批拷贝并计算就可以进一步提速cuda核函数。</p>
<p>在下一小节，将通过分批处理来进一步优化矩阵加法，并阐述了gpu编程中流(stream)的概念。</p>
<h2 id="分批处理矩阵流stream">分批处理矩阵：流(stream)</h2>
<p>上节讲到，当数据计算量较大时，gpu无法同时计算所有的数据，这时候gpu可以将计算任务分批放在一个队列中，排队顺序执行。这种按照队列顺序流水线处理的操作叫做流(stream)。这一小节依然进行矩阵加法，但将计算量是上一小节的100倍，并采用了流进行分批处理。具体代码如下。</p>
<details>
<summary>
代码：矩阵加法-多流
</summary>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numba <span class="keyword">import</span> cuda</span><br><span class="line"></span><br><span class="line"><span class="meta">@cuda.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_add_numba_cuda</span>(<span class="params">matrix_A, matrix_B, matrix_C, n_rows, n_cols</span>):</span><br><span class="line">    idx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x</span><br><span class="line">    ridx = idx // n_cols</span><br><span class="line">    cidx = idx %  n_cols</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ridx &lt; n_rows <span class="keyword">and</span> cidx &lt; n_cols:</span><br><span class="line">        matrix_C[ridx][cidx] = matrix_A[ridx][cidx] + matrix_B[ridx][cidx]</span><br><span class="line"></span><br><span class="line"><span class="meta">@cuda.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_add_numba_cuda_queue</span>(<span class="params">matrix_A, matrix_B, matrix_C, n_rows, n_cols</span>):</span><br><span class="line">    idx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x</span><br><span class="line">    ridx = idx // n_cols</span><br><span class="line">    cidx = idx %  n_cols</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ridx &lt; n_rows <span class="keyword">and</span> cidx &lt; n_cols:</span><br><span class="line">        matrix_C[ridx][cidx] = matrix_A[ridx][cidx] + matrix_B[ridx][cidx]</span><br><span class="line"></span><br><span class="line"><span class="meta">@cuda.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_add_numba_cuda_stream</span>(<span class="params">matrix_A, matrix_B, matrix_C, n_rows, n_cols</span>):</span><br><span class="line">    idx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x</span><br><span class="line">    ridx = idx // n_cols</span><br><span class="line">    cidx = idx %  n_cols</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ridx &lt; n_rows <span class="keyword">and</span> cidx &lt; n_cols:</span><br><span class="line">        matrix_C[ridx][cidx] = matrix_A[ridx][cidx] + matrix_B[ridx][cidx]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    n_row, n_col = <span class="number">10800</span>, <span class="number">19200</span></span><br><span class="line"></span><br><span class="line">    arr_A, arr_B, arr_C = np.random.random((n_row, n_col)), np.random.random((n_row, n_col)), np.random.random((n_row, n_col))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># basic one time</span></span><br><span class="line">    threadsperblock = <span class="number">256</span></span><br><span class="line">    blockspergrid   = math.ceil((n_col*n_row)/threadsperblock)</span><br><span class="line"></span><br><span class="line">    elapsed_time = time.time()</span><br><span class="line">    d_arr_A, d_arr_B, d_arr_C = cuda.to_device(arr_A), cuda.to_device(arr_B), cuda.to_device(arr_C)</span><br><span class="line">    matrix_add_numba_cuda[blockspergrid, threadsperblock](d_arr_A, d_arr_B, d_arr_C, n_row, n_col)</span><br><span class="line">    cuda.synchronize()</span><br><span class="line">    arr_result = d_arr_C.copy_to_host()</span><br><span class="line">    cuda.synchronize()</span><br><span class="line">    elapsed_time = time.time() - elapsed_time</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process time on numba cuda : &#123;:.03f&#125;ms&quot;</span>.<span class="built_in">format</span>(elapsed_time * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># queue</span></span><br><span class="line">    n_streams     = <span class="number">4</span></span><br><span class="line">    seg_row       = n_row // n_streams</span><br><span class="line">    blockspergrid = math.ceil((seg_row* n_col) / threadsperblock)</span><br><span class="line"></span><br><span class="line">    elapsed_time = time.time()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n_streams):</span><br><span class="line">        d_arr_A = cuda.to_device(arr_A[i * seg_row : (i + <span class="number">1</span>) * seg_row])</span><br><span class="line">        d_arr_B = cuda.to_device(arr_B[i * seg_row : (i + <span class="number">1</span>) * seg_row])</span><br><span class="line">        cuda.synchronize()</span><br><span class="line">        matrix_add_numba_cuda_queue[blockspergrid, threadsperblock](</span><br><span class="line">                d_arr_A,</span><br><span class="line">                d_arr_B,</span><br><span class="line">                d_arr_C[i * seg_row : (i + <span class="number">1</span>) * seg_row],</span><br><span class="line">                seg_row,</span><br><span class="line">                n_col)</span><br><span class="line">        cuda.synchronize()</span><br><span class="line">        arr_C[i * seg_row : (i + <span class="number">1</span>) * seg_row] = d_arr_C[i * seg_row : (i + <span class="number">1</span>) * seg_row].copy_to_host()</span><br><span class="line">        cuda.synchronize()</span><br><span class="line">    elapsed_time = time.time() - elapsed_time</span><br><span class="line">    <span class="keyword">if</span> (arr_result - arr_C).<span class="built_in">max</span>() &lt; <span class="number">1e-8</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process time on numba cuda queue: &#123;:.03f&#125;ms&quot;</span>.<span class="built_in">format</span>(elapsed_time * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># stream</span></span><br><span class="line">    stream_list   = <span class="built_in">list</span>()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">0</span>, n_streams):</span><br><span class="line">        stream = cuda.stream()</span><br><span class="line">        stream_list.append(stream)</span><br><span class="line"></span><br><span class="line">    elapsed_time = time.time()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n_streams):</span><br><span class="line">        d_arr_A = cuda.to_device(arr_A[i * seg_row : (i + <span class="number">1</span>) * seg_row], stream=stream_list[i])</span><br><span class="line">        d_arr_B = cuda.to_device(arr_B[i * seg_row : (i + <span class="number">1</span>) * seg_row], stream=stream_list[i])</span><br><span class="line">        matrix_add_numba_cuda_stream[blockspergrid, threadsperblock, stream_list[i]](</span><br><span class="line">                d_arr_A,</span><br><span class="line">                d_arr_B,</span><br><span class="line">                d_arr_C[i * seg_row : (i + <span class="number">1</span>) * seg_row],</span><br><span class="line">                seg_row,</span><br><span class="line">                n_col)</span><br><span class="line">        arr_C[i * seg_row : (i + <span class="number">1</span>) * seg_row] = d_arr_C[i * seg_row : (i + <span class="number">1</span>) * seg_row].copy_to_host(stream=stream_list[i])</span><br><span class="line">    cuda.synchronize()</span><br><span class="line">    elapsed_time = time.time() - elapsed_time</span><br><span class="line">    <span class="keyword">if</span> (arr_result - arr_C).<span class="built_in">max</span>() &lt; <span class="number">1e-8</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process time on numba cuda stream: &#123;:.03f&#125;ms&quot;</span>.<span class="built_in">format</span>(elapsed_time * <span class="number">1000</span>))</span><br></pre></td></tr></table></figure>
<p>输出1: <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">process time on numba cuda : 1180.036ms</span><br><span class="line">process time on numba cuda queue: 1019.919ms</span><br><span class="line">process time on numba cuda stream: 973.997ms</span><br></pre></td></tr></table></figure></p>
<p>输出2： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">process time on numba cuda (1st): 1187.603ms</span><br><span class="line">process time on numba cuda (2nd): 883.670ms</span><br><span class="line">process time on numba cuda queue (1st): 1064.817ms</span><br><span class="line">process time on numba cuda queue (2nd): 932.134ms</span><br><span class="line">process time on numba cuda stream (1st): 959.764ms</span><br><span class="line">process time on numba cuda stream (2nd): 887.833ms</span><br></pre></td></tr></table></figure></p>
</details>
<pre class="mermaid">
gantt
title Default vs Queue vs Stream
dateFormat  ss
   axisFormat  %S
section Default
Host2Device      : a1, 0s, 3s
Kernal           : a2, after a1 , 3s
   Device2Host      : after a2 , 3s

section Queue
Host2Device1     : 0s, 1s
Kernal1          : 1s
   Device2Host1     : 1s
Host2Device2     : 1s
Kernal2          : 1s
   Device2Host2     : 1s
Host2Device3     : 1s
Kernal3          : 1s
   Device2Host3     : 1s

section Stream
Host2Device1     : b1, 0s, 1s
Kernal1          : b2, after b1 , 1s
   Device2Host1     : after b2 , 1s
Host2Device2     : after b1,  1s
Kernal2          : b3, after b2, 1s
   Device2Host2     : after b3 , 1s
Host2Device3     : after b2,  1s
Kernal3          : b4, after b3, 1s
   Device2Host3     : after b4 , 1s
</pre>
<p>由于gpu的硬件特性，cuda中的很多操作是相互独立的，比如核函数的计算，cpu内存与gpu显存间的相互拷贝等。针对这种互相独立的硬件架构，cuda使用多流(multistream)作为一种高并发的方案：把一个大任务拆分开放到多个流中，每次只对一部分数据进行拷贝、计算和回写，并把这个流程做成流水线。这样数据拷贝和核函数计算重叠的时间是重叠的，进而获得性能的提升。如在本例中，通过cuda流处理比简单的队列处理要略快一些。</p>
<p>既然要形成多流的队列，那么队列中的每一步操作都需要知道自己在流。因此在cuda编程中，需要先创建每个流的对象，再把流对象赋值给每一步操作。在numba中，流对象可通过<code>cuda.stream()</code>来创建；执行核函数时，需要与线程块和线程个数一起定义，如<code>kernal[num_block_per_grid, num_thread_per_block, stream]()</code>；而在拷贝数据时，也需要将其加入拷贝函数，如<code>cuda.to_device(stream=stream)</code>和<code>arr.copy_to_host(stream=stream)</code>。这样，每个流水线可以知道自己所需要执行的步骤了。当不指定具体流对象时，这些操作会在默认的流上执行。</p>
<p>每个流水线是顺序执行的，但非默认的流水线是异步操作的，也就是说先创建的流水线可能后完成。默认流有阻塞的作用。如果调用默认流，那么默认流会等非默认流都执行完才能执行；同样，默认流执行完，才能再次执行其他非默认流。另外，流不宜分配过多，当流过多时，有可能在每个流水线上的核函数计算时间代价会小于单次数据拷贝的时间（如内存到显存或显存到内存）。由于每种操作自身时不独立的，所以其必须等待其他的流完成数据拷贝操作才可以开始自己的数据拷贝操作。这样反而会导致流水线处理的执行效率变低。在实践中，可能需要硬件设备的执行效率和核函数的计算量来微调流的个数，进而达到相对好的表现。</p>
<p>关于矩阵加法的例子先到这里，下面的小节将通过矩阵求和，来阐述并行归约（Reduction）算法及其优化过程中的线程束分化和内存访问。</p>
<h2 id="矩阵求和并行归约reduction">矩阵求和：并行归约（Reduction）</h2>
<p>在之前的矩阵加法中，每个点的计算是相互独立的，各个线程不需要考虑其他线程的计算结果，所以其循环替代和核函数计算相对简单直观。但在很多任务中，循环需要依赖上一步的执行结果，替换此类核函数相对复杂了。本小节将引入矩阵求和，来简单介绍通过并行归约算法解决二元操作问题。其代码如下：</p>
<details>
<summary>
代码：矩阵求和
</summary>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numba <span class="keyword">import</span> cuda, jit</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_sum_numpy</span>(<span class="params">matrix, n_rows, n_cols</span>):</span><br><span class="line">    <span class="keyword">return</span> matrix.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_sum_cpu</span>(<span class="params">matrix, n_rows, n_cols</span>):</span><br><span class="line">    res = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> ridx <span class="keyword">in</span> <span class="built_in">range</span>(n_rows):</span><br><span class="line">        <span class="keyword">for</span> cidx <span class="keyword">in</span> <span class="built_in">range</span>(n_cols):</span><br><span class="line">            res += matrix[ridx][cidx]</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="meta">@jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_sum_numba_jit</span>(<span class="params">matrix, n_rows, n_cols</span>):</span><br><span class="line">    res = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> ridx <span class="keyword">in</span> <span class="built_in">range</span>(n_rows):</span><br><span class="line">        <span class="keyword">for</span> cidx <span class="keyword">in</span> <span class="built_in">range</span>(n_cols):</span><br><span class="line">            res += matrix[ridx][cidx]</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="meta">@cuda.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_sum_reduce_numba_cuda</span>(<span class="params">matrix, res, max_stride_iter, n_rows, n_cols</span>):</span><br><span class="line">    idx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x</span><br><span class="line">    volume = n_rows * n_cols</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> idx &lt; volume:</span><br><span class="line">        ridx = idx // n_cols</span><br><span class="line">        cidx = idx %  n_cols</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(max_stride_iter):</span><br><span class="line">            stride = <span class="number">2</span>**s</span><br><span class="line">            <span class="keyword">if</span> (cuda.threadIdx.x % (<span class="number">2</span> * stride)) == <span class="number">0</span>:</span><br><span class="line">                idx_s = idx + stride</span><br><span class="line">                <span class="keyword">if</span> idx_s &lt; volume:</span><br><span class="line">                    ridx_s = idx_s // n_cols</span><br><span class="line">                    cidx_s = idx_s %  n_cols</span><br><span class="line">                    matrix[ridx][cidx] += matrix[ridx_s][cidx_s]</span><br><span class="line">            cuda.syncthreads()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cuda.threadIdx.x == <span class="number">0</span>:</span><br><span class="line">        res[cuda.blockIdx.x] = matrix[ridx][cidx]</span><br><span class="line"></span><br><span class="line"><span class="meta">@cuda.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_sum_reduce_numba_cuda_v2</span>(<span class="params">matrix, res, max_stride_iter, n_rows, n_cols</span>):</span><br><span class="line">    blockIdx = cuda.blockDim.x * cuda.blockIdx.x</span><br><span class="line">    idx = cuda.threadIdx.x + blockIdx</span><br><span class="line">    volume = n_rows * n_cols</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> idx &lt; volume:</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(max_stride_iter):</span><br><span class="line">            stride = <span class="number">2</span>**s</span><br><span class="line">            idx =  <span class="number">2</span> * stride * cuda.threadIdx.x</span><br><span class="line">            <span class="keyword">if</span> idx &lt; cuda.blockDim.x:</span><br><span class="line">                idx = idx + blockIdx</span><br><span class="line">                idx_s = idx + stride</span><br><span class="line">                <span class="keyword">if</span> idx &lt; volume <span class="keyword">and</span> idx_s &lt; volume:</span><br><span class="line">                    ridx = idx // n_cols</span><br><span class="line">                    cidx = idx %  n_cols</span><br><span class="line">                    ridx_s = idx_s // n_cols</span><br><span class="line">                    cidx_s = idx_s %  n_cols</span><br><span class="line">                    matrix[ridx][cidx] += matrix[ridx_s][cidx_s]</span><br><span class="line">            cuda.syncthreads()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cuda.threadIdx.x == <span class="number">0</span>:</span><br><span class="line">        res[cuda.blockIdx.x] = matrix[ridx][cidx]</span><br><span class="line"></span><br><span class="line"><span class="meta">@cuda.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_sum_reduce_numba_cuda_v3</span>(<span class="params">matrix, res, max_stride_iter, n_rows, n_cols</span>):</span><br><span class="line">    idx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x</span><br><span class="line">    volume = n_rows * n_cols</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> idx &lt; volume:</span><br><span class="line">        ridx = idx // n_cols</span><br><span class="line">        cidx = idx %  n_cols</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_stride_iter):</span><br><span class="line">            stride = cuda.blockDim.x // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> cuda.threadIdx.x &lt; stride:</span><br><span class="line">                idx_s = idx + stride</span><br><span class="line">                <span class="keyword">if</span> idx_s &lt; volume:</span><br><span class="line">                    ridx_s = idx_s // n_cols</span><br><span class="line">                    cidx_s = idx_s %  n_cols</span><br><span class="line">                    matrix[ridx][cidx] += matrix[ridx_s][cidx_s]</span><br><span class="line">            cuda.syncthreads()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cuda.threadIdx.x == <span class="number">0</span>:</span><br><span class="line">        res[cuda.blockIdx.x] = matrix[ridx][cidx]</span><br><span class="line"></span><br><span class="line"><span class="meta">@cuda.reduce</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_sum_reduce_numba_cuda_reduce</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">return</span> a + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    n_row, n_col = <span class="number">1080</span>, <span class="number">1920</span></span><br><span class="line">    arr = np.random.random((n_row, n_col))</span><br><span class="line"></span><br><span class="line">    elapsed_time = time.time()</span><br><span class="line">    res_numpy = matrix_sum_numpy(arr, n_row, n_col)</span><br><span class="line">    elapsed_time = time.time() - elapsed_time</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process time on numpy : &#123;:.03f&#125;ms&quot;</span>.<span class="built_in">format</span>(elapsed_time * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    elapsed_time = time.time()</span><br><span class="line">    res_cpu = matrix_sum_cpu(arr, n_row, n_col)</span><br><span class="line">    elapsed_time = time.time() - elapsed_time</span><br><span class="line">    <span class="keyword">if</span> (res_numpy - res_cpu) &lt; <span class="number">1e-6</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process time on cpu : &#123;:.03f&#125;ms&quot;</span>.<span class="built_in">format</span>(elapsed_time * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    elapsed_time = time.time()</span><br><span class="line">    res_n_jit = matrix_sum_numba_jit(arr, n_row, n_col)</span><br><span class="line">    elapsed_time = time.time() - elapsed_time</span><br><span class="line">    <span class="keyword">if</span> (res_numpy - res_n_jit) &lt; <span class="number">1e-6</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process time on numba jit : &#123;:.03f&#125;ms&quot;</span>.<span class="built_in">format</span>(elapsed_time * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># numba cuda v1, v2, v3 and reduce version</span></span><br><span class="line">    threadsperblock = <span class="number">1024</span></span><br><span class="line">    blockspergrid   = math.ceil((n_col*n_row)/threadsperblock)</span><br><span class="line">    max_stride_iter = math.ceil(math.log2(threadsperblock))</span><br><span class="line">    block_res_n_cuda = np.zeros(blockspergrid)</span><br><span class="line">    d_arr = cuda.to_device(arr)</span><br><span class="line">    d_block_res_n_cuda = cuda.to_device(block_res_n_cuda)</span><br><span class="line">    cuda.synchronize()</span><br><span class="line"></span><br><span class="line">    elapsed_time = time.time()</span><br><span class="line">    matrix_sum_reduce_numba_cuda[blockspergrid, threadsperblock](d_arr, d_block_res_n_cuda, max_stride_iter, n_row, n_col)</span><br><span class="line">    cuda.synchronize()</span><br><span class="line">    block_res_n_cuda = d_block_res_n_cuda.copy_to_host()</span><br><span class="line">    cuda.synchronize()</span><br><span class="line">    res_n_cuda = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> res <span class="keyword">in</span> block_res_n_cuda:</span><br><span class="line">        res_n_cuda += res</span><br><span class="line">    elapsed_time = time.time() - elapsed_time</span><br><span class="line">    <span class="keyword">if</span> (res_numpy - res_n_cuda) &lt; <span class="number">1e-6</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process time on numba cuda: &#123;:.03f&#125;ms&quot;</span>.<span class="built_in">format</span>(elapsed_time * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    elapsed_time = time.time()</span><br><span class="line">    matrix_sum_reduce_numba_cuda_v2[blockspergrid, threadsperblock](d_arr, d_block_res_n_cuda, max_stride_iter, n_row, n_col)</span><br><span class="line">    cuda.synchronize()</span><br><span class="line">    block_res_n_cuda = d_block_res_n_cuda.copy_to_host()</span><br><span class="line">    cuda.synchronize()</span><br><span class="line">    res_n_cuda = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> res <span class="keyword">in</span> block_res_n_cuda:</span><br><span class="line">        res_n_cuda += res</span><br><span class="line">    elapsed_time = time.time() - elapsed_time</span><br><span class="line">    <span class="keyword">if</span> (res_numpy - res_n_cuda) &lt; <span class="number">1e-6</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process time on numba cuda v2: &#123;:.03f&#125;ms&quot;</span>.<span class="built_in">format</span>(elapsed_time * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    elapsed_time = time.time()</span><br><span class="line">    matrix_sum_reduce_numba_cuda_v3[blockspergrid, threadsperblock](d_arr, d_block_res_n_cuda, max_stride_iter, n_row, n_col)</span><br><span class="line">    cuda.synchronize()</span><br><span class="line">    block_res_n_cuda = d_block_res_n_cuda.copy_to_host()</span><br><span class="line">    cuda.synchronize()</span><br><span class="line">    res_n_cuda = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> res <span class="keyword">in</span> block_res_n_cuda:</span><br><span class="line">        res_n_cuda += res</span><br><span class="line">    elapsed_time = time.time() - elapsed_time</span><br><span class="line">    <span class="keyword">if</span> (res_numpy - res_n_cuda) &lt; <span class="number">1e-6</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process time on numba cuda v3: &#123;:.03f&#125;ms&quot;</span>.<span class="built_in">format</span>(elapsed_time * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    arr_1d = arr.reshape(-<span class="number">1</span>)</span><br><span class="line">    d_arr_1d = cuda.to_device(arr_1d)</span><br><span class="line">    elapsed_time = time.time()</span><br><span class="line">    res_n_cuda_reduce = matrix_sum_reduce_numba_cuda_reduce(d_arr_1d)</span><br><span class="line">    elapsed_time = time.time() - elapsed_time</span><br><span class="line">    <span class="keyword">if</span> (res_numpy - res_n_cuda_reduce) &lt; <span class="number">1e-6</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;process time on numba reduce: &#123;:.03f&#125;ms&quot;</span>.<span class="built_in">format</span>(elapsed_time * <span class="number">1000</span>))</span><br></pre></td></tr></table></figure>
<p>输出1: <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">process time on numpy : 0.820ms</span><br><span class="line">process time on cpu : 566.166ms</span><br><span class="line">process time on numba jit : 187.324ms</span><br><span class="line">process time on numba cuda: 208.259ms</span><br><span class="line">process time on numba cuda v2: 137.212ms</span><br><span class="line">process time on numba cuda v3: 120.333ms</span><br><span class="line">process time on numba reduce: 376.588ms</span><br></pre></td></tr></table></figure></p>
<p>输出2: <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">process time on numpy : 0.871ms</span><br><span class="line">process time on cpu : 567.084ms</span><br><span class="line">process time on numba jit (1st): 236.218ms</span><br><span class="line">process time on numba jit (2nd): 2.202ms</span><br><span class="line">process time on numba cuda (1st): 252.695ms</span><br><span class="line">process time on numba cuda (2nd): 1.662ms</span><br><span class="line">process time on numba cuda v2 (1st): 158.547ms</span><br><span class="line">process time on numba cuda v2 (2nd): 1.468ms</span><br><span class="line">process time on numba cuda v3 (1st): 130.770ms</span><br><span class="line">process time on numba cuda v3 (2nd): 1.101ms</span><br><span class="line">process time on numba reduce (1st): 434.946ms</span><br><span class="line">process time on numba reduce (2nd): 0.686ms</span><br></pre></td></tr></table></figure></p>
</details>
<p>并行归约（Reduction）是一种基础的并行算法。该算法常处理如下问题：假设有N个输入数据，使用一个符合结合律的二元操作符作用其上，最终生成1个结果。这个二元操作符可以是求和、取最大、取最小、平方、逻辑与或等等。本例以加法为例。这种问题的最基本的解决思路就是串行遍历，如示例代码中的<code>matrix_sum_cpu</code>函数。当将问题转化为并行计算时，由于加法的交换律和结合律，矩阵可以以任意顺序求和，其解决思路可以为：首先把输入数组划分为更小的数据块，之后用一个线程计算一个数据块的部分和，最后把所有部分和再求和得出最终结果。这种思路就被称为并行归约。相较于串行计算，并行归约的时间复杂度由<code>O(N)</code>变为<code>O(logN)</code>。由于并行归约非常经典，又在cuda编程中常常使用，numba.cuda本身就通过<code>cuda.reduce</code>实现了为1维数组的并行归约算法，其相关调用可参考本例中的<code>matrix_sum_reduce_numba_cuda_reduce</code>函数。</p>
<p>在本例中，并行归约在numba.cuda上的具体实现可参考<code>matrix_sum_reduce_numba_cuda</code>函数。具体而言，核函数循环stride，stride的上限为线程块中的线程个数，每次循环stride*2。当线程所索引到的数据对步长取余为0时，将线程所索引到的数据和与该数据距离stride的数据进行求和。最后返回该线程块的求和结果，并在cpu为所有线程块的结果求和。之所以只求得线程块的和，是因为核函数只能在每一个线程块中实现同步线程进度。具体而言，在并行归约中，循环中的每一步都需要上一步全部完成才能得到正确结果。因此如果想实现全局求和，需要同步每一个线程块的每一个线程来确认都已完成该步骤。而基于上面小节得知，不同的线程块的开始和结束是不一致的，所以在cuda编程中核函数无法同步不同线程块，甚至需要分批计算。假如某核函数需要10个线程块，但gpu只能并行计算5个线程块，这10个线程块将分成两批进行计算，也就无法为这10个线程块的所有线程达到同一时间的同步。因此核函数只能在每一个线程块中实现同步，无法跨线程块同步线程进度。在numba.cuda中，线程块内线程进度的同步通过<code>cuda.syncthreads()</code>来完成。</p>
<p>在本例的<code>matrix_sum_reduce_numba_cuda</code>函数中，每次循环只有线程索引为是stride的倍数的线程在进行执行，其他线程则保持静默。这样就导致在每一个线程束中只有稀疏的线程在执行，这种情况被称为线程束分化。但由于硬件设计，调度会以一整个线程束为单位进行，所以影响了程序的效率。线程束分化可以通过重新组织线程索引来解决。如本例<code>matrix_sum_reduce_numba_cuda_v2</code>函数所示，通过重新组织线程索引，可以保证在每一个block中一部分线程束的所有线程都在活跃，而另一部分线程束的所有线程都在静默。如在第一轮迭代，前16个线程束执行计算，后16个线程束什么都不做。通过简单整理线程索引，可以提高程序效率，如本例v2比v1执行速度提升了1.5倍。</p>
<p>此外，对全局内存的访问尽量进行合并访问与存储，能够达到尽量最大的带宽，也能够提升程序效率。比如在本例<code>matrix_sum_reduce_numba_cuda_v2</code>函数，每一循环所访问的数据索引不仅由线程索引决定，也由循环的跨度决定，即<code>idx =  2 * stride * cuda.threadIdx.x</code>。这就导致每次循环依据索引所访问到的数据并不连续，其跨度为stride。为了缓解这种现象，在并行归约中可以重新组织配对方法，进而让对内存的访问更加集中。如本例中<code>matrix_sum_reduce_numba_cuda_v3</code>所示，通过交错配对的方法，使得结果每次循环中只访问和存储到统一地址，进而提升了访问效率。如本例v3比v2执行速度提升了1.14倍。</p>
<p>（未完待续）</p>
<h2 id="参考">参考</h2>
<p>在撰写本文时，大量的参考了官方文档和其他博客，收益良多。观点表述如有雷同，可视为出自原作者。相关参考链接如下：
* <a href="http://numba.pydata.org/numba-doc/latest/">numba document</a>
* <a href="https://lulaoshi.info/gpu/">Python GPU快速教程</a> * <a
href="https://zhuanlan.zhihu.com/c_1188568938097819648">CUDA编程入门</a></p>
<p>眼过千遍，不如手过一遍。希望在阅读本文之后能够自己复现出来上述cuda代码。共勉。</p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>numba</tag>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>Numba: 简单装饰器加速python代码</title>
    <url>/zh/2020/07/04/zh/numba_python/</url>
    <content><![CDATA[<p>简单易用与运行效率低是贴在python身上的两大标签。开发人员一方面对其简单的语法和丰富的库爱不释手，一方面又对其由于动态编译和解释执行带来的较低的运行效率和GIL带来的多线程难扩展的情况深恶痛绝。为了解决这些python中的固有问题，一些解释器如cython尝试对一些函数提前编译进而提高执行效率。但绝大多数的解释器或库函数的语法非常不pythonic，而且也不能够做到即插即用。本文简单介绍了如何通过numba库，为python函数装饰器的方式来对python函数进行加速。为读者提供一中，简单易用，灵活编译方式去解决python的固有问题，提高python代码的执行效率。</p>
<span id="more"></span>
<h2 id="环境配置">环境配置</h2>
<p>为了简化环境配置，本文中的编程环境将全部由conda配置，并在conda的虚拟环境中测试。所涉及的依赖库分别是<code>numba</code>和<code>numpy</code>。相关环境可以通过以下语令进行配置。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda create -n numba-python python=3.7</span><br><span class="line">conda activate numba-python</span><br><span class="line">conda install numba numpy</span><br></pre></td></tr></table></figure>
<h2 id="基本概念">基本概念</h2>
<h3
id="编译型语言compiled-language解释型语言interpreted-language和即时编译just-in-time">编译型语言（Compiled
language），解释型语言（Interpreted
language）和即时编译（Just-In-Time）</h3>
<p>用高级语言编写的程序一般可通过由解释器（Interpreter）直接执行，或由编译器（Compiler）编译成机械码再执行。由解释器直接执行的高级语言称为解释型语言（Interpreted
language），其执行流程一般为：（解释型语言-&gt;程序运行-&gt;字节码-&gt;机械码），常见的解释型语言有Perl，Python，MATLAB和Ruby。需要通过编译器进行提前编译（AoT:
ahead of time）再执行的高级语言称为编译型语言（Compiled
language），其执行流程一般为：（编译型语言-&gt;机械码-&gt;程序运行），常见的编译型语言有c和c++。由于解释型语言可直接运行，其代码的灵活性相对更高。但在一般情况下，解释型语言需要边编译边运行，所以其执行效率相对于编译型语言较低。尤其对于部分反复执行的代码，解释型语言常需要对其进行反复编译。即时编译（Just-In-Time）结合了编译器的速度和解释器的灵活性，并允许自适应优化。</p>
<p>python是一种典型的解释型语言。在执行时，解释器首先将程序的字节码存储到.pyc，在将字节码发送到python虚拟机上进一步的解释执行字节码。如果程序未发生变化，python的字节码并不需要反复生成，但在python虚拟机上的解释步骤是需要反复执行的。在python
numba中，可将numba.jit的函数在第一次执行时生成的机械码，进而在该函数可以直接调用生成的机械码而省去反复解释的过程，进而达到与编译型语言相似的速度。</p>
<h3 id="装饰器decorator">装饰器（Decorator）</h3>
<p>装饰器是一种常见的设计模式，其允许向一个现有的对象添加新的功能，同时又不改变其结构。在python中，装饰器可以通过函数或类的形式定义，并通过<code>@decorator</code>的方式调用，一些常见的装饰器结构定义和调用方式可见如下代码。</p>
<details>
<summary>
代码：装饰器
</summary>
<p>通过函数定义装饰器 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">timef0</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;simplist decorator, with wrong func name&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kw</span>):</span><br><span class="line">        startt = time.time()</span><br><span class="line">        result = func(*args, **kw)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> time cost: <span class="subst">&#123;time.time()-startt:<span class="number">0.6</span>f&#125;</span>s&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">timef1</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;decorator with correct func name&quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">    @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kw</span>):</span><br><span class="line">        startt = time.time()</span><br><span class="line">        result = func(*args, **kw)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> time cost: <span class="subst">&#123;time.time()-startt:<span class="number">0.6</span>f&#125;</span>s&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">timef2</span>(<span class="params">num_runs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;decorator with param&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decorator</span>(<span class="params">func</span>):</span><br><span class="line"><span class="meta">        @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kw</span>):</span><br><span class="line">            startt = time.time()</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_runs):</span><br><span class="line">                result = func(*args, **kw)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> time cost: <span class="subst">&#123;(time.time()-startt)/num_runs:<span class="number">0.6</span>f&#125;</span>s (Avg over <span class="subst">&#123;num_runs&#125;</span> runs)&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line">    <span class="keyword">return</span> decorator</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">timef3</span>(<span class="params">*args, **kw</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;decorator with and without param&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(args) == <span class="number">1</span> <span class="keyword">and</span> <span class="built_in">len</span>(kw)==<span class="number">0</span>:</span><br><span class="line">        func = args[<span class="number">0</span>]</span><br><span class="line"><span class="meta">        @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kw</span>):</span><br><span class="line">            startt = time.time()</span><br><span class="line">            result = func(*args, **kw)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> time cost: <span class="subst">&#123;time.time()-startt:<span class="number">0.6</span>f&#125;</span>s&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">len</span>(args) == <span class="number">0</span> <span class="keyword">and</span> <span class="built_in">len</span>(kw)!=<span class="number">0</span>:</span><br><span class="line">        num_runs = kw[<span class="string">&quot;num_runs&quot;</span>] <span class="keyword">if</span> <span class="string">&quot;num_runs&quot;</span> <span class="keyword">in</span> kw <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        warmup   = kw[<span class="string">&quot;warmup&quot;</span>] <span class="keyword">if</span> <span class="string">&quot;warmup&quot;</span> <span class="keyword">in</span> kw <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">decorator</span>(<span class="params">func</span>):</span><br><span class="line"><span class="meta">            @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">            <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kw</span>):</span><br><span class="line">                <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(warmup):</span><br><span class="line">                    result = func(*args, **kw)</span><br><span class="line">                startt = time.time()</span><br><span class="line">                <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_runs):</span><br><span class="line">                    result = func(*args, **kw)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> time cost: <span class="subst">&#123;(time.time()-startt)/num_runs:<span class="number">0.6</span>f&#125;</span>s (Avg over <span class="subst">&#123;num_runs&#125;</span> runs)&quot;</span>)</span><br><span class="line">                <span class="keyword">return</span> result</span><br><span class="line">            <span class="keyword">return</span> wrapper</span><br><span class="line">        <span class="keyword">return</span> decorator</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Params for decorator are not expected!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">timef4</span>(<span class="params">func=<span class="literal">None</span>, num_runs=<span class="number">1</span>, warmup=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;decorator with and without param, a more flatten way&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> func:</span><br><span class="line">        <span class="keyword">return</span> functools.partial(timef4, num_runs=num_runs, warmup=warmup)</span><br><span class="line"><span class="meta">    @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kw</span>):</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(warmup):</span><br><span class="line">            result = func(*args, **kw)</span><br><span class="line">        startt = time.time()</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_runs):</span><br><span class="line">            result = func(*args, **kw)</span><br><span class="line">        <span class="keyword">if</span> num_runs == <span class="number">1</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> time cost: <span class="subst">&#123;time.time()-startt:<span class="number">0.6</span>f&#125;</span>s&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> time cost: <span class="subst">&#123;(time.time()-startt)/num_runs:<span class="number">0.6</span>f&#125;</span>s (Avg over <span class="subst">&#123;num_runs&#125;</span> runs)&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef0</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f0</span>(<span class="params">_<span class="built_in">list</span>, val</span>):</span><br><span class="line">    <span class="keyword">return</span> [l+val <span class="keyword">for</span> l <span class="keyword">in</span> _<span class="built_in">list</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f1</span>(<span class="params">_<span class="built_in">list</span>, val</span>):</span><br><span class="line">    <span class="keyword">return</span> [l+val <span class="keyword">for</span> l <span class="keyword">in</span> _<span class="built_in">list</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef2(<span class="params">num_runs=<span class="number">2</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f2</span>(<span class="params">_<span class="built_in">list</span>, val</span>):</span><br><span class="line">    <span class="keyword">return</span> [l+val <span class="keyword">for</span> l <span class="keyword">in</span> _<span class="built_in">list</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef3</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f3_woparam</span>(<span class="params">_<span class="built_in">list</span>, val</span>):</span><br><span class="line">    <span class="keyword">return</span> [l+val <span class="keyword">for</span> l <span class="keyword">in</span> _<span class="built_in">list</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef3(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f3_wparam</span>(<span class="params">_<span class="built_in">list</span>, val</span>):</span><br><span class="line">    <span class="keyword">return</span> [l+val <span class="keyword">for</span> l <span class="keyword">in</span> _<span class="built_in">list</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef4</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f4_woparam</span>(<span class="params">_<span class="built_in">list</span>, val</span>):</span><br><span class="line">    <span class="keyword">return</span> [l+val <span class="keyword">for</span> l <span class="keyword">in</span> _<span class="built_in">list</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef4(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f4_wparam</span>(<span class="params">_<span class="built_in">list</span>, val</span>):</span><br><span class="line">    <span class="keyword">return</span> [l+val <span class="keyword">for</span> l <span class="keyword">in</span> _<span class="built_in">list</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;add_val_f0 func name: <span class="subst">&#123;add_val_f0.__name__&#125;</span>&quot;</span>)</span><br><span class="line">add_val_f0(np.ones(<span class="number">1000000</span>), <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;add_val_f1 func name: <span class="subst">&#123;add_val_f1.__name__&#125;</span>&quot;</span>)</span><br><span class="line">add_val_f1(np.ones(<span class="number">1000000</span>), <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;add_val_f2 func name: <span class="subst">&#123;add_val_f2.__name__&#125;</span>&quot;</span>)</span><br><span class="line">add_val_f2(np.ones(<span class="number">1000000</span>), <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;add_val_f3_woparam func name: <span class="subst">&#123;add_val_f3_woparam.__name__&#125;</span>&quot;</span>)</span><br><span class="line">add_val_f3_woparam(np.ones(<span class="number">1000000</span>), <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;add_val_f3_wparam func name: <span class="subst">&#123;add_val_f3_wparam.__name__&#125;</span>&quot;</span>)</span><br><span class="line">add_val_f3_wparam(np.ones(<span class="number">1000000</span>), <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;add_val_f4_woparam func name: <span class="subst">&#123;add_val_f4_woparam.__name__&#125;</span>&quot;</span>)</span><br><span class="line">add_val_f4_woparam(np.ones(<span class="number">1000000</span>), <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;add_val_f4_wparam func name: <span class="subst">&#123;add_val_f4_wparam.__name__&#125;</span>&quot;</span>)</span><br><span class="line">add_val_f4_wparam(np.ones(<span class="number">1000000</span>), <span class="number">10</span>)</span><br></pre></td></tr></table></figure></p>
<p>输出: <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">add_val_f0 func name: wrapper</span><br><span class="line">add_val_f0 time cost: 0.257897s</span><br><span class="line">add_val_f1 func name: add_val_f1</span><br><span class="line">add_val_f1 time cost: 0.263955s</span><br><span class="line">add_val_f2 func name: add_val_f2</span><br><span class="line">add_val_f2 time cost: 0.269486s (Avg over 2 runs)</span><br><span class="line">add_val_f3_woparam func name: add_val_f3_woparam</span><br><span class="line">add_val_f3_woparam time cost: 0.257728s</span><br><span class="line">add_val_f3_wparam func name: add_val_f3_wparam</span><br><span class="line">add_val_f3_wparam time cost: 0.283820s (Avg over 2 runs)</span><br><span class="line">add_val_f4_woparam func name: add_val_f4_woparam</span><br><span class="line">add_val_f4_woparam time cost: 0.254313s</span><br><span class="line">add_val_f4_wparam func name: add_val_f4_wparam</span><br><span class="line">add_val_f4_wparam time cost: 0.282968s (Avg over 2 runs)</span><br></pre></td></tr></table></figure></p>
<p>通过类定义装饰器</p>
</details>
<p>装饰器的本质是为函数添加新功能但不更改函数结构，比如在本例中计算时间代价，或是将函数注册到模块中。因此，为了保证函数调用的一致性，装饰器返回的仍是函数对象。被返回的函数一般有两种类型，原始输入函数，或是调用原始输入函数的wrapper函数。需要注意的是，当返回函数为wrapper函数时，由于其是一个新的函数对象，函数相关的属性也就不同。如上例<code>timef0</code>所示，简单返回wrapper函数会导致装饰函数的名字变为新函数的名字，进而破坏了装饰函数在装饰前和装饰后的一致性。为了使得装饰前与装饰后函数属性一致，需要在使用时对wrapper函数加入<code>@functools.wraps(func)</code>装饰器。</p>
<p>在装饰器被调用时，其真正执行的是<code>decorator(func)</code>。如对<code>add_val_f1</code>添加<code>@timef1</code>装饰器时，其在声明时是等同于<code>timef1(add_val_f1)</code>。因此也就可以通过函数嵌套的形式，为装饰器传参。比如上例中的<code>timef2</code>，在声明时相当于<code>timef2(num_runs=2)(add_val_f2)</code>；由于<code>timef2(num_runs=2)</code>的返回值为<code>decorator</code>函数，所以该声明也就等价于<code>decorator(add_val_f2)</code>。在这种函数情况下，<code>timef2</code>只负责传参，而<code>decorator</code>函数才完成类似于<code>timef1</code>的功能。当然笔者更喜欢使用<code>timef2</code>所示的递归的方式，来进行更加灵活的定义和传参。</p>
<p>此外，装饰器是可以嵌套的，具体例子会在下面小节展示。</p>
<h2 id="通过numba.jit加速python-for循环">通过numba.jit加速python
for循环</h2>
<p>上小节提到，在python的for中，即使是相同的代码也需要对字节码进行反复解释，这种执行编译的方式是低效的。在numba中，可以通过添加简单<code>@numba.jit</code>装饰器进行加速，比如可以通过如下代码对上述例子进行加速。</p>
<details>
<summary>
代码：numba.jit加速for循环
</summary>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> numba</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">timef</span>(<span class="params">func=<span class="literal">None</span>, num_runs=<span class="number">1</span>, warmup=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;calculate the time cost for the function&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> func:</span><br><span class="line">        <span class="keyword">return</span> functools.partial(timef, num_runs=num_runs, warmup=warmup)</span><br><span class="line"><span class="meta">    @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kw</span>):</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(warmup):</span><br><span class="line">            result = func(*args, **kw)</span><br><span class="line">        startt = time.time()</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_runs):</span><br><span class="line">            result = func(*args, **kw)</span><br><span class="line">        <span class="keyword">if</span> num_runs == <span class="number">1</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> time cost: <span class="subst">&#123;time.time()-startt:<span class="number">0.6</span>f&#125;</span>s&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> time cost: <span class="subst">&#123;(time.time()-startt)/num_runs:<span class="number">0.6</span>f&#125;</span>s (Avg over <span class="subst">&#123;num_runs&#125;</span> runs)&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f0</span>(<span class="params">_<span class="built_in">list</span>, val</span>):</span><br><span class="line">    <span class="keyword">return</span> [l+val <span class="keyword">for</span> l <span class="keyword">in</span> _<span class="built_in">list</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f1</span>(<span class="params">ls, val</span>):</span><br><span class="line">    a = np.zeros(<span class="built_in">len</span>(ls))</span><br><span class="line">    <span class="keyword">for</span> index,_ <span class="keyword">in</span> <span class="built_in">enumerate</span>(ls):</span><br><span class="line">        a[index] = ls[index]+val</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef</span></span><br><span class="line"><span class="meta">@numba.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f2</span>(<span class="params">ls, val</span>):</span><br><span class="line">    a = np.zeros(<span class="built_in">len</span>(ls))</span><br><span class="line">    <span class="keyword">for</span> index,_ <span class="keyword">in</span> <span class="built_in">enumerate</span>(ls):</span><br><span class="line">        a[index] = ls[index]+val</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="meta">@numba.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f2_warmedup</span>(<span class="params">ls, val</span>):</span><br><span class="line">    a = np.zeros(<span class="built_in">len</span>(ls))</span><br><span class="line">    <span class="keyword">for</span> index,_ <span class="keyword">in</span> <span class="built_in">enumerate</span>(ls):</span><br><span class="line">        a[index] = ls[index]+val</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef</span></span><br><span class="line"><span class="meta">@numba.jit(<span class="params">nopython=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f3</span>(<span class="params">ls, val</span>):</span><br><span class="line">    a = np.zeros(<span class="built_in">len</span>(ls))</span><br><span class="line">    <span class="keyword">for</span> index,_ <span class="keyword">in</span> <span class="built_in">enumerate</span>(ls):</span><br><span class="line">        a[index] = ls[index]+val</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="meta">@numba.jit(<span class="params">nopython=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f3_warmedup</span>(<span class="params">ls, val</span>):</span><br><span class="line">    a = np.zeros(<span class="built_in">len</span>(ls))</span><br><span class="line">    <span class="keyword">for</span> index,_ <span class="keyword">in</span> <span class="built_in">enumerate</span>(ls):</span><br><span class="line">        a[index] = ls[index]+val</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef</span></span><br><span class="line"><span class="meta">@numba.jit(<span class="params">nopython=<span class="literal">True</span>, cache=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f4</span>(<span class="params">ls, val</span>):</span><br><span class="line">    a = np.zeros(<span class="built_in">len</span>(ls))</span><br><span class="line">    <span class="keyword">for</span> index,_ <span class="keyword">in</span> <span class="built_in">enumerate</span>(ls):</span><br><span class="line">        a[index] = ls[index]+val</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="meta">@numba.jit(<span class="params">nopython=<span class="literal">True</span>, cache=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f4_warmedup</span>(<span class="params">ls, val</span>):</span><br><span class="line">    a = np.zeros(<span class="built_in">len</span>(ls))</span><br><span class="line">    <span class="keyword">for</span> index,_ <span class="keyword">in</span> <span class="built_in">enumerate</span>(ls):</span><br><span class="line">        a[index] = ls[index]+val</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">numba.config.NUMBA_DEFAULT_NUM_THREADS=<span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@timef</span></span><br><span class="line"><span class="meta">@numba.jit(<span class="params">nopython=<span class="literal">True</span>, parallel=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f5</span>(<span class="params">ls, val</span>):</span><br><span class="line">    a = np.zeros(<span class="built_in">len</span>(ls))</span><br><span class="line">    <span class="keyword">for</span> index,_ <span class="keyword">in</span> <span class="built_in">enumerate</span>(ls):</span><br><span class="line">        a[index] = ls[index]+val</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="meta">@numba.jit(<span class="params">nopython=<span class="literal">True</span>, parallel=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f5_warmedup</span>(<span class="params">ls, val</span>):</span><br><span class="line">    a = np.zeros(<span class="built_in">len</span>(ls))</span><br><span class="line">    <span class="keyword">for</span> index,_ <span class="keyword">in</span> <span class="built_in">enumerate</span>(ls):</span><br><span class="line">        a[index] = ls[index]+val</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef</span></span><br><span class="line"><span class="meta">@numba.jit(<span class="params">nopython=<span class="literal">True</span>, parallel=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f6</span>(<span class="params">ls, val</span>):</span><br><span class="line">    a = np.zeros(<span class="built_in">len</span>(ls))</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> numba.prange(<span class="built_in">len</span>(ls)):</span><br><span class="line">        a[index] = ls[index]+val</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="meta">@numba.jit(<span class="params">nopython=<span class="literal">True</span>, parallel=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_f6_warmedup</span>(<span class="params">ls, val</span>):</span><br><span class="line">    a = np.zeros(<span class="built_in">len</span>(ls))</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> numba.prange(<span class="built_in">len</span>(ls)):</span><br><span class="line">        a[index] = ls[index]+val</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val_numpy</span>(<span class="params">ls, val</span>):</span><br><span class="line">    <span class="keyword">return</span> ls + val</span><br><span class="line"></span><br><span class="line">size=<span class="number">10000000</span></span><br><span class="line">add_val_f0(np.ones(size), <span class="number">10</span>)</span><br><span class="line">add_val_f1(np.ones(size), <span class="number">10</span>)</span><br><span class="line">add_val_f2(np.ones(size), <span class="number">10</span>)</span><br><span class="line">add_val_f2_warmedup(np.ones(size), <span class="number">10</span>)</span><br><span class="line">add_val_f3(np.ones(size), <span class="number">10</span>)</span><br><span class="line">add_val_f3_warmedup(np.ones(size), <span class="number">10</span>)</span><br><span class="line">add_val_f4(np.ones(size), <span class="number">10</span>)</span><br><span class="line">add_val_f4_warmedup(np.ones(size), <span class="number">10</span>)</span><br><span class="line">add_val_f5(np.ones(size), <span class="number">10</span>)</span><br><span class="line">add_val_f5_warmedup(np.ones(size), <span class="number">10</span>)</span><br><span class="line">add_val_f6(np.ones(size), <span class="number">10</span>)</span><br><span class="line">add_val_f6_warmedup(np.ones(size), <span class="number">10</span>)</span><br><span class="line">add_val_numpy(np.ones(size), <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>输出: <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">add_val_f0 time cost: 2.478387s</span><br><span class="line">add_val_f1 time cost: 4.142665s</span><br><span class="line">add_val_f2 time cost: 0.228293s</span><br><span class="line">add_val_f2_warmedup time cost: 0.037352s (Avg over 2 runs)</span><br><span class="line">add_val_f3 time cost: 0.129881s</span><br><span class="line">add_val_f3_warmedup time cost: 0.036910s (Avg over 2 runs)</span><br><span class="line">add_val_f4 time cost: 0.038818s</span><br><span class="line">add_val_f4_warmedup time cost: 0.036762s (Avg over 2 runs)</span><br><span class="line">add_val_f5 time cost: 0.214643s</span><br><span class="line">add_val_f5_warmedup time cost: 0.017964s (Avg over 2 runs)</span><br><span class="line">add_val_f6 time cost: 0.273015s</span><br><span class="line">add_val_f6_warmedup time cost: 0.010958s (Avg over 2 runs)</span><br><span class="line">add_val_numpy time cost: 0.012424s</span><br></pre></td></tr></table></figure></p>
<p>其中__pychache__中生成对应的缓存文件： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── numba_jit.py</span><br><span class="line">└── __pycache__</span><br><span class="line">    ├── numba_jit.add_val_f4-68.py37m.1.nbc</span><br><span class="line">    ├── numba_jit.add_val_f4-68.py37m.nbi</span><br><span class="line">    ├── numba_jit.add_val_f4_warmedup-76.py37m.1.nbc</span><br><span class="line">    └── numba_jit.add_val_f4_warmedup-76.py37m.nbi</span><br></pre></td></tr></table></figure></p>
</details>
<p>在上诉例子中，可以看到通过简单添加<code>@numba.jit</code>装饰器来对python代码进行加速。通过<code>add_val_fn</code>和<code>add_val_fn_warmedup</code>极大的速度差距可以看到，在<code>@numba.jit</code>中，对于第一次执行还是存在解释编译，因此执行效率相对缓慢。而在第一次解释编译之后，其执行速度几乎达到甚至超过numpy经过优化之后的速度。</p>
<p>在numba.jit中常用到的参数有三个，分别为：nopython，cache和parallel。
*
nopython参数用于控制numba的编译模式，numba有两种编译模式，分别为非python模式和对象模式。当nopython=True时，numba编译模式为非python模式。这种编译模式会产生最高性能的代码，但由于其生成的代码无法访问python
c
api，对于原生python代码的兼容型一般。比如非python模式无法兼任原生的python类作为输入类型，但其可以通过numba.jitclass或是numpy的结构体来解决。
*
cache参数用于控制函数的缓存至磁盘文件，可以通过传递cache=True避免每次调用Python程序时都要进行编译，进而提升第一次执行相对缓慢相对缓慢的情况。如在上例中，add_val_f4通过cache=True缓存下编译过的文件，进而达到与<code>add_val_f4_warmedup</code>相似的速度。需要注意的是当函数缓存文件不存在时，第一次执行该函数将生成缓存文件，而本次执行的速度同样相对缓慢。
*
parallel参数用于控制函数的并行。当parallel=True时，numba对函数内的操作进行并行优化。也可以通过numba.prange进行显示并行循环进而进一步提高执行效率。此外，numba通过<code>numba.config.NUMBA_DEFAULT_NUM_THREADS</code>来指定并行的线程数。</p>
<p>numba.jit足以应付绝大多数的循环，但当一个操作只想针对数组的某一维度进行操作，numba.jit就显得力不从心了。下一小节将讨论如何通过numba中的vectroize和guvectorize生成numpy的通用函数去解决多维数组的扩展问题。</p>
<h2
id="通过numba中的vectroize和guvectorize创建高效的numpy通用函数">通过numba中的vectroize和guvectorize创建高效的numpy通用函数</h2>
<p>在numpy中通用函数ufunc（universal
function），是一种能对数组的每个元素进行操作的函数，一些基本操作如add，dot，max，any等在numpy中都通过c来实现进而达到较好的执行效率。</p>
<p>对于相对较复杂的操作，numpy允许自定义ufunc。但相较于通过c实现的ufunc，通过python自定义的ufunc与原始python函数速度差异不大。而且numpy自定义的ufunc只支持按位操作（elementwise
operation），对于相对较复杂的操作如矩阵乘法这种广义通用函数，numpy的支持并不友好。而numba的vectroize和guvectorize在兼容numpy.ufunc的特性的同时，执行效率高并且支持广义通用函数。</p>
<p>本节示例展示了通过numba中的vectroize和guvectorize分别来加速numpy自定义的ufunc。</p>
<details>
<summary>
代码：numba.vectroize创建高效numpy通用函数 - 按位乘法
</summary>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> numba</span><br><span class="line"></span><br><span class="line">numba.config.NUMBA_NUM_THREADS=<span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">timef</span>(<span class="params">func=<span class="literal">None</span>, num_runs=<span class="number">1</span>, warmup=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;calculate the time cost for the function&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> func:</span><br><span class="line">        <span class="keyword">return</span> functools.partial(timef, num_runs=num_runs, warmup=warmup)</span><br><span class="line"><span class="meta">    @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kw</span>):</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(warmup):</span><br><span class="line">            result = func(*args, **kw)</span><br><span class="line">        startt = time.time()</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_runs):</span><br><span class="line">            result = func(*args, **kw)</span><br><span class="line">        <span class="keyword">if</span> num_runs == <span class="number">1</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> time cost: <span class="subst">&#123;time.time()-startt:<span class="number">0.6</span>f&#125;</span>s&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> time cost: <span class="subst">&#123;(time.time()-startt)/num_runs:<span class="number">0.6</span>f&#125;</span>s (Avg over <span class="subst">&#123;num_runs&#125;</span> runs)&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dotmul_numpy</span>(<span class="params">matA, matB</span>):</span><br><span class="line">    <span class="keyword">return</span> matA*matB</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dotmul_python</span>(<span class="params">matA, matB</span>):</span><br><span class="line">    matC = np.empty_like(matA)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(matA.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(matA.shape[<span class="number">1</span>]):</span><br><span class="line">            matC[i,j] = matA[i,j] * matB[i,j]</span><br><span class="line">    <span class="keyword">return</span> matC</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="meta">@numba.jit(<span class="params">nopython=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dotmul_numba_jit</span>(<span class="params">matA, matB</span>):</span><br><span class="line">    matC = np.empty_like(matA)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(matA.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(matA.shape[<span class="number">1</span>]):</span><br><span class="line">            matC[i,j] = matA[i,j] * matB[i,j]</span><br><span class="line">    <span class="keyword">return</span> matC</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="meta">@numba.jit(<span class="params">nopython=<span class="literal">True</span>, parallel=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dotmul_numba_jit_parallel</span>(<span class="params">matA, matB</span>):</span><br><span class="line">    matC = np.empty_like(matA)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> numba.prange(matA.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> numba.prange(matA.shape[<span class="number">1</span>]):</span><br><span class="line">            matC[i,j] = matA[i,j] * matB[i,j]</span><br><span class="line">    <span class="keyword">return</span> matC</span><br><span class="line"></span><br><span class="line"><span class="meta">@np.vectorize</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dotmul_numpy_vectorize</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">return</span> a*b</span><br><span class="line">dotmul_numpy_vectorize.__name__ = <span class="string">&quot;dotmul_numpy_vectorize&quot;</span></span><br><span class="line">dotmul_numpy_vectorize = timef(num_runs=<span class="number">2</span>, warmup=<span class="number">1</span>)(dotmul_numpy_vectorize)</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="meta">@numba.vectorize(<span class="params">nopython=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dotmul_numba_vectorize</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">return</span> a*b</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="meta">@numba.vectorize(<span class="params"><span class="string">&#x27;float64(float64, float64)&#x27;</span>, target=<span class="string">&#x27;parallel&#x27;</span>, nopython=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dotmul_numba_vectorize_parallel</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">return</span> a*b</span><br><span class="line"></span><br><span class="line"><span class="meta">@numba.vectorize(<span class="params"><span class="string">&#x27;float64(float64, float64)&#x27;</span>, target=<span class="string">&#x27;cuda&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dotmul_numba_vectorize_cuda</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">return</span> a*b</span><br><span class="line">dotmul_numba_vectorize_cuda.__name__ = <span class="string">&quot;dotmul_numba_vectorize_cuda&quot;</span></span><br><span class="line">dotmul_numba_vectorize_cuda = timef(num_runs=<span class="number">2</span>, warmup=<span class="number">1</span>)(dotmul_numba_vectorize_cuda)</span><br><span class="line"></span><br><span class="line">size=<span class="number">1000</span></span><br><span class="line">dotmul_python(np.ones((size,size)), np.ones((size,size)))</span><br><span class="line">dotmul_numpy(np.ones((size,size)), np.ones((size,size)))</span><br><span class="line">dotmul_numba_jit(np.ones((size,size)), np.ones((size,size)))</span><br><span class="line">dotmul_numba_jit_parallel(np.ones((size,size)), np.ones((size,size)))</span><br><span class="line">dotmul_numpy_vectorize(np.ones((size,size)), np.ones((size,size)))</span><br><span class="line">dotmul_numba_vectorize(np.ones((size,size)), np.ones((size,size)))</span><br><span class="line">dotmul_numba_vectorize_parallel(np.ones((size,size)), np.ones((size,size)))</span><br><span class="line">dotmul_numba_vectorize_cuda(np.ones((size,size)), np.ones((size,size)))</span><br></pre></td></tr></table></figure>
<p>输出： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">dotmul_python time cost: 0.390452s (Avg over 2 runs)</span><br><span class="line">dotmul_numpy time cost: 0.002007s (Avg over 2 runs)</span><br><span class="line">dotmul_numba_jit time cost: 0.002113s (Avg over 2 runs)</span><br><span class="line">dotmul_numba_jit_parallel time cost: 0.001313s (Avg over 2 runs)</span><br><span class="line">dotmul_numpy_vectorize time cost: 0.143894s (Avg over 2 runs)</span><br><span class="line">dotmul_numba_vectorize time cost: 0.001535s (Avg over 2 runs)</span><br><span class="line">dotmul_numba_vectorize_parallel time cost: 0.001899s (Avg over 2 runs)</span><br><span class="line">dotmul_numba_vectorize_cuda time cost: 0.004523s (Avg over 2 runs)</span><br></pre></td></tr></table></figure></p>
</details>
<details>
<summary>
代码：numba.guvectroize创建高效numpy广义通用函数 - 矩阵乘法
</summary>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> numba</span><br><span class="line"></span><br><span class="line">numba.config.NUMBA_NUM_THREADS=<span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">timef</span>(<span class="params">func=<span class="literal">None</span>, num_runs=<span class="number">1</span>, warmup=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;calculate the time cost for the function&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> func:</span><br><span class="line">        <span class="keyword">return</span> functools.partial(timef, num_runs=num_runs, warmup=warmup)</span><br><span class="line"><span class="meta">    @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kw</span>):</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(warmup):</span><br><span class="line">            result = func(*args, **kw)</span><br><span class="line">        startt = time.time()</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_runs):</span><br><span class="line">            result = func(*args, **kw)</span><br><span class="line">        <span class="keyword">if</span> num_runs == <span class="number">1</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> time cost: <span class="subst">&#123;time.time()-startt:<span class="number">0.6</span>f&#125;</span>s&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> time cost: <span class="subst">&#123;(time.time()-startt)/num_runs:<span class="number">0.6</span>f&#125;</span>s (Avg over <span class="subst">&#123;num_runs&#125;</span> runs)&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matmul_numpy</span>(<span class="params">matA, matB</span>):</span><br><span class="line">    <span class="keyword">return</span> np.matmul(matA, matB)</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matmul_python</span>(<span class="params">matA, matB</span>):</span><br><span class="line">    m, n = matA.shape</span><br><span class="line">    n, p = matB.shape</span><br><span class="line">    matC = np.zeros((m,p))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(p):</span><br><span class="line">            matC[i, j] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                matC[i, j] += matA[i, k] * matB[k, j]</span><br><span class="line">    <span class="keyword">return</span> matC</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="meta">@numba.jit(<span class="params">nopython=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matmul_numba_jit</span>(<span class="params">matA, matB</span>):</span><br><span class="line">    m, n = matA.shape</span><br><span class="line">    n, p = matB.shape</span><br><span class="line">    matC = np.zeros((m,p))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(p):</span><br><span class="line">            matC[i, j] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                matC[i, j] += matA[i, k] * matB[k, j]</span><br><span class="line">    <span class="keyword">return</span> matC</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="meta">@numba.jit(<span class="params">nopython=<span class="literal">True</span>, parallel=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matmul_numba_jit_parallel</span>(<span class="params">matA, matB</span>):</span><br><span class="line">    m, n = matA.shape</span><br><span class="line">    n, p = matB.shape</span><br><span class="line">    matC = np.zeros((m,p))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(p):</span><br><span class="line">            matC[i, j] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                matC[i, j] += matA[i, k] * matB[k, j]</span><br><span class="line">    <span class="keyword">return</span> matC</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="meta">@numba.guvectorize(<span class="params"><span class="string">&quot;float64[:,:], float64[:,:], float64[:,:]&quot;</span>, <span class="string">&quot;(m,n),(n,p)-&gt;(m,p)&quot;</span>, nopython=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matmul_numba_guvectorize</span>(<span class="params">matA, matB, matC</span>):</span><br><span class="line">    m, n = matA.shape</span><br><span class="line">    n, p = matB.shape</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(p):</span><br><span class="line">            matC[i, j] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                matC[i, j] += matA[i, k] * matB[k, j]</span><br><span class="line"></span><br><span class="line"><span class="meta">@timef(<span class="params">num_runs=<span class="number">2</span>, warmup=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="meta">@numba.guvectorize(<span class="params"><span class="string">&quot;float64[:,:], float64[:,:], float64[:,:]&quot;</span>, <span class="string">&quot;(m,n),(n,p)-&gt;(m,p)&quot;</span>, target=<span class="string">&#x27;parallel&#x27;</span>, nopython=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matmul_numba_guvectorize_parallel</span>(<span class="params">matA, matB, matC</span>):</span><br><span class="line">    m, n = matA.shape</span><br><span class="line">    n, p = matB.shape</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(p):</span><br><span class="line">            matC[i, j] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                matC[i, j] += matA[i, k] * matB[k, j]</span><br><span class="line"></span><br><span class="line"><span class="meta">@numba.guvectorize(<span class="params"><span class="string">&quot;float64[:,:], float64[:,:], float64[:,:]&quot;</span>, <span class="string">&quot;(m,n),(n,p)-&gt;(m,p)&quot;</span>, target=<span class="string">&#x27;cuda&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matmul_numba_guvectorize_cuda</span>(<span class="params">matA, matB, matC</span>):</span><br><span class="line">    m, n = matA.shape</span><br><span class="line">    n, p = matB.shape</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(p):</span><br><span class="line">            matC[i, j] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                matC[i, j] += matA[i, k] * matB[k, j]</span><br><span class="line">matmul_numba_guvectorize_cuda.__name__ = <span class="string">&quot;matmul_numba_guvectorize_cuda&quot;</span></span><br><span class="line">matmul_numba_guvectorize_cuda = timef(num_runs=<span class="number">2</span>, warmup=<span class="number">1</span>)(matmul_numba_guvectorize_cuda)</span><br><span class="line"></span><br><span class="line">size=<span class="number">1000</span></span><br><span class="line">matmul_python(np.ones((size,size)), np.ones((size,size)))</span><br><span class="line">matmul_numpy(np.ones((size,size)), np.ones((size,size)))</span><br><span class="line">matmul_numba_jit(np.ones((size,size)), np.ones((size,size)))</span><br><span class="line">matmul_numba_jit_parallel(np.ones((size,size)), np.ones((size,size)))</span><br><span class="line">matmul_numba_guvectorize(np.ones((size,size)), np.ones((size,size)), np.zeros((size,size)))</span><br><span class="line">matmul_numba_guvectorize_parallel(np.ones((size,size)), np.ones((size,size)), np.zeros((size,size)))</span><br><span class="line"></span><br><span class="line">size=<span class="number">100</span></span><br><span class="line">matmul_numba_guvectorize_cuda(np.ones((size,size)), np.ones((size,size)), np.zeros((size,size)))</span><br></pre></td></tr></table></figure>
<p>输出： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">matmul_python time cost: 578.654421s (Avg over 2 runs)</span><br><span class="line">matmul_numpy time cost: 0.011648s (Avg over 2 runs)</span><br><span class="line">matmul_numba_jit time cost: 1.189993s (Avg over 2 runs)</span><br><span class="line">matmul_numba_jit_parallel time cost: 1.129338s (Avg over 2 runs)</span><br><span class="line">matmul_numba_guvectorize time cost: 1.124118s (Avg over 2 runs)</span><br><span class="line">matmul_numba_guvectorize_parallel time cost: 1.129258s (Avg over 2 runs)</span><br><span class="line">matmul_numba_guvectorize_cuda time cost: 0.179321s (Avg over 2 runs)</span><br></pre></td></tr></table></figure></p>
</details>
<p>通过上例可以看到，相较于numpy.vectorize所生成的ufunc，numba.vectorize执行效率更高，甚至超过numpy通过c实现的通用函数的执行速度。而对于numba.guvectorize虽然可以灵活自定义，并且远远超过原始python实现，但相较于numpy优化过的代码仍有差距。</p>
<p>类似于numba.jit，numba.vectorize和numba.guvectorize同样可以通过nopython和cache参数来设置编译模式和缓存文件。与之不同的是，vectorize和guvectorize通过target参数来控制并行计算。当target="cpu"时numba使用单线程，当target="parallel"时numba使用多线程并行。值得一提的是target还可以允许使用nvidia
cuda作为numpy通用函数的计算单元，虽然笔者并不建议。如果读者希望通过cuda加速python代码，可以看笔者的另一篇博客<a
href="https://foreveryounggithub.github.io/zh/2020/06/10/numba_cuda/">Numba:
通过python快速学习cuda编程</a>。</p>
<p>为什么要用ufunc而不是jit？jit更直观，更灵活，甚至更快。主要原因是因为numba生成的numpy.ufunc可以对某些轴进行广播（broadcast），在一些情况下可以会某一些轴进行缩减和累积。这样输入数据的维度就可以更自由。但除此以外，笔者更推荐numba.jit。</p>
<h2 id="numba.pycc提前编译python函数">numba.pycc提前编译python函数</h2>
<p>虽然numba主要的编译方式都是jit，但numba也如cython一样提供提前编译（AoT）的编译方式。本小节对之前numba.jit的例子进行改写，达到了提前编译的效果。</p>
<details>
<summary>
代码：numba.pycc提前编译python函数
</summary>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># numba_pycc_module.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numba</span><br><span class="line"><span class="keyword">from</span> numba.pycc <span class="keyword">import</span> CC</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">cc = CC(<span class="string">&#x27;numba_pycc_test_module&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@cc.export(<span class="params"><span class="string">&#x27;add_valf&#x27;</span>, <span class="string">&#x27;f8[:](f8[:], f8)&#x27;</span></span>)</span></span><br><span class="line"><span class="meta">@cc.export(<span class="params"><span class="string">&#x27;add_vali&#x27;</span>, <span class="string">&#x27;i4[:](i4[:], i4)&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_val</span>(<span class="params">ls, val</span>):</span><br><span class="line">    a = np.empty_like(ls)</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> numba.prange(<span class="built_in">len</span>(ls)):</span><br><span class="line">        a[index] = ls[index]+val</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    cc.<span class="built_in">compile</span>()</span><br></pre></td></tr></table></figure>
<p>运行生成.so的二进制文件 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── numba_pycc_module.py</span><br><span class="line">├── numba_pycc_test_module.cpython-37m-x86_64-linux-gnu.so</span><br><span class="line">└── numba_pycc_test.py</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># numba_pycc_test.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numba_pycc_test_module</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">timef</span>(<span class="params">func=<span class="literal">None</span>, num_runs=<span class="number">1</span>, warmup=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;calculate the time cost for the function&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> func:</span><br><span class="line">        <span class="keyword">return</span> functools.partial(timef, num_runs=num_runs, warmup=warmup)</span><br><span class="line"><span class="meta">    @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kw</span>):</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(warmup):</span><br><span class="line">            result = func(*args, **kw)</span><br><span class="line">        startt = time.time()</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_runs):</span><br><span class="line">            result = func(*args, **kw)</span><br><span class="line">        <span class="keyword">if</span> num_runs == <span class="number">1</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> time cost: <span class="subst">&#123;time.time()-startt:<span class="number">0.6</span>f&#125;</span>s&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> time cost: <span class="subst">&#123;(time.time()-startt)/num_runs:<span class="number">0.6</span>f&#125;</span>s (Avg over <span class="subst">&#123;num_runs&#125;</span> runs)&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line">size=<span class="number">10000000</span></span><br><span class="line">timef(numba_pycc_test_module.add_valf)(np.ones(size), <span class="number">10</span>)</span><br><span class="line">timef(num_runs=<span class="number">2</span>, warmup=<span class="number">1</span>)(numba_pycc_test_module.add_valf)(np.ones(size), <span class="number">10</span>)</span><br><span class="line">timef(numba_pycc_test_module.add_vali)(np.ones(size, dtype=np.int32), <span class="number">10</span>)</span><br><span class="line">timef(num_runs=<span class="number">2</span>, warmup=<span class="number">1</span>)(numba_pycc_test_module.add_vali)(np.ones(size, dtype=np.int32), <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
输出: <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">add_valf time cost: 0.027234s</span><br><span class="line">add_valf time cost: 0.029468s (Avg over 2 runs)</span><br><span class="line">add_vali time cost: 0.019918s</span><br><span class="line">add_vali time cost: 0.018991s (Avg over 2 runs)</span><br></pre></td></tr></table></figure>
</details>
<p>通过上例可以看到，通过numba.pycc可以提前编译生成的python函数为机械码。被编译的模块在运行时没有编译开销，也不依赖于Numba库。numba.pycc还可以将编译步骤集成到setuptools等脚本中。虽然numba.pycc使用起来灵活方便，但也有其局限性。如摆脱对numba依赖的同时也无法使用numba的一下特性，比如并行计算。而且numba.pycc编译仅允许使用常规的numba.jit函数，不能使用ufuncs。</p>
<p>至此，笔者对于通过numba加速python代码的介绍就结束了。在笔者使用numba时，虽然有些许的局限性，但其灵活简单的语法，对numpy数组较为完备的支持，保持python原有函数结构上对其极大的提速令笔者非常满意。此外，本文对一些其他笔者不太常用的功能并没有提及，如numba中的cfunc，jit中的nogil等。而且numba在仍在高效的迭代开发，一些实验性的功能如jit_module等都非常有趣。希望读者通过本文对numba有初步的了解，进而运用到自己的项目中。如有需要，多多查看官方手册。祝好，共勉。</p>
<h2 id="参考">参考</h2>
<p>在撰写本文时，大量的参考了官方文档和其他博客，收益良多。观点表述如有雷同，可视为出自原作者。相关参考链接如下：
* <a href="http://numba.pydata.org/numba-doc/latest/">numba document</a>
* <a
href="https://zhuanlan.zhihu.com/c_1111226743037448192">python高性能编程与实战案例代码总结</a>
* <a href="https://zhuanlan.zhihu.com/p/27152060">numba 的基本应用</a> *
<a
href="https://blog.reverberate.org/2012/12/hello-jit-world-joy-of-simple-jits.html">Hello,
JIT World: The Joy of Simple JITs</a></p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>numba</tag>
      </tags>
  </entry>
  <entry>
    <title>基本图像局部区域（Patch）的描述符（local descriptor）学习策略</title>
    <url>/zh/2022/02/27/zh/patch_based_local_descriptor/</url>
    <content><![CDATA[<p>一些计算机视觉任务会依赖图像的几何关系，比如相机定位，三维重建等等。一种方法用于求解图像的几何关系是通过图像局部特征的相关匹配来求解。图像的局部特征则是从<strong>图像局部</strong>区域中抽取的<strong>特征</strong>，包括边缘、角点、线、曲线和特别属性的区域等。一般来说，图像的局部特征包含两个部分，局部特征点的位置，局部特征的描述符。在一方面上，描述符可分为基于人工的特征符和基于学习的特征符。</p>
<p>文本将关注于在特征点已知的情况下，如何通过学习去生成描述符去描述局部区域(Patch)的。相较于网络架构，本文主要关注于不同论文的数据处理和学习策略。</p>
<span id="more"></span>
<h2 id="数据和任务">数据和任务</h2>
<p>基本Patch的描述符（local descriptor）即给定Patch生成local
descriptor。对于神经网络来说，即意味着网络的输入是图像局部区域(Patch)，网络的输出是该Patch的特征。</p>
<p>其中图像局部区域(Patch)是基于图像的特征点所提取的的。如下图所示，下面五个图像来自于同一场景，图上的圆圈的中心即是特征点，而圆圈所选择的区域即是Patch。</p>
<img data-src="/zh/2022/02/27/zh/patch_based_local_descriptor/images_hard.jpg" class="" title="HPatch原始和特征点">
<p>需要注意的是，由于五个图像来自于同一场景，图像中的特征点也一一匹配。如下图所示，下图的每一列都是从匹配的特征点所提取的局部区域，而每一行则代表了从每幅图像所提取的全部Patch。</p>
<img data-src="/zh/2022/02/27/zh/patch_based_local_descriptor/patches_hard.jpg" class="" title="HPatch提取的局部区域">
<p>理想的描述符（local
descriptor）是使得匹配的特征点的描述符一致或距离相近，不匹配的特征点的描述符距离较远。对应于上图，即每一列样本之间的描述符应距离相近，而每一行的样本之间的描述符应距离较远。</p>
<p>在数学上，假设有一特征点<span
class="math inline">\(A\)</span>，其描述符标为<span
class="math inline">\(\textbf{a}\)</span>，称之参考样本或锚点（anchor）。与之匹配的特征点<span
class="math inline">\(P\)</span>可被称为正样本（positive
sample），其描述符标为<span
class="math inline">\(\textbf{p}\)</span>。与之匹配的特征点<span
class="math inline">\(N\)</span>可被称为负样本（negative
sample），其描述符标为<span
class="math inline">\(\textbf{n}\)</span>。理想情况下，参考样本与正样本之间的距离应远小于参考样本与负样本之间的距离，即<span
class="math inline">\(0 \simeq dist(\textbf{a}, \textbf{p}) \ll
dist(\textbf{a}, \textbf{n})\)</span>。</p>
<h2 id="训练流程">训练流程</h2>
<p>对于基本Patch的描述符（local
descriptor），一个常见的训练流程如下图所示。</p>
<img data-src="/zh/2022/02/27/zh/patch_based_local_descriptor/general_pipeline.jpg" class="" title="Pipeline">
<h3 id="数据采样">数据采样</h3>
<p>具体而言，随机选取<span
class="math inline">\(n\)</span>对匹配的patches组成最终的training
batch，所以最终的training batch为<span class="math inline">\(\textbf{x}
= (\textbf{A}_i, \textbf{P}_{i})_{i=1,...,n}\)</span>。其中<span
class="math inline">\((\textbf{A}_i, \textbf{P}_{i})\)</span>
表示对应的一对匹配的patch对。</p>
<h3 id="描述子和距离矩阵">描述子和距离矩阵</h3>
<p>通过神经网络，可以生成样本<span class="math inline">\((\textbf{A}_i,
\textbf{P}_{i})\)</span>所对应的描述子<span
class="math inline">\((\textbf{a}_i,
\textbf{p}_{i})\)</span>。针对于同一batch中的参考样本和正样本俩俩组合计算距离，可得到上图右侧所示的距离矩阵。当组合的参考样本和正样本匹配时，则该距离为正样本距离，不匹配时，则该距离为负样本样本距离。</p>
<p>对于描述子和距离矩阵，可采用不同的损失函数，进而学习网络。</p>
<h2 id="学习策略">学习策略</h2>
<h3
id="l2net-l2-net-deep-learning-of-discriminative-patch-descriptor-in-euclidean-space">L2Net
| <a
href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Tian_L2-Net_Deep_Learning_CVPR_2017_paper.pdf">L2-Net:
Deep Learning of Discriminative Patch Descriptor in Euclidean
Space</a></h3>
<p>L2Net是相对早期的Patch-Based基于深度学习的描述符。其损失函数基本已不再使用，但其模型直至现在仍被广泛使用。</p>
<p>L2Net损失函数主要分为三个部分： - Error term for descriptor
similarity：利用相对距离区分匹配上和未匹配上的 patch
pairs，即在距离矩阵的行和列上求softmax； - Error term for descriptor
compactness：考虑最后输出特征向量的
compactness，也就是特征向量的各个维度尽可能不相关。即在相关矩阵上加penalty；
- Error term for intermediate feature maps：学习过程中间的 feature maps
进行额外的监督，可以得到更好的性能。即求每个中间层的距离矩阵，并在该距离矩阵的行和列上求softmax。</p>
<p>L2Net模型如下图所示。L2Net采用单路全卷积框架，图中3x3
Conv代表Conv+BN+Relu，8×8
Conv代表Conv+BN，在第三层和第五层的卷积层步长为2用于下采样。LRN(Local
Response Normalization
layer)用于归一化输出，等价于L2Norm。需要注意的是，由于区域图像变化较大，为了消除光照等其他因素的影响，一般也会在模型初始加InstanceNorm。</p>
<pre class="mermaid">
flowchart LR
Input[Patch] --&gt; B(3x3 Conv 32):::Conv
B --&gt; C(3x3 Conv 32):::Conv
C --&gt; D(3x3 Conv 64&#x2F;2):::ConvDown
D --&gt; E(3x3 Conv 64):::Conv
E --&gt; F(3x3 Conv 128&#x2F;2):::ConvDown
F --&gt; G(3x3 Conv 128):::Conv
G --&gt; H(8x8 Conv 128):::ConvBN
H --&gt; J(LRN):::LRN
J --&gt; Descriptor(Descriptor)

classDef Conv fill:#cfc;
classDef ConvDown fill:#cff;
classDef ConvBN fill:#ffc;
classDef LRN fill:#fcc;
</pre>
<p>该网络简单直接，特征提取速度在ms级，在低端gpu上基本在1ms左右。</p>
<h3
id="hardnet-working-hard-to-know-your-neighbors-margins-local-descriptor-learning-loss">HardNet
| <a href="https://arxiv.org/abs/1705.10872">Working hard to know your
neighbor's margins: Local descriptor learning loss</a></h3>
<p>HardNet的训练流程如下图所示。</p>
<img data-src="/zh/2022/02/27/zh/patch_based_local_descriptor/hardnet_pipeline.jpg" class="" title="HardNet_Pipeline">
<p>简单而讲，HardNet采用了度量学习中的triplet loss去最大化training
batch中正负样本之间的距离。</p>
<p>具体而言，当距离矩阵已知时，对于匹配的Patch对，可以找到： - <span
class="math inline">\(\textbf{a}_i\)</span>: anchor 描述符 - <span
class="math inline">\(\textbf{p}_{i}\)</span> : positive 描述符 - <span
class="math inline">\(\textbf{p}_{j_{ \min }}\)</span>: 表示距离<span
class="math inline">\(\textbf{a}_i\)</span>最近的非匹配描述符，其中
<span class="math inline">\(j_{\min} = \arg\min_{j=1 . . n , j \neq i }
d \left( a _ { i } , p _ { j }\right)\)</span> ，在上图中对应 <span
class="math inline">\(\textbf{p}_{4}\)</span> - <span
class="math inline">\(\textbf{a}_{k_{ \min }}\)</span>: 表示距离 <span
class="math inline">\(\textbf{p}_{i}\)</span>最近的非匹配描述符，其中
<span class="math inline">\(k_{\min} = \arg\min_{k=1 . . n , k \neq i }
d \left( a _ { k } , p _ { i }\right)\)</span>，在上图中对应 <span
class="math inline">\(\textbf{a}_2\)</span></p>
<p>这样对于每个匹配的 patch pair 在都可以生成一个四元组 <span
class="math inline">\((\textbf{a}_i, \textbf{p}_{i}, \textbf{p}_{j_{
\min }}, \textbf{a}_{k_{ \min }})\)</span>。而在四元组的距离包括:</p>
<ul>
<li><span class="math inline">\(d \left( \textbf{a}_i , \textbf{p}_{i}
\right)\)</span>: anchor-positive 距离</li>
<li><span class="math inline">\(d \left( \textbf{a}_i , \textbf{p}_{j_{
\min }} \right)\)</span>: anchor-negative 距离</li>
<li><span class="math inline">\(d \left( \textbf{a}_{k_{ \min }} ,
\textbf{p}_{i} \right)\)</span>: anchor-negative 距离</li>
</ul>
<p>由于最后的目标是最大化training
batch中正负样本之间的距离。所以将选取最难的样本进行训练，对应于anchor-negative
距离中，即意味着距离最小的样本。因此，最终的损失函数为：</p>
<p><span class="math display">\[L = \frac { 1 } { n } \sum _ { i = 1 , n
} \max \left( 0,1 + d \left( \textbf{a}_i , \textbf{p}_{i} \right) -
\min \left( d \left( \textbf{a}_i , \textbf{p}_{j_{ \min }} \right) , d
\left( \textbf{a}_{k_{ \min }} , \textbf{p}_{i} \right) \right)
\right)\]</span></p>
<p>损失函数对应的代码为</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">triplet_loss</span>(<span class="params">x, label, margin=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="comment"># x is D x N</span></span><br><span class="line">    dim = x.size(<span class="number">0</span>) <span class="comment"># D</span></span><br><span class="line">    nq = torch.<span class="built_in">sum</span>(label.data==-<span class="number">1</span>).item() <span class="comment"># number of tuples</span></span><br><span class="line">    S = x.size(<span class="number">1</span>) // nq <span class="comment"># number of images per tuple including query: 1+1+n</span></span><br><span class="line"></span><br><span class="line">    xa = x[:, label.data==-<span class="number">1</span>].permute(<span class="number">1</span>,<span class="number">0</span>).repeat(<span class="number">1</span>,S-<span class="number">2</span>).view((S-<span class="number">2</span>)*nq,dim).permute(<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">    xp = x[:, label.data==<span class="number">1</span>].permute(<span class="number">1</span>,<span class="number">0</span>).repeat(<span class="number">1</span>,S-<span class="number">2</span>).view((S-<span class="number">2</span>)*nq,dim).permute(<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">    xn = x[:, label.data==<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    dist_pos = torch.<span class="built_in">sum</span>(torch.<span class="built_in">pow</span>(xa - xp, <span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line">    dist_neg = torch.<span class="built_in">sum</span>(torch.<span class="built_in">pow</span>(xa - xn, <span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">sum</span>(torch.clamp(dist_pos - dist_neg + margin, <span class="built_in">min</span>=<span class="number">0</span>)) / nq</span><br></pre></td></tr></table></figure>
<h3
id="sosnet-sosnet-second-order-similarity-regularization-for-local-descriptor-learning">SOSNet
| <a href="https://arxiv.org/abs/1904.05019">SOSNet: Second Order
Similarity Regularization for Local Descriptor Learning</a></h3>
<p>HardNet的loss是由距离度量直接组成的，其可以被认为是基于一阶相似距离的loss。SOSNet在此之上又增加了基于二阶相似距离的loss。</p>
<p>所谓二阶距离，这里指在四元组 <span
class="math inline">\((\textbf{a}_i, \textbf{p}_{i}, \textbf{p}_{j_{
\min }}, \textbf{a}_{k_{ \min }})\)</span> 中两个 anchor-negative
距离的距离， 即 <span class="math inline">\(d \left( d \left(
\textbf{a}_i , \textbf{p}_{j_{ \min }} \right) , d \left(
\textbf{a}_{k_{ \min }} , \textbf{p}_{i} \right)
\right)\)</span>。通过最小化二阶距离，可使得local
descriptor更加聚积，如下图所示。</p>
<img data-src="/zh/2022/02/27/zh/patch_based_local_descriptor/second_order_dist.jpg" class="" title="Second_Order_Distance">
<img data-src="/zh/2022/02/27/zh/patch_based_local_descriptor/sosnet.jpg" class="" title="SOSNet_output">
<p>二阶距离损失函数对应的代码为</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sos_loss</span>(<span class="params">x, label</span>):</span><br><span class="line">    <span class="comment"># x is D x N</span></span><br><span class="line">    dim = x.size(<span class="number">0</span>) <span class="comment"># D</span></span><br><span class="line">    nq = torch.<span class="built_in">sum</span>(label.data==-<span class="number">1</span>).item() <span class="comment"># number of tuples</span></span><br><span class="line">    S = x.size(<span class="number">1</span>) // nq <span class="comment"># number of images per tuple including query: 1+1+n</span></span><br><span class="line"></span><br><span class="line">    xa = x[:, label.data==-<span class="number">1</span>].permute(<span class="number">1</span>,<span class="number">0</span>).repeat(<span class="number">1</span>,S-<span class="number">2</span>).view((S-<span class="number">2</span>)*nq,dim).permute(<span class="number">1</span>,<span class="number">0</span>) <span class="comment"># D * (B * num_neg)</span></span><br><span class="line">    xp = x[:, label.data==<span class="number">1</span>].permute(<span class="number">1</span>,<span class="number">0</span>).repeat(<span class="number">1</span>,S-<span class="number">2</span>).view((S-<span class="number">2</span>)*nq,dim).permute(<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">    xn = x[:, label.data==<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    dist_an = torch.<span class="built_in">sum</span>(torch.<span class="built_in">pow</span>(xa - xn, <span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line">    dist_pn = torch.<span class="built_in">sum</span>(torch.<span class="built_in">pow</span>(xp - xn, <span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">sum</span>(torch.<span class="built_in">pow</span>(dist_an - dist_pn, <span class="number">2</span>)) ** <span class="number">0.5</span> / nq</span><br></pre></td></tr></table></figure>
<h3
id="log-polar-transformation-beyond-cartesian-representations-for-local-descriptors">Log
Polar Transformation | <a href="https://arxiv.org/abs/1908.05547">Beyond
Cartesian Representations for Local Descriptors</a></h3>
<p>另一篇对局部特征很有帮助的工作是Log Polar
Transformation。其核心是通过在图像预处理中引入Log Polar
Transformation，将图像从欧式坐标系转换为<a
href="https://en.wikipedia.org/wiki/Log-polar_coordinates">对数极坐标系</a>表示。由于不同的旋转和尺度变化在对数极坐标系下对图像不会有太大影响，因此Log
Polar Transformation会使得模型对旋转和尺度变化的鲁棒性较好。</p>
<img data-src="/zh/2022/02/27/zh/patch_based_local_descriptor/log_polar_transformation.jpg" class="" title="Rotation and Scaling transformations to a Euclidean image can be read as horizontal and vertical shift respectively, after a log-polar transformation. The log-polar transformation translates rotation and scale in Euclidean images into vertical and horizontal translations (respectively) in the log-polar model.">
<p>如上图所示，在欧式图像上的旋转和尺度变化对应到对数极坐标系下是竖直和水平方向上的平移变化。而卷积操作的滑动窗口计算方式使得其对平移变化非常鲁棒。需要注意的是，由于一般Patch-based卷积神经网络都非常浅，导致其无法很好的消除由旋转的尺度变化所导致的偏差。甚至为了消除这种偏差，在工程上常常使用重力的方向来旋转输入CNN的Patch。因此，Log
Polar
Transformation通过增强模型对旋转和尺度变化的鲁棒性，进而提升了模型的鲁棒性和准确度。</p>
<p>更多的旋转和尺度变化在对欧式坐标系和对数极坐标系的表现如下图所示。</p>
<img data-src="/zh/2022/02/27/zh/patch_based_local_descriptor/log_poloar_samples.jpg" class="" title="Example of rotation and scale transformations for a single MNIST image.">
<p>Log Polar Transformation在图像预处理时可以实现为</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">to_log_polar</span>(<span class="params">img, dsize, max_radius</span>):</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">isinstance</span>(img, Image.Image)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dsize = img.size</span></span><br><span class="line">    center = [s // <span class="number">2</span> <span class="keyword">for</span> s <span class="keyword">in</span> img.size]</span><br><span class="line">    flags = cv2.WARP_POLAR_LOG</span><br><span class="line">    out = cv2.warpPolar(</span><br><span class="line">        np.asarray(img), dsize=dsize, center=center, maxRadius=max_radius, flags=flags</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> Image.fromarray(out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">from_log_polar</span>(<span class="params">polar_img, dsize, max_radius</span>):</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">isinstance</span>(polar_img, Image.Image)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dsize = polar_img.size</span></span><br><span class="line">    center = [s // <span class="number">2</span> <span class="keyword">for</span> s <span class="keyword">in</span> polar_img.size]</span><br><span class="line">    flags = cv2.WARP_POLAR_LOG | cv2.WARP_INVERSE_MAP</span><br><span class="line">    out = cv2.warpPolar(</span><br><span class="line">        np.asarray(polar_img),</span><br><span class="line">        dsize=dsize,</span><br><span class="line">        center=center,</span><br><span class="line">        maxRadius=max_radius,</span><br><span class="line">        flags=flags,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> Image.fromarray(out)</span><br></pre></td></tr></table></figure>
<p>需要注意的是由于pytorch GridSample 的计算方式不同，用opencv实现的log
polar
transform会与论文原始PTN网络输出不一致，对于原始PTN网络实现可以参考<a
href="https://github.com/cvlab-epfl/log-polar-descriptors/blob/master/modules/ptn/pytorch/models.py">论文代码</a>。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a
href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Tian_L2-Net_Deep_Learning_CVPR_2017_paper.pdf">L2-Net:
Deep Learning of Discriminative Patch Descriptor in Euclidean
Space</a></li>
<li><a href="https://arxiv.org/abs/1705.10872">Working hard to know your
neighbor's margins: Local descriptor learning loss</a></li>
<li><a href="https://arxiv.org/abs/1904.05019">SOSNet: Second Order
Similarity Regularization for Local Descriptor Learning</a></li>
<li><a href="https://arxiv.org/abs/1908.05547">Beyond Cartesian
Representations for Local Descriptors</a></li>
<li><a href="https://arxiv.org/abs/1911.01141">Human eye inspired
log-polar pre-processing for neural networks</a></li>
</ul>
]]></content>
      <categories>
        <category>computer vision</category>
        <category>local descriptor</category>
      </categories>
      <tags>
        <tag>local descriptor</tag>
      </tags>
  </entry>
  <entry>
    <title>pybind:为cpp/cuda代码提供python接口</title>
    <url>/zh/2020/09/20/zh/python_cpp_extension/</url>
    <content><![CDATA[<p>python调用C++/CUDA有不少的方法，如boost.python, cython,
pybind11等。其中，pybind11是一个轻量级的仅标头的库。由于pybind11的易用性，pybind11被很多库用于于创建现有C++/CUDA代码的Python绑定，比如pytorch，tvm等。此外，由于Python的缓冲区协议可以公开自定义数据类型的内部存储，python的矩阵类型（如numpy.ndarry,torch.Tensor）可以快速转化到C++中对应矩阵类型（如Eigen，cv::Mat，vector等），不须额外的复制操作。本篇文章将通过一个数列求和的例子来讲解如何使用pybind11来将C++/CUDA代码进行Python绑定。</p>
<p>在通过C++和CUDA实现数列求和，在将其绑定为python函数，并在python中调用对应函数，验证结果。</p>
<span id="more"></span>
<h2 id="基本环境">基本环境</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GCC: 7.5</span><br><span class="line">CUDA: 10.2</span><br><span class="line">CUDNN: 7.6.5</span><br><span class="line">Python: 3.7</span><br><span class="line">pybind11: 2.5</span><br></pre></td></tr></table></figure>
<h2 id="hello-world-for-pybind11">"hello, world!" for pybind11</h2>
<p>不同于pybind11官网推荐的通过CMake编译的编译方式，本文将采用pytorch中pybind11的编译方式，即在setup创建python包时编译项目中C++和CUDA代码。</p>
<details>
<summary>
代码："hello, world!"
</summary>
<p>工程目录及编译： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">.../pybind$ tree</span><br><span class="line">.</span><br><span class="line">└── hello_world</span><br><span class="line">    ├── cpp_extension.py</span><br><span class="line">    ├── hello_world.cpp</span><br><span class="line">    └── setup_hello.py</span><br><span class="line">.../pybind$ python setup_hello.py clean -a install</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p><code>hello_world.cpp</code>: <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;pybind11/pybind11.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> py = pybind11;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HelloRobot</span> &#123;</span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        <span class="built_in">HelloRobot</span>(<span class="type">const</span> std::string &amp; robot_name) : <span class="built_in">robot_name_</span>(robot_name) &#123;&#125;;</span><br><span class="line">        <span class="function"><span class="type">void</span> <span class="title">hello</span><span class="params">(std::string guest_name)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(guest_name == <span class="string">&quot;&quot;</span>) &#123;</span><br><span class="line">                guest_name = <span class="string">&quot;World&quot;</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Hello, &quot;</span> + guest_name + <span class="string">&quot;! I&#x27;m &quot;</span> + robot_name_ + <span class="string">&quot;.\n&quot;</span>;</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="function">std::string <span class="title">get_robot_name</span><span class="params">()</span> </span>&#123;<span class="keyword">return</span> robot_name_;&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span>:</span><br><span class="line">        std::string robot_name_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(hello_world, m) &#123;</span><br><span class="line">    m.<span class="built_in">doc</span>() = <span class="string">&quot;pybind11 hello world&quot;</span>;</span><br><span class="line"></span><br><span class="line">    py::<span class="built_in">class_</span>&lt;HelloRobot&gt;(m, <span class="string">&quot;HelloRobot&quot;</span>)</span><br><span class="line">        .<span class="built_in">def</span>(py::<span class="built_in">init</span>&lt;<span class="type">const</span> std::string &amp;&gt;())</span><br><span class="line">        .<span class="built_in">def</span>(<span class="string">&quot;hello&quot;</span>, &amp;HelloRobot::hello, <span class="string">&quot;Provide your name...&quot;</span>, py::<span class="built_in">arg</span>(<span class="string">&quot;guest_name&quot;</span>)=<span class="string">&quot;&quot;</span>)</span><br><span class="line">        .<span class="built_in">def_property_readonly</span>(<span class="string">&quot;robot_name&quot;</span>, &amp;HelloRobot::get_robot_name);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>setup_hello.py</code>: <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> cpp_extension <span class="keyword">import</span> BuildExtension, CUDAExtension</span><br><span class="line"></span><br><span class="line">setup(</span><br><span class="line">    name=<span class="string">&#x27;hello&#x27;</span>,</span><br><span class="line">    version=<span class="string">&#x27;0.1&#x27;</span>,</span><br><span class="line">    description=<span class="string">&#x27;pybind11 hello world&#x27;</span>,</span><br><span class="line">    python_requires=<span class="string">&#x27;&gt;=3.7&#x27;</span>,</span><br><span class="line">    setup_requires=[<span class="string">&#x27;pybind11&gt;=2.5.0&#x27;</span>],</span><br><span class="line">    ext_modules=[CUDAExtension(<span class="string">&#x27;hello_world&#x27;</span>,</span><br><span class="line">        [<span class="string">&quot;hello_world.cpp&quot;</span>],</span><br><span class="line">        extra_compile_args=&#123;</span><br><span class="line">            <span class="string">&#x27;cxx&#x27;</span>: [<span class="string">&#x27;-std=c++14&#x27;</span>, <span class="string">&#x27;-O2&#x27;</span>, <span class="string">&#x27;-Wall&#x27;</span>],</span><br><span class="line">            <span class="string">&#x27;nvcc&#x27;</span>: [</span><br><span class="line">                <span class="string">&#x27;-std=c++14&#x27;</span>, <span class="string">&#x27;--expt-extended-lambda&#x27;</span>, <span class="string">&#x27;--use_fast_math&#x27;</span>, <span class="string">&#x27;-Xcompiler&#x27;</span>, <span class="string">&#x27;-Wall&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;-gencode=arch=compute_60,code=sm_60&#x27;</span>, <span class="string">&#x27;-gencode=arch=compute_61,code=sm_61&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;-gencode=arch=compute_70,code=sm_70&#x27;</span>, <span class="string">&#x27;-gencode=arch=compute_72,code=sm_72&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;-gencode=arch=compute_75,code=sm_75&#x27;</span>, <span class="string">&#x27;-gencode=arch=compute_75,code=compute_75&#x27;</span></span><br><span class="line">            ],</span><br><span class="line">        &#125;,</span><br><span class="line">        include_dirs = [],</span><br><span class="line">        library_dirs = [<span class="string">&#x27;/usr/local/lib&#x27;</span>, <span class="string">&#x27;/usr/local/lib64/&#x27;</span>],</span><br><span class="line">        )</span><br><span class="line">    ],</span><br><span class="line">    cmdclass=&#123;<span class="string">&#x27;build_ext&#x27;</span>: BuildExtension.with_options(no_python_abi_suffix=<span class="literal">True</span>, use_ninja=<span class="literal">False</span>)&#125;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>简单测试： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">.../pybind/hello_world$ python</span><br><span class="line">Python <span class="number">3.7</span><span class="number">.7</span> (default, May  <span class="number">7</span> <span class="number">2020</span>, <span class="number">21</span>:<span class="number">25</span>:<span class="number">33</span>) </span><br><span class="line">[GCC <span class="number">7.3</span><span class="number">.0</span>] :: Anaconda, Inc. on linux</span><br><span class="line"><span class="type">Type</span> <span class="string">&quot;help&quot;</span>, <span class="string">&quot;copyright&quot;</span>, <span class="string">&quot;credits&quot;</span> <span class="keyword">or</span> <span class="string">&quot;license&quot;</span> <span class="keyword">for</span> more information.</span><br><span class="line">ModuleNotFoundError: No module named <span class="string">&#x27;hello&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> hello_world</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>hello_world.HelloRobot</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;hello_world.HelloRobot&#x27;</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>robot = hello_world.HelloRobot(<span class="string">&quot;Eva&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>robot.robot_name</span><br><span class="line"><span class="string">&#x27;Eva&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>robot.hello()</span><br><span class="line">Hello, World! I<span class="string">&#x27;m Eva.</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; robot.hello(&quot;Young&quot;)</span></span><br><span class="line"><span class="string">Hello, Young! I&#x27;</span>m Eva.</span><br></pre></td></tr></table></figure></p>
<p>通过pybind11生成模块的相关文档： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> hello_world</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">help</span>(hello_world)</span><br><span class="line"></span><br><span class="line">Help on module hello_world:</span><br><span class="line"></span><br><span class="line">NAME</span><br><span class="line">    hello_world - pybind11 hello world</span><br><span class="line"></span><br><span class="line">CLASSES</span><br><span class="line">    pybind11_builtins.pybind11_object(builtins.<span class="built_in">object</span>)</span><br><span class="line">        HelloRobot</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">HelloRobot</span>(pybind11_builtins.pybind11_object)</span><br><span class="line">     |  Method resolution order:</span><br><span class="line">     |      HelloRobot</span><br><span class="line">     |      pybind11_builtins.pybind11_object</span><br><span class="line">     |      builtins.<span class="built_in">object</span></span><br><span class="line">     |  </span><br><span class="line">     |  Methods defined here:</span><br><span class="line">     |  </span><br><span class="line">     |  __init__(...)</span><br><span class="line">     |      __init__(self: hello_world.HelloRobot, arg0: <span class="built_in">str</span>) -&gt; <span class="literal">None</span></span><br><span class="line">     |  </span><br><span class="line">     |  hello(...)</span><br><span class="line">     |      hello(self: hello_world.HelloRobot, guest_name: <span class="built_in">str</span> = <span class="string">&#x27;&#x27;</span>) -&gt; <span class="literal">None</span></span><br><span class="line">     |      </span><br><span class="line">     |      Provide your name...</span><br><span class="line">     |  </span><br><span class="line">     |  ----------------------------------------------------------------------</span><br><span class="line">     |  Data descriptors defined here:</span><br><span class="line">     |  </span><br><span class="line">     |  robot_name</span><br><span class="line">     |  </span><br><span class="line">     |  ----------------------------------------------------------------------</span><br><span class="line">     |  Static methods inherited <span class="keyword">from</span> pybind11_builtins.pybind11_object:</span><br><span class="line">     |  </span><br><span class="line">     |  __new__(*args, **kwargs) <span class="keyword">from</span> pybind11_builtins.pybind11_type</span><br><span class="line">     |      Create <span class="keyword">and</span> <span class="keyword">return</span> a new <span class="built_in">object</span>.  See <span class="built_in">help</span>(<span class="built_in">type</span>) <span class="keyword">for</span> accurate signature.</span><br></pre></td></tr></table></figure></p>
</details>
<h2 id="数列求和">数列求和</h2>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>cuda</tag>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title>通过NPP加速TensorRT部署时图片数据预处理</title>
    <url>/zh/2020/06/17/zh/trt_preproc_npp/</url>
    <content><![CDATA[<p>TensorRT(TRT)是NVIDIA推出的一个高性能的深度学习推理框架，可以让深度学习模型在NVIDIA
GPU上实现低延迟，高吞吐量的部署。主流框架的模型可以通过转换为TensorRT在NVIDIA
GPU进而达到极大地提速。然而，由于TensorRT并不支持常见的图片数据类型uint8，这使得往往需要在cpu上进行图片数据预处理，转换为其所支持的float并传入到gpu模型输入。当图片较大时，数据在cpu上的处理和传递时间较慢。本文将介绍如何通过cuda中的npp库来加速这一过程。</p>
<span id="more"></span>
<h2 id="基本环境">基本环境</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SYS: Ubuntu 18.04</span><br><span class="line">GPU: T4</span><br><span class="line">GCC: 7.5</span><br><span class="line">CMake: 3.16.6</span><br><span class="line">CUDA: 10.2</span><br><span class="line">CUDNN: 7.6.5</span><br><span class="line">TensorRT: 7.0</span><br><span class="line">OpenCV: 4.3.0/3.4.10</span><br></pre></td></tr></table></figure>
<p>模型是由ssds.pytorch训练的Yolov3目标检测器，已转换为TRT模型。其输入大小为1x3x736x1280，特征提取器为ResNet18，计算精度为int8。</p>
<p>需要注意，当将模型转换为TRT模型时，TRT会根据gpu框架和性能来来选择不同的核函数及其参数，进而最大程度优化推理速度。因此TRT在执行时，必须使用统一gpu框架所生成的TRT模型，否则将无法推断。并且即使是统一框架不同型号的gpu所生成的TRT模型，其推理速度也会有些许削弱。比如虽然2080ti和t4同属7.5计算框架，在T4上推断，由2080ti所生成的TRT模型要比由T4生成的模型推断速度慢3~10%。</p>
<h2 id="cpu图片数据预处理">CPU图片数据预处理</h2>
<p>在一些深度学习框架中，在其推断时可以指定推断时所接受的数据类型，并将数据预处理的步骤在计算图中定义。如在tensorflow将权重转化为推断模型(frozen
graph)时，可以通过<code>tf.placeholder(dtype=tf.uint8, shape=input_shape, name='image_tensor')</code>来指定推理时模型接受的数据类型。这种改变在推理时模型的操作在TensorRT上似乎不那么行得通。虽然TensorRT支持多种数据类型，并且输入输出接口的数据类型可由转换的onnx或uff文件决定，但当输入输出的数据类型改为除float32以外的其他类型时，常常无法成功转换为TRT推理模型。</p>
<p>TensorRT中这种输入输出类型的限制，对于计算机视觉的模型显得更加不友好。计算机视觉常处理的图像或视频在计算机中常存储为0～255的uint8数据，这种类型本身就不被TensorRT所支持，须将图片先转为float数据再输入到TensorRT。而在一些任务中，其输入模型的图片或视频片段分辨率较大，如4k或8k，uint8和float从cpu内存传输到gpu显存的速度差别就比较大。这些原因导致了在计算机视觉模型部署时，图片数据预处理和传输有时候成为了模型部署的瓶颈。</p>
<p>大多数github上的TRT项目在进行推断中的图片数据预处理常采用TensorRT<a
href="https://github.com/NVIDIA/TensorRT/blob/572d54f91791448c015e74a4f1d6923b77b79795/samples/opensource/sampleSSD/sampleSSD.cpp#L276-L309">官方示例</a>中给出了一种在CPU图片数据预处理。笔者个人喜欢用OpenCV自带函数进行操作，这两种图片数据预处理的示例代码如下。</p>
<details>
<summary>
代码：CPU图片数据预处理
</summary>
<p>TensorRT官方示例图片数据预处理代码 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">SampleUffSSD::processInput</span><span class="params">(<span class="type">const</span> samplesCommon::BufferManager&amp; buffers)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> inputC = mInputDims.d[<span class="number">0</span>];</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> inputH = mInputDims.d[<span class="number">1</span>];</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> inputW = mInputDims.d[<span class="number">2</span>];</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> batchSize = mParams.batchSize;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Available images</span></span><br><span class="line">    std::vector&lt;std::string&gt; imageList = &#123;<span class="string">&quot;dog.ppm&quot;</span>, <span class="string">&quot;bus.ppm&quot;</span>&#125;;</span><br><span class="line">    mPPMs.<span class="built_in">resize</span>(batchSize);</span><br><span class="line">    <span class="built_in">assert</span>(mPPMs.<span class="built_in">size</span>() &lt;= imageList.<span class="built_in">size</span>());</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; batchSize; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">readPPMFile</span>(<span class="built_in">locateFile</span>(imageList[i], mParams.dataDirs), mPPMs[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span>* hostDataBuffer = <span class="built_in">static_cast</span>&lt;<span class="type">float</span>*&gt;(buffers.<span class="built_in">getHostBuffer</span>(mParams.inputTensorNames[<span class="number">0</span>]));</span><br><span class="line">    <span class="comment">// Host memory for input buffer</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>, volImg = inputC * inputH * inputW; i &lt; mParams.batchSize; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> c = <span class="number">0</span>; c &lt; inputC; ++c)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// The color image to input should be in BGR order</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">unsigned</span> j = <span class="number">0</span>, volChl = inputH * inputW; j &lt; volChl; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                hostDataBuffer[i * volImg + c * volChl + j]</span><br><span class="line">                    = (<span class="number">2.0</span> / <span class="number">255.0</span>) * <span class="built_in">float</span>(mPPMs[i].buffer[j * inputC + c]) - <span class="number">1.0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>OpenCV图片数据预处理代码 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">imageToTensor</span><span class="params">(<span class="type">const</span> std::vector&lt;cv::Mat&gt; &amp; images, <span class="type">float</span> * tensor, <span class="type">const</span> <span class="type">int</span> batch_size, <span class="type">const</span> <span class="type">float</span> alpha, <span class="type">const</span> <span class="type">float</span> beta)</span> </span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> height = images[<span class="number">0</span>].rows;</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> width = images[<span class="number">0</span>].cols;</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> channels = images[<span class="number">0</span>].<span class="built_in">channels</span>();</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> stridesCv[<span class="number">3</span>] = &#123; width * channels, channels, <span class="number">1</span> &#125;;</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> strides[<span class="number">4</span>] = &#123; height * width * channels, height * width, width, <span class="number">1</span> &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> omp parallel for num_threads(c_numOmpThread) schedule(static, 1)</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> b = <span class="number">0</span>; b &lt; batch_size; b++)</span><br><span class="line">    &#123;</span><br><span class="line">        cv::Mat image_f;</span><br><span class="line">        images[b].<span class="built_in">convertTo</span>(image_f, CV_32F, alpha, beta);</span><br><span class="line">        std::vector&lt;cv::Mat&gt; split_channels = &#123;</span><br><span class="line">                cv::<span class="built_in">Mat</span>(images[b].<span class="built_in">size</span>(),CV_32FC1,tensor + b * strides[<span class="number">0</span>]),</span><br><span class="line">                cv::<span class="built_in">Mat</span>(images[b].<span class="built_in">size</span>(),CV_32FC1,tensor + b * strides[<span class="number">0</span>] + strides[<span class="number">1</span>]),</span><br><span class="line">                cv::<span class="built_in">Mat</span>(images[b].<span class="built_in">size</span>(),CV_32FC1,tensor + b * strides[<span class="number">0</span>] + <span class="number">2</span>*strides[<span class="number">1</span>]),</span><br><span class="line">        &#125;;</span><br><span class="line">        cv::<span class="built_in">split</span>(image_f, split_channels);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> batch_size * height * width * channels;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>OpenCV图片数据预处理运算速度(ms)</p>
<table>
<colgroup>
<col style="width: 38%" />
<col style="width: 17%" />
<col style="width: 13%" />
<col style="width: 15%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">GPU(Precision)</th>
<th style="text-align: center;">Image2Float</th>
<th style="text-align: center;">Copy2GPU</th>
<th style="text-align: center;">Inference</th>
<th style="text-align: center;">GPU2CPU</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">t4(int8)</td>
<td style="text-align: center;">2.53026</td>
<td style="text-align: center;">0.935451</td>
<td style="text-align: center;">2.56143</td>
<td style="text-align: center;">0.0210528</td>
</tr>
</tbody>
</table>
</details>
<p>如上例代码所示，在CPU图片数据预处理中，首先将数据转化为float类型并做归一化，然后将数据排列从NHWC转化为NCHW，最后将float型nchw图片数据传递给到TRT模型预留的gpu显存。可以看到，对于该模型图像预处理和传输的速度反而大于模型推断速度。这种cpu图片预处理方式无法高效使用gpu的性能，使得整个模型部署效率较低。</p>
<h2 id="gpu-npp图片数据预处理">GPU-NPP图片数据预处理</h2>
<p>上面提到，cpu图片数据预处理效率较低由两方面原因导致：其一，cpu将图像从uint8转化为float32的效率较低；其二，相较于uint8，float32从cpu传输到gpu数据量大四倍，传输效率较慢。所以比较朴素的提速想法就是将uint8数据传输到gpu，并由gpu完成从uint8转化为float32的转化。NPP就可以方便快速的实现如上过程。</p>
<p>NPP是nvidia推出的用于gpu加速2D图像和信号处理的cuda库，其本身就内嵌于cuda库中。其分为多个部分，可以在gpu上高效的进行数据类型转换，颜色变化，几何变化等功能。本例中采用其中的NPPC，NPPIDEI和NPPIAL部分来进行图片数据预处理中的uint8到float32的数据类型转化，NHWC到NCHW的通道变化，以及归一化操作。其具体代码如下。</p>
<details>
<summary>
代码：GPU-NPP图片数据预处理
</summary>
<p>NPP图片数据预处理代码 <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">imageToTensorGPUFloat</span><span class="params">(<span class="type">const</span> std::vector&lt;cv::Mat&gt; &amp; images, <span class="type">void</span> * gpu_images, <span class="type">void</span> * tensor, <span class="type">const</span> <span class="type">int</span> batch_size, <span class="type">const</span> <span class="type">float</span> alpha)</span> </span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> height = images[<span class="number">0</span>].rows;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> width  = images[<span class="number">0</span>].cols;</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> channels = images[<span class="number">0</span>].<span class="built_in">channels</span>();</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> stride = height * width * channels;</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> stride_s = width * channels;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> dstOrder[<span class="number">3</span>] = &#123;<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>&#125;;</span><br><span class="line">    Npp32f scale[<span class="number">3</span>] = &#123;alpha, alpha, alpha&#125;;</span><br><span class="line">    NppiSize dstSize = &#123;width, height&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> omp parallel for num_threads(c_numOmpThread) schedule(static, 1)</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> b = <span class="number">0</span>; b &lt; batch_size; b++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cudaMemcpy</span>((Npp8u*)gpu_images + b * stride, images[b].data, stride, cudaMemcpyHostToDevice);</span><br><span class="line">        <span class="built_in">nppiSwapChannels_8u_C3IR</span>((Npp8u*)gpu_images + b * stride, stride_s, dstSize, dstOrder);</span><br><span class="line">        <span class="built_in">nppiConvert_8u32f_C3R</span>((Npp8u*)gpu_images + b * stride, stride_s, (Npp32f*)tensor, stride_s*<span class="built_in">sizeof</span>(<span class="type">float</span>), dstSize);</span><br><span class="line">        <span class="built_in">nppiMulC_32f_C3IR</span>(scale, (Npp32f*)tensor, stride_s*<span class="built_in">sizeof</span>(<span class="type">float</span>), dstSize);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> batch_size * stride;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>NPP图片数据预处理代码（无通道变化和归一化） <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">imageToTensorGPUFloat</span><span class="params">(<span class="type">const</span> std::vector&lt;cv::Mat&gt; &amp; images, <span class="type">void</span> * gpu_images, <span class="type">void</span> * tensor, <span class="type">const</span> <span class="type">int</span> batch_size)</span> </span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> height = images[<span class="number">0</span>].rows;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> width  = images[<span class="number">0</span>].cols;</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> channels = images[<span class="number">0</span>].<span class="built_in">channels</span>();</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> stride = height * width * channels;</span><br><span class="line">    NppiSize dstSize = &#123;width, height&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> omp parallel for num_threads(c_numOmpThread) schedule(static, 1)</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> b = <span class="number">0</span>; b &lt; batch_size; b++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cudaMemcpy</span>((Npp8u*)gpu_images + b * stride, images[b].data, stride, cudaMemcpyHostToDevice);</span><br><span class="line">        <span class="built_in">nppiConvert_8u32f_C3R</span>((Npp8u*)gpu_images + b * stride, width * channels, (Npp32f*)tensor, width * channels*<span class="built_in">sizeof</span>(<span class="type">float</span>), dstSize);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> batch_size * stride;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>NPP图片数据预处理（无通道变化和归一化）运算速度(ms)</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">GPU(Precision)</th>
<th style="text-align: center;">Image2GPU2Float</th>
<th style="text-align: center;">Inference</th>
<th style="text-align: center;">GPU2CPU</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">t4(int8)</td>
<td style="text-align: center;">0.532469</td>
<td style="text-align: center;">3.07869</td>
<td style="text-align: center;">0.0208867</td>
</tr>
</tbody>
</table>
</details>
<p>如上例代码所示，在GPU图片数据预处理中，首先将uint8数据传输到gpu显存中，然后将数据排列从NHWC转化为NCHW，最后转化为float类型并做归一化，归一化后的数据直接存储在TRT模型预留的gpu显存中。由于按位运算(elementwise)和通道转换(channel
permute)在TRT模型中执行效率较高，可将预处理中的归一化和通道转换移到模型内计算。可以看到，相较于cpu的图像预处理，gpu图像预处理的时间从3.5ms降到0.5ms，整个模型总运行时间从6ms降到3.5ms，每秒帧处理量（fps）从166帧提升到了285帧，整体达到了1.7倍的提速。</p>
<p>需要注意的是，由于TRT模型转化时间较长，本文示例只测试batch为1时的执行速度。如果在部署时遇到batch较大而导致gpu图片预处理速度较慢，由于cuda代码执行和传输特性，可考虑整批图像一起从cpu内存拷贝到gpu并进行uint8到float32的转化，进而提高大batch情况下的GPU图片数据预处理处理速度。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a
href="https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/infer/FoundationalTypes/DataType.html">tensorrt
document</a></li>
<li><a
href="https://developer.nvidia.com/cuda-gpus#compute">gpu计算框架</a></li>
<li><a
href="https://github.com/ShuangXieIrene/ssds.pytorch">ssds.pytorch</a></li>
</ul>
<p>如果有TensorRT的项目，不妨来试试通过本文提到的GPU图片数据预处理的方法来提速模型推断流程的速度吧！</p>
]]></content>
      <categories>
        <category>model inference</category>
      </categories>
      <tags>
        <tag>cuda</tag>
        <tag>cpp</tag>
        <tag>tensorrt</tag>
      </tags>
  </entry>
</search>

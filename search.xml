<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Preprocess the Image Data by NPP in TensorRT Model Inference</title>
    <url>/2020/06/17/en/trt_preproc_npp/</url>
    <content><![CDATA[<p>NVIDIA TensorRT(TRT) library is a high-performance deep learning
inference engine and delivers low latency and high-throughput for deep
learning inference applications. It allows users to convert the model
from other popular frameworks like pytorch or tensorflow. However,
TensorRT only supports float32 rather than uint8 data type as input data
type, which is the most common format for image data. In this case, when
we try to deploy the image-based tasks with TensorRT, it always needs to
convert the images from uint8 to float32, and then transfer the float32
image date to gpu to allow the TRT engine inference the model. When the
image size is large, this preprocessing stage is slightly slow. In this
blog, we are trying to introduce the NVIDIA NPP library to speed up this
preprocessing progress.</p>
<span id="more"></span>
<h2 id="experiments">Experiments</h2>
<p>This blog uses the environment shown below and used a detection model
trained by ssds.pytorch to do the experiments and evaluations. The model
uses ResNet18 as feature extractor and YoloV3 as detection head. The
model has already been converted to TRT with 1x3x736x1280 input and int8
computation precision.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SYS: Ubuntu 18.04</span><br><span class="line">GPU: T4</span><br><span class="line">GCC: 7.5</span><br><span class="line">CMake: 3.16.6</span><br><span class="line">CUDA: 10.2</span><br><span class="line">CUDNN: 7.6.5</span><br><span class="line">TensorRT: 7.0</span><br><span class="line">OpenCV: 4.3.0/3.4.10</span><br></pre></td></tr></table></figure>
<p>It should be noted that when the model is converted to a TRT model,
TRT will select different kernel functions and their parameters
according to the GPU framework, and thus optimize the inference speed.
Therefore, it has to use the same GPU framework for TRT model generation
and execution. And even the TRT model generated by different types of
GPUs with the same framework, its inference speed will be slightly
weakened based on its execution gpu machine. For example, although
2080ti and t4 belong to the same 7.5 computing framework, when we infer
the model on T4, the model generated by 2080ti is 3 to 10% slower than
the model generated by T4.</p>
<h2 id="cpu-image-preprocessing">CPU image preprocessing</h2>
<p>In some deep learning frameworks, it can specify the input data type
&amp; format and the data preprocessing ops in the inference graph. For
example, when we freeze the weights into the frozen graph in tensorflow,
we can specify the data type accepted by the model during inference
through
<code>tf.placeholder(dtype=tf.uint8, shape=input_shape, name='image_tensor')</code>.
This preprocessing way is not supportted on TensorRT. TensorRT does
support multiple data types, and the data type of the input and output
ops can be determined by the converted onnx/uff file. However, when the
input and output data type of onnx/uff model is changed to other types
than float32, it can not be successfully converted into a TRT inference
model in the most cases.</p>
<p>This data type limitation in TensorRT is even more unfriendly to the
computer vision models. Images or videos in computer vision tasks are
often stored in the computer as uint8 data ([0, 255]) which is not
supported by TensorRT. In this case, the images must be converted to
float and then do the TensorRT model inference. In some tasks, the
resolution of the images or video clips of the input model is large,
such as 4k or 8k, and it is slow to tranfer the data from uint8 to float
in cpu and from cpu memory to gpu memory. In some cases, the time cost
of pre-processing and transmission is the bottleneck in model
deployment.</p>
<p>Most of the TRT projects on github often use the official <a
href="https://github.com/NVIDIA/TensorRT/blob/572d54f91791448c015e74a4f1d6923b77b79795/samples/opensource/sampleSSD/sampleSSD.cpp#L276-L309">TensorRT
example</a> to preprocess the image data in cpu, while I prefer to use
the OpenCV functions to preprocess the images. The code for these two
methods are shown as below.</p>
<details>
<summary>
Code: Preprocess data in cpu
</summary>
<p>TensorRT official preprocessing code <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">SampleUffSSD::processInput</span><span class="params">(<span class="type">const</span> samplesCommon::BufferManager&amp; buffers)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> inputC = mInputDims.d[<span class="number">0</span>];</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> inputH = mInputDims.d[<span class="number">1</span>];</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> inputW = mInputDims.d[<span class="number">2</span>];</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> batchSize = mParams.batchSize;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Available images</span></span><br><span class="line">    std::vector&lt;std::string&gt; imageList = &#123;<span class="string">&quot;dog.ppm&quot;</span>, <span class="string">&quot;bus.ppm&quot;</span>&#125;;</span><br><span class="line">    mPPMs.<span class="built_in">resize</span>(batchSize);</span><br><span class="line">    <span class="built_in">assert</span>(mPPMs.<span class="built_in">size</span>() &lt;= imageList.<span class="built_in">size</span>());</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; batchSize; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">readPPMFile</span>(<span class="built_in">locateFile</span>(imageList[i], mParams.dataDirs), mPPMs[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span>* hostDataBuffer = <span class="built_in">static_cast</span>&lt;<span class="type">float</span>*&gt;(buffers.<span class="built_in">getHostBuffer</span>(mParams.inputTensorNames[<span class="number">0</span>]));</span><br><span class="line">    <span class="comment">// Host memory for input buffer</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>, volImg = inputC * inputH * inputW; i &lt; mParams.batchSize; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> c = <span class="number">0</span>; c &lt; inputC; ++c)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// The color image to input should be in BGR order</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">unsigned</span> j = <span class="number">0</span>, volChl = inputH * inputW; j &lt; volChl; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                hostDataBuffer[i * volImg + c * volChl + j]</span><br><span class="line">                    = (<span class="number">2.0</span> / <span class="number">255.0</span>) * <span class="built_in">float</span>(mPPMs[i].buffer[j * inputC + c]) - <span class="number">1.0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>OpenCV preprocessing code <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">imageToTensor</span><span class="params">(<span class="type">const</span> std::vector&lt;cv::Mat&gt; &amp; images, <span class="type">float</span> * tensor, <span class="type">const</span> <span class="type">int</span> batch_size, <span class="type">const</span> <span class="type">float</span> alpha, <span class="type">const</span> <span class="type">float</span> beta)</span> </span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> height = images[<span class="number">0</span>].rows;</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> width = images[<span class="number">0</span>].cols;</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> channels = images[<span class="number">0</span>].<span class="built_in">channels</span>();</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> stridesCv[<span class="number">3</span>] = &#123; width * channels, channels, <span class="number">1</span> &#125;;</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> strides[<span class="number">4</span>] = &#123; height * width * channels, height * width, width, <span class="number">1</span> &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> omp parallel for num_threads(c_numOmpThread) schedule(static, 1)</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> b = <span class="number">0</span>; b &lt; batch_size; b++)</span><br><span class="line">    &#123;</span><br><span class="line">        cv::Mat image_f;</span><br><span class="line">        images[b].<span class="built_in">convertTo</span>(image_f, CV_32F, alpha, beta);</span><br><span class="line">        std::vector&lt;cv::Mat&gt; split_channels = &#123;</span><br><span class="line">                cv::<span class="built_in">Mat</span>(images[b].<span class="built_in">size</span>(),CV_32FC1,tensor + b * strides[<span class="number">0</span>]),</span><br><span class="line">                cv::<span class="built_in">Mat</span>(images[b].<span class="built_in">size</span>(),CV_32FC1,tensor + b * strides[<span class="number">0</span>] + strides[<span class="number">1</span>]),</span><br><span class="line">                cv::<span class="built_in">Mat</span>(images[b].<span class="built_in">size</span>(),CV_32FC1,tensor + b * strides[<span class="number">0</span>] + <span class="number">2</span>*strides[<span class="number">1</span>]),</span><br><span class="line">        &#125;;</span><br><span class="line">        cv::<span class="built_in">split</span>(image_f, split_channels);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> batch_size * height * width * channels;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>The time cost of OpenCV preprocessing method(ms)</p>
<table>
<colgroup>
<col style="width: 38%" />
<col style="width: 17%" />
<col style="width: 13%" />
<col style="width: 15%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">GPU(Precision)</th>
<th style="text-align: center;">Image2Float</th>
<th style="text-align: center;">Copy2GPU</th>
<th style="text-align: center;">Inference</th>
<th style="text-align: center;">GPU2CPU</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">t4(int8)</td>
<td style="text-align: center;">2.53026</td>
<td style="text-align: center;">0.935451</td>
<td style="text-align: center;">2.56143</td>
<td style="text-align: center;">0.0210528</td>
</tr>
</tbody>
</table>
</details>
<p>As shown in the example preprocessing code above, in the cpu, the
data is first converted to float type and normalized to [0,1]. The
arrangement is also permuted from NHWC to NCHW. Then the float data is
transferred to gpu memory to do TRT model inference. In the time cost
table, it shows that the speed of image preprocessing and transmission
for this model is actually greater than the speed of model inference. In
this case, the model deployed in gpu is not efficient and still has room
to speed up.</p>
<h2 id="gpu-image-preprocessing-by-npp">GPU Image Preprocessing by
NPP</h2>
<p>As mentioned, there are two reasons make the CPU image data
preprocessing slow: the efficiency of CPU to convert the image from
uint8 to float32 is low; since the float32 data is 4 times larger than
uint8 data, the transmission efficiency between cpu memory and gpu
memory is slower for float data. In this case, a simple speed-up way is
to transfer uint8 data to gpu and allows gpu to complete the conversion
from uint8 to float32. These processes can be done by NPP easily and
efficiently.</p>
<p>Nvidia NPP is a cuda library for GPU accelerated 2D image and signal
processing. It contains multiple submodules, which allows users to
efficiently do the image computation on the gpu like the data type
conversion, the color or geometric transformation and etc.. In this
example, the NPPC, NPPIDEI and NPPIAL in NPP are used to perform the
data type conversion from uint8 to float32 in the image data
preprocessing, the channel change from NHWC to NCHW, and the
normalization. The code is shown as follows.</p>
<details>
<summary>
Code: Preprocess data in gpu
</summary>
<p>NPP preprocessing code <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">imageToTensorGPUFloat</span><span class="params">(<span class="type">const</span> std::vector&lt;cv::Mat&gt; &amp; images, <span class="type">void</span> * gpu_images, <span class="type">void</span> * tensor, <span class="type">const</span> <span class="type">int</span> batch_size, <span class="type">const</span> <span class="type">float</span> alpha)</span> </span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> height = images[<span class="number">0</span>].rows;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> width  = images[<span class="number">0</span>].cols;</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> channels = images[<span class="number">0</span>].<span class="built_in">channels</span>();</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> stride = height * width * channels;</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> stride_s = width * channels;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> dstOrder[<span class="number">3</span>] = &#123;<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>&#125;;</span><br><span class="line">    Npp32f scale[<span class="number">3</span>] = &#123;alpha, alpha, alpha&#125;;</span><br><span class="line">    NppiSize dstSize = &#123;width, height&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> omp parallel for num_threads(c_numOmpThread) schedule(static, 1)</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> b = <span class="number">0</span>; b &lt; batch_size; b++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cudaMemcpy</span>((Npp8u*)gpu_images + b * stride, images[b].data, stride, cudaMemcpyHostToDevice);</span><br><span class="line">        <span class="built_in">nppiSwapChannels_8u_C3IR</span>((Npp8u*)gpu_images + b * stride, stride_s, dstSize, dstOrder);</span><br><span class="line">        <span class="built_in">nppiConvert_8u32f_C3R</span>((Npp8u*)gpu_images + b * stride, stride_s, (Npp32f*)tensor, stride_s*<span class="built_in">sizeof</span>(<span class="type">float</span>), dstSize);</span><br><span class="line">        <span class="built_in">nppiMulC_32f_C3IR</span>(scale, (Npp32f*)tensor, stride_s*<span class="built_in">sizeof</span>(<span class="type">float</span>), dstSize);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> batch_size * stride;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>NPP preprocessing code (without normalization and channel
permutation) <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">imageToTensorGPUFloat</span><span class="params">(<span class="type">const</span> std::vector&lt;cv::Mat&gt; &amp; images, <span class="type">void</span> * gpu_images, <span class="type">void</span> * tensor, <span class="type">const</span> <span class="type">int</span> batch_size)</span> </span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> height = images[<span class="number">0</span>].rows;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> width  = images[<span class="number">0</span>].cols;</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> channels = images[<span class="number">0</span>].<span class="built_in">channels</span>();</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> stride = height * width * channels;</span><br><span class="line">    NppiSize dstSize = &#123;width, height&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> omp parallel for num_threads(c_numOmpThread) schedule(static, 1)</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> b = <span class="number">0</span>; b &lt; batch_size; b++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cudaMemcpy</span>((Npp8u*)gpu_images + b * stride, images[b].data, stride, cudaMemcpyHostToDevice);</span><br><span class="line">        <span class="built_in">nppiConvert_8u32f_C3R</span>((Npp8u*)gpu_images + b * stride, width * channels, (Npp32f*)tensor, width * channels*<span class="built_in">sizeof</span>(<span class="type">float</span>), dstSize);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> batch_size * stride;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>The time cost of NPP preprocessing (without normalization and channel
permutation) (ms)</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">GPU(Precision)</th>
<th style="text-align: center;">Image2GPU2Float</th>
<th style="text-align: center;">Inference</th>
<th style="text-align: center;">GPU2CPU</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">t4(int8)</td>
<td style="text-align: center;">0.532469</td>
<td style="text-align: center;">3.07869</td>
<td style="text-align: center;">0.0208867</td>
</tr>
</tbody>
</table>
</details>
<p>As shown in the example code above, for preprocessing in the gpu, the
uint8 data is first transferred to the gpu memory. Then the data
arrangement is permuted from NHWC to NCHW and finally the uint8 data
converted to the float type and normalized to [0., 1.]. The normalized
data is directly stored in the gpu memory reserved by the TRT model.
Since elementwise operation and channel permute are performed
efficiently in the TRT model, the normalization and channel conversion
in the preprocessing can be moved to the model as operations. Compared
with CPU image preprocessing, the GPU image preprocessing time is
reduced from 3.5ms to 0.5ms, the total running time of the entire model
is reduced from 6ms to 3.5ms, and the frame processing per second (fps)
is from 166 frames raised to 285 frames, the overall speed has reached
1.7 times faster.</p>
<p>It should be noted that due to the long conversion time of the TRT
model, the example in this blog only tests the execution speed when
batch is 1. If large batches are encountered during deployment and the
gpu preprocessing speed is slow, it may be due to the cuda code
execution and transmission. In this case, it would be better to copy the
entire batch of images to the GPU memory and the data type conversion to
improve the preprocessing speed in the batch. Another way to speed up
the progress is process each image sample in a stream.</p>
<h2 id="reference">Reference</h2>
<ul>
<li><a
href="https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/infer/FoundationalTypes/DataType.html">tensorrt
document</a></li>
<li><a href="https://developer.nvidia.com/cuda-gpus#compute">gpu
computation framework</a></li>
<li><a
href="https://github.com/ShuangXieIrene/ssds.pytorch">ssds.pytorch</a></li>
</ul>
]]></content>
      <categories>
        <category>model inference</category>
      </categories>
      <tags>
        <tag>cpp</tag>
        <tag>cuda</tag>
        <tag>tensorrt</tag>
      </tags>
  </entry>
</search>
